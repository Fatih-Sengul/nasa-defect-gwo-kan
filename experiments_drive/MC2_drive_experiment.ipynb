{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸš€ NASA Defect Prediction: MC2\n\n**Dataset:** MC2\n**Method:** Random Forest â†’ KAN â†’ Attention-KAN\n**Goal:** Accuracy korunurken Recall artÄ±rma (F2 odaklÄ±)\n\n## Problem TanÄ±mÄ±\nNASA yazÄ±lÄ±m hata (defect) veri setleri, bileÅŸenlerin hatalÄ± olup olmadÄ±ÄŸÄ±nÄ± tahmin etmeyi amaÃ§lar. Dengesiz sÄ±nÄ±f daÄŸÄ±lÄ±mÄ± nedeniyle **Recall** kritik Ã¶nemdedir. Ancak yalnÄ±zca Recall'u artÄ±rmak, Accuracy'yi dÃ¼ÅŸÃ¼rebilir. Bu notebook'ta Random Forest, KAN ve Attention-KAN modellerini karÅŸÄ±laÅŸtÄ±rarak Recall/Accuracy dengesini gÃ¶zlemleyeceÄŸiz.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Veri HazÄ±rlama\n- Google Drive baÄŸlantÄ±sÄ±\n- ARFF dosyasÄ±nÄ± okuma\n- Etiket dÃ¶nÃ¼ÅŸÃ¼mÃ¼ ve eksik deÄŸer yÃ¶netimi\n- Train/Val/Test bÃ¶lÃ¼nmesi (stratified)\n- Min-Max Ã¶lÃ§ekleme\n- SMOTE ile oversampling (sadece eÄŸitim seti)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip install scipy scikit-learn imbalanced-learn pandas numpy torch seaborn matplotlib openpyxl -q"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from google.colab import drive\n\ndrive.mount('/content/drive')\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom scipy.io import arff\nfrom io import StringIO\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom imblearn.over_sampling import SMOTE\n\nDATASET_NAME = \"MC2\"\nbase_path = \"/content/drive/MyDrive/nasa_datasets/\"  # <- Drive yolunuza gÃ¶re gÃ¼ncelleyin\nfile_path = os.path.join(base_path, f\"{DATASET_NAME}.arff\")\n\n\ndef load_arff_data(file_path):\n    try:\n        data, meta = arff.loadarff(file_path)\n        df = pd.DataFrame(data)\n        for col in df.select_dtypes([object]).columns:\n            try:\n                df[col] = df[col].str.decode('utf-8')\n            except Exception:\n                pass\n        return df\n    except Exception:\n        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n            content = f.read()\n        data_start = content.lower().find('@data')\n        if data_start == -1:\n            raise ValueError(\"ARFF dosyasÄ±nda @data bÃ¶lÃ¼mÃ¼ bulunamadÄ±.\")\n        data_section = content[data_start + 5:].strip()\n        df = pd.read_csv(StringIO(data_section), header=None)\n        return df\n\n\ndf = load_arff_data(file_path)\nprint(f\"{DATASET_NAME} veri seti yÃ¼klendi. Ã–rnek sayÄ±sÄ±: {len(df)}\")\n\nX = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values\nif y.dtype == object or y.dtype == 'bool' or np.issubdtype(y.dtype, np.str_):\n    y = LabelEncoder().fit_transform(y)\n\nX_full_train, X_test, y_full_train, y_test = train_test_split(\n    X, y, test_size=0.20, stratify=y, random_state=42\n)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_full_train, y_full_train, test_size=0.10, stratify=y_full_train, random_state=42\n)\n\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\nsmote_ratio = 0.8\nsm = SMOTE(sampling_strategy=smote_ratio, random_state=42)\nX_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\nprint(f\"SMOTE sonrasÄ± eÄŸitim daÄŸÄ±lÄ±mÄ±: {np.bincount(y_train_smote)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Model 1 â€” Random Forest (Baseline)\nDoÄŸrulama seti Ã¼zerinde F1 maksimize edecek eÅŸik seÃ§ilir, ardÄ±ndan Train/Test metrikleri raporlanÄ±r.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score\n\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train_smote, y_train_smote)\n\ny_val_proba = rf.predict_proba(X_val)[:, 1]\nbest_thresh = 0.5\nbest_f1 = 0.0\nfor thresh in np.arange(0.1, 0.91, 0.01):\n    y_val_pred = (y_val_proba >= thresh).astype(int)\n    f1 = f1_score(y_val, y_val_pred)\n    if f1 > best_f1:\n        best_f1 = f1\n        best_thresh = thresh\n\nprint(f\"RF optimum eÅŸik: {best_thresh:.2f} (F1={best_f1:.3f})\")\n\ny_train_pred_rf = (rf.predict_proba(X_train)[:, 1] >= best_thresh).astype(int)\ny_test_pred_rf = (rf.predict_proba(X_test)[:, 1] >= best_thresh).astype(int)\n\nmetrics_rf = {\n    'Train': {\n        'Accuracy': accuracy_score(y_train, y_train_pred_rf),\n        'Precision': precision_score(y_train, y_train_pred_rf, zero_division=0),\n        'Recall': recall_score(y_train, y_train_pred_rf),\n        'F1': f1_score(y_train, y_train_pred_rf),\n        'F2': fbeta_score(y_train, y_train_pred_rf, beta=2),\n    },\n    'Test': {\n        'Accuracy': accuracy_score(y_test, y_test_pred_rf),\n        'Precision': precision_score(y_test, y_test_pred_rf, zero_division=0),\n        'Recall': recall_score(y_test, y_test_pred_rf),\n        'F1': f1_score(y_test, y_test_pred_rf),\n        'F2': fbeta_score(y_test, y_test_pred_rf, beta=2),\n    },\n}\n\nprint(\"Random Forest SonuÃ§larÄ±:\")\nfor phase in ['Train', 'Test']:\n    m = metrics_rf[phase]\n    print(\n        f\" {phase} -> Accuracy: {m['Accuracy']:.3f}, Precision: {m['Precision']:.3f}, \"\n        f\"Recall: {m['Recall']:.3f}, F1: {m['F1']:.3f}, F2: {m['F2']:.3f}\"\n    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Model 2 â€” KAN (Kolmogorovâ€“Arnold Network)\nKAN, spline tabanlÄ± dÃ¶nÃ¼ÅŸÃ¼m katmanlarÄ±yla tabular veride esnek temsil saÄŸlar. Focal Loss ve erken durdurma ile eÄŸitilir.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\n\nclass KANLinear(nn.Module):\n    def __init__(self, in_features, out_features, grid_size=5, spline_order=3):\n        super().__init__()\n        self.grid_size = grid_size\n        self.spline_order = spline_order\n        self.grid = nn.Parameter(\n            torch.linspace(-1, 1, grid_size)\n            .unsqueeze(0)\n            .unsqueeze(0)\n            .repeat(out_features, in_features, 1)\n        )\n        self.coef = nn.Parameter(torch.randn(out_features, in_features, grid_size + spline_order) * 0.1)\n        self.base_weight = nn.Parameter(torch.randn(out_features, in_features) * 0.1)\n\n    def forward(self, x):\n        x_expanded = x.unsqueeze(1).unsqueeze(-1)\n        grid = self.grid.unsqueeze(0)\n        distances = torch.abs(x_expanded - grid)\n        basis = torch.relu(1 - distances)\n        spline_out = torch.einsum('boij,boij->boi', basis, self.coef[..., : self.grid_size])\n        spline_out = spline_out.sum(dim=-1)\n        linear_out = torch.matmul(x, self.base_weight.t())\n        return linear_out + spline_out\n\n\nclass KANModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim=64, grid_size=5, spline_order=3):\n        super().__init__()\n        self.kan1 = KANLinear(input_dim, hidden_dim, grid_size, spline_order)\n        self.bn1 = nn.BatchNorm1d(hidden_dim)\n        self.kan2 = KANLinear(hidden_dim, hidden_dim // 2, grid_size, spline_order)\n        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n        self.fc_out = nn.Linear(hidden_dim // 2, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.bn1(self.kan1(x)))\n        x = torch.relu(self.bn2(self.kan2(x)))\n        return torch.sigmoid(self.fc_out(x))\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2.0):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, inputs, targets):\n        bce_loss = nn.functional.binary_cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-bce_loss)\n        focal_loss = self.alpha * ((1 - pt) ** self.gamma) * bce_loss\n        return focal_loss.mean()\n\n\ndef train_model(model, X_train, y_train, X_val, y_val, epochs=50, batch_size=32, lr=0.01):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    train_ds = TensorDataset(\n        torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n    )\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n    criterion = FocalLoss(alpha=0.25, gamma=2.0)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    best_model_state = None\n    best_f1 = 0.0\n    epochs_no_improve = 0\n    for _ in range(epochs):\n        model.train()\n        for X_batch, y_batch in train_loader:\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n            optimizer.zero_grad()\n            y_pred = model(X_batch).view(-1)\n            loss = criterion(y_pred, y_batch)\n            loss.backward()\n            optimizer.step()\n\n        model.eval()\n        with torch.no_grad():\n            X_val_t = torch.tensor(X_val, dtype=torch.float32).to(device)\n            val_preds = model(X_val_t).view(-1).cpu().numpy()\n        val_pred_labels = (val_preds >= 0.5).astype(int)\n        val_f1 = f1_score(y_val, val_pred_labels)\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            epochs_no_improve = 0\n            best_model_state = model.state_dict()\n        else:\n            epochs_no_improve += 1\n        if epochs_no_improve >= 5:\n            break\n\n    if best_model_state:\n        model.load_state_dict(best_model_state)\n    return model\n\n\nkan_model = KANModel(input_dim=X_train_smote.shape[1], hidden_dim=64, grid_size=5, spline_order=3)\nkan_model = train_model(kan_model, X_train_smote, y_train_smote, X_val, y_val)\n\nkan_model.eval()\nwith torch.no_grad():\n    val_probs = kan_model(torch.tensor(X_val, dtype=torch.float32)).view(-1).numpy()\n\nbest_thresh = 0.5\nbest_f1 = 0.0\nfor t in np.arange(0.1, 0.91, 0.01):\n    f1 = f1_score(y_val, (val_probs >= t).astype(int))\n    if f1 > best_f1:\n        best_f1 = f1\n        best_thresh = t\n\nprint(f\"KAN en iyi eÅŸik: {best_thresh:.2f} (F1={best_f1:.3f})\")\n\nwith torch.no_grad():\n    train_probs_kan = kan_model(torch.tensor(X_train, dtype=torch.float32)).view(-1).numpy()\n    test_probs_kan = kan_model(torch.tensor(X_test, dtype=torch.float32)).view(-1).numpy()\n\ny_train_pred_kan = (train_probs_kan >= best_thresh).astype(int)\ny_test_pred_kan = (test_probs_kan >= best_thresh).astype(int)\n\nprint(\"KAN SonuÃ§larÄ±:\")\nprint(\n    f\" Train -> Accuracy: {accuracy_score(y_train, y_train_pred_kan):.3f}, \"\n    f\"Precision: {precision_score(y_train, y_train_pred_kan, zero_division=0):.3f}, \"\n    f\"Recall: {recall_score(y_train, y_train_pred_kan):.3f}, \"\n    f\"F1: {f1_score(y_train, y_train_pred_kan):.3f}\"\n)\nprint(\n    f\" Test  -> Accuracy: {accuracy_score(y_test, y_test_pred_kan):.3f}, \"\n    f\"Precision: {precision_score(y_test, y_test_pred_kan, zero_division=0):.3f}, \"\n    f\"Recall: {recall_score(y_test, y_test_pred_kan):.3f}, \"\n    f\"F1: {f1_score(y_test, y_test_pred_kan):.3f}, \"\n    f\"F2: {fbeta_score(y_test, y_test_pred_kan, beta=2):.3f}\"\n)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Model 3 â€” Attention-KAN\nGirdi Ã¶zelliklerine dikkat mekanizmasÄ± eklenir. AmaÃ§, kritik Ã¶zelliklere odaklanarak Recall'u artÄ±rÄ±rken Accuracy'yi korumaktÄ±r.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class AttentionKAN(nn.Module):\n    def __init__(self, input_dim, hidden_dim=64, grid_size=5, spline_order=3):\n        super().__init__()\n        self.att_fc1 = nn.Linear(input_dim, input_dim)\n        self.att_fc2 = nn.Linear(input_dim, input_dim)\n        self.kan1 = KANLinear(input_dim, hidden_dim, grid_size, spline_order)\n        self.bn1 = nn.BatchNorm1d(hidden_dim)\n        self.kan2 = KANLinear(hidden_dim, hidden_dim // 2, grid_size, spline_order)\n        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n        self.fc_out = nn.Linear(hidden_dim // 2, 1)\n\n    def forward(self, x):\n        att_scores = torch.relu(self.att_fc1(x))\n        att_scores = torch.sigmoid(self.att_fc2(att_scores))\n        x_att = x * att_scores\n        x = torch.relu(self.bn1(self.kan1(x_att)))\n        x = torch.relu(self.bn2(self.kan2(x)))\n        return torch.sigmoid(self.fc_out(x))\n\n\natt_kan_model = AttentionKAN(input_dim=X_train_smote.shape[1], hidden_dim=64, grid_size=5, spline_order=3)\natt_kan_model = train_model(att_kan_model, X_train_smote, y_train_smote, X_val, y_val)\n\natt_kan_model.eval()\nwith torch.no_grad():\n    val_probs = att_kan_model(torch.tensor(X_val, dtype=torch.float32)).view(-1).numpy()\n\nbest_thresh = 0.5\nbest_f1 = 0.0\nfor t in np.arange(0.1, 0.91, 0.01):\n    f1 = f1_score(y_val, (val_probs >= t).astype(int))\n    if f1 > best_f1:\n        best_f1 = f1\n        best_thresh = t\n\nprint(f\"Attention-KAN en iyi eÅŸik: {best_thresh:.2f} (F1={best_f1:.3f})\")\n\nwith torch.no_grad():\n    train_probs_att = att_kan_model(torch.tensor(X_train, dtype=torch.float32)).view(-1).numpy()\n    test_probs_att = att_kan_model(torch.tensor(X_test, dtype=torch.float32)).view(-1).numpy()\n\ny_train_pred_att = (train_probs_att >= best_thresh).astype(int)\ny_test_pred_att = (test_probs_att >= best_thresh).astype(int)\n\nprint(\"Attention-KAN SonuÃ§larÄ±:\")\nprint(\n    f\" Train -> Accuracy: {accuracy_score(y_train, y_train_pred_att):.3f}, \"\n    f\"Precision: {precision_score(y_train, y_train_pred_att, zero_division=0):.3f}, \"\n    f\"Recall: {recall_score(y_train, y_train_pred_att):.3f}, \"\n    f\"F1: {f1_score(y_train, y_train_pred_att):.3f}\"\n)\nprint(\n    f\" Test  -> Accuracy: {accuracy_score(y_test, y_test_pred_att):.3f}, \"\n    f\"Precision: {precision_score(y_test, y_test_pred_att, zero_division=0):.3f}, \"\n    f\"Recall: {recall_score(y_test, y_test_pred_att):.3f}, \"\n    f\"F1: {f1_score(y_test, y_test_pred_att):.3f}, \"\n    f\"F2: {fbeta_score(y_test, y_test_pred_att, beta=2):.3f}\"\n)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) KarÅŸÄ±laÅŸtÄ±rma\nAÅŸaÄŸÄ±da metrikler **Accuracy, Precision, Recall, F1, F2** ÅŸeklinde raporlanÄ±r. Ä°lgili veri seti iÃ§in sonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±rÄ±p yorumlayabilirsiniz.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n\nsummary = pd.DataFrame(\n    [\n        {\n            'Model': 'Random Forest',\n            **metrics_rf['Test'],\n        },\n        {\n            'Model': 'KAN',\n            'Accuracy': accuracy_score(y_test, y_test_pred_kan),\n            'Precision': precision_score(y_test, y_test_pred_kan, zero_division=0),\n            'Recall': recall_score(y_test, y_test_pred_kan),\n            'F1': f1_score(y_test, y_test_pred_kan),\n            'F2': fbeta_score(y_test, y_test_pred_kan, beta=2),\n        },\n        {\n            'Model': 'Attention-KAN',\n            'Accuracy': accuracy_score(y_test, y_test_pred_att),\n            'Precision': precision_score(y_test, y_test_pred_att, zero_division=0),\n            'Recall': recall_score(y_test, y_test_pred_att),\n            'F1': f1_score(y_test, y_test_pred_att),\n            'F2': fbeta_score(y_test, y_test_pred_att, beta=2),\n        },\n    ]\n)\nsummary\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}