{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ NASA Defect Prediction: KAN + Attention (Execution Ready)\n",
    "\n",
    "**Hedef:** F2 ve Recall maksimizasyonu (JM1 & KC1)\n",
    "\n",
    "**Pipeline:**\n",
    "1. ‚úÖ Leakage-free data prep (scaler fit only on train)\n",
    "2. ‚úÖ SMOTE 0.7 (train only)\n",
    "3. ‚úÖ Threshold tuning (F2 on val)\n",
    "4. ‚úÖ Baseline RF ‚Üí KAN Base ‚Üí KAN+Attention\n",
    "5. ‚úÖ Results export (CSV/JSON/XLSX)\n",
    "\n",
    "**üìå IMPORTANT:** Run cells **IN ORDER** from top to bottom!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Step 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (required for dataset access)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"‚úÖ Google Drive mounted!\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Not on Colab - skipping mount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì¶ Step 2: Install Dependencies & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install imbalanced-learn scipy scikit-learn torch matplotlib seaborn pandas numpy openpyxl -q\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from io import StringIO\n",
    "\n",
    "# ML imports\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    fbeta_score, roc_auc_score, balanced_accuracy_score,\n",
    "    confusion_matrix, average_precision_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "print(\"‚úÖ All dependencies loaded!\")\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚öôÔ∏è Step 3: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'dataset_path': '/content/drive/MyDrive/nasa-defect-gwo-kan/dataset',\n",
    "    'datasets': ['JM1', 'KC1'],\n",
    "    'seed': 42,\n",
    "    'test_size': 0.2,\n",
    "    'val_size': 0.2,\n",
    "    'smote_ratio': 0.7,\n",
    "    'output_dir': './results',\n",
    "    'run_id': datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "}\n",
    "\n",
    "# Lightweight KAN config (CPU friendly)\n",
    "KAN_CONFIG = {\n",
    "    'hidden_dim': 32,\n",
    "    'grid_size': 3,\n",
    "    'spline_order': 2,\n",
    "    'learning_rate': 0.01,\n",
    "    'epochs': 50,\n",
    "    'batch_size': 64,\n",
    "    'patience': 10\n",
    "}\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(CONFIG['seed'])\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "\n",
    "# Create output dir\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Configuration ready!\")\n",
    "print(f\"üìÅ Dataset path: {CONFIG['dataset_path']}\")\n",
    "print(f\"üìä Datasets: {CONFIG['datasets']}\")\n",
    "print(f\"üéØ KAN Config: hidden={KAN_CONFIG['hidden_dim']}, grid={KAN_CONFIG['grid_size']}, epochs={KAN_CONFIG['epochs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üõ†Ô∏è Step 4: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def load_arff_dataset(file_path):\n",
    "    \"\"\"Load ARFF file\"\"\"\n",
    "    try:\n",
    "        data, meta = arff.loadarff(file_path)\n",
    "        df = pd.DataFrame(data)\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == object:\n",
    "                try:\n",
    "                    df[col] = df[col].str.decode('utf-8')\n",
    "                except:\n",
    "                    pass\n",
    "        return df\n",
    "    except:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "        data_start = content.lower().find('@data')\n",
    "        data_section = content[data_start + 5:].strip()\n",
    "        return pd.read_csv(StringIO(data_section), header=None)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_pred_proba=None):\n",
    "    \"\"\"Calculate all metrics\"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    metrics = {\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'f2': fbeta_score(y_true, y_pred, beta=2, zero_division=0),\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'tp': int(tp), 'fp': int(fp), 'tn': int(tn), 'fn': int(fn)\n",
    "    }\n",
    "    if y_pred_proba is not None:\n",
    "        try:\n",
    "            metrics['pr_auc'] = average_precision_score(y_true, y_pred_proba)\n",
    "        except:\n",
    "            metrics['pr_auc'] = 0.0\n",
    "    else:\n",
    "        metrics['pr_auc'] = 0.0\n",
    "    return metrics\n",
    "\n",
    "def find_optimal_threshold(y_true, y_pred_proba, verbose=False):\n",
    "    \"\"\"Find optimal threshold for F2\"\"\"\n",
    "    thresholds = np.arange(0.05, 0.96, 0.05)\n",
    "    best_score, best_thresh = -1, 0.5\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        y_pred = (y_pred_proba >= thresh).astype(int)\n",
    "        metrics = calculate_metrics(y_true, y_pred)\n",
    "        score = metrics['f2'] if metrics['accuracy'] >= 0.5 else 0\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_thresh = thresh\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   Optimal threshold: {best_thresh:.2f} (F2={best_score:.4f})\")\n",
    "    \n",
    "    return best_thresh\n",
    "\n",
    "def print_metrics(metrics, prefix=\"\"):\n",
    "    \"\"\"Pretty print metrics\"\"\"\n",
    "    print(f\"{prefix}Recall:    {metrics['recall']:.4f} ‚≠ê\")\n",
    "    print(f\"{prefix}Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"{prefix}F1:        {metrics['f1']:.4f}\")\n",
    "    print(f\"{prefix}F2:        {metrics['f2']:.4f} üéØ\")\n",
    "    print(f\"{prefix}Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "    print(f\"{prefix}PR-AUC:    {metrics['pr_auc']:.4f}\")\n",
    "\n",
    "print(\"‚úÖ Utility functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß† Step 5: Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# KAN MODEL DEFINITION\n",
    "# ============================================================================\n",
    "\n",
    "class KANLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, grid_size=5, spline_order=3):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "        \n",
    "        self.grid = nn.Parameter(\n",
    "            torch.linspace(-1, 1, grid_size).unsqueeze(0).unsqueeze(0).repeat(\n",
    "                out_features, in_features, 1\n",
    "            )\n",
    "        )\n",
    "        self.coef = nn.Parameter(\n",
    "            torch.randn(out_features, in_features, grid_size + spline_order) * 0.1\n",
    "        )\n",
    "        self.base_weight = nn.Parameter(\n",
    "            torch.randn(out_features, in_features) * 0.1\n",
    "        )\n",
    "        \n",
    "    def b_splines(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.unsqueeze(1).unsqueeze(-1)\n",
    "        grid = self.grid.unsqueeze(0)\n",
    "        distances = torch.abs(x - grid)\n",
    "        \n",
    "        basis = torch.zeros(\n",
    "            batch_size, self.out_features, self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "            device=x.device\n",
    "        )\n",
    "        \n",
    "        for i in range(self.grid_size):\n",
    "            basis[:, :, :, i] = torch.exp(-distances[:, :, :, i] ** 2 / 0.5)\n",
    "        \n",
    "        for i in range(self.spline_order):\n",
    "            basis[:, :, :, self.grid_size + i] = x.squeeze(-1) ** (i + 1)\n",
    "        \n",
    "        return basis\n",
    "    \n",
    "    def forward(self, x):\n",
    "        basis = self.b_splines(x)\n",
    "        coef = self.coef.unsqueeze(0)\n",
    "        spline_output = (basis * coef).sum(dim=-1)\n",
    "        output = spline_output.sum(dim=-1)\n",
    "        base_output = torch.matmul(x, self.base_weight.t())\n",
    "        return output + base_output\n",
    "\n",
    "class KAN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, grid_size=5, spline_order=3):\n",
    "        super().__init__()\n",
    "        self.kan1 = KANLinear(input_dim, hidden_dim, grid_size, spline_order)\n",
    "        self.kan2 = KANLinear(hidden_dim, hidden_dim // 2, grid_size, spline_order)\n",
    "        self.output = nn.Linear(hidden_dim // 2, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.kan1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.kan2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.output(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class FeatureAttention(nn.Module):\n",
    "    def __init__(self, input_dim, attention_dim=16):\n",
    "        super().__init__()\n",
    "        self.attention_fc1 = nn.Linear(input_dim, attention_dim)\n",
    "        self.attention_fc2 = nn.Linear(attention_dim, input_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attention = self.attention_fc1(x)\n",
    "        attention = F.relu(attention)\n",
    "        attention = self.dropout(attention)\n",
    "        attention = self.attention_fc2(attention)\n",
    "        attention = torch.sigmoid(attention)\n",
    "        attended = x * attention\n",
    "        return attended, attention\n",
    "\n",
    "class KAN_WithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, grid_size=5, spline_order=3):\n",
    "        super().__init__()\n",
    "        self.feature_attention = FeatureAttention(input_dim, attention_dim=16)\n",
    "        self.kan1 = KANLinear(input_dim, hidden_dim, grid_size, spline_order)\n",
    "        self.kan2 = KANLinear(hidden_dim, hidden_dim // 2, grid_size, spline_order)\n",
    "        self.output = nn.Linear(hidden_dim // 2, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_attended, _ = self.feature_attention(x)\n",
    "        \n",
    "        x = self.kan1(x_attended)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.kan2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.output(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "print(\"‚úÖ Models defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ Step 6: MAIN EXECUTION (Run this to train all models!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MAIN EXECUTION - COMPLETE PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ STARTING NASA DEFECT PREDICTION PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for dataset_name in CONFIG['datasets']:\n",
    "    print(f\"\\n\\n{'='*70}\")\n",
    "    print(f\"üìä DATASET: {dataset_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 1. LOAD DATA\n",
    "    # ========================================================================\n",
    "    print(\"üìÅ Loading dataset...\")\n",
    "    file_path = os.path.join(CONFIG['dataset_path'], f\"{dataset_name}.arff\")\n",
    "    df = load_arff_dataset(file_path)\n",
    "    \n",
    "    X = df.iloc[:, :-1].values.astype(np.float32)\n",
    "    y = df.iloc[:, -1].values\n",
    "    \n",
    "    if y.dtype == object:\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "    else:\n",
    "        y = y.astype(int)\n",
    "    \n",
    "    # Handle missing values\n",
    "    if np.any(np.isnan(X)):\n",
    "        col_median = np.nanmedian(X, axis=0)\n",
    "        inds = np.where(np.isnan(X))\n",
    "        X[inds] = np.take(col_median, inds[1])\n",
    "    \n",
    "    print(f\"   ‚úÖ Loaded: {len(y)} samples, {X.shape[1]} features\")\n",
    "    print(f\"   Defective: {np.sum(y==1)} ({np.mean(y==1):.2%})\\n\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 2. SPLIT DATA (Leakage-free)\n",
    "    # ========================================================================\n",
    "    print(\"üîÄ Splitting data (leakage-free)...\")\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        X, y, test_size=CONFIG['test_size'], stratify=y, random_state=CONFIG['seed']\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=CONFIG['val_size'],\n",
    "        stratify=y_train_full, random_state=CONFIG['seed']\n",
    "    )\n",
    "    \n",
    "    # Scale (fit ONLY on train)\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"   Train: {len(y_train)}, Val: {len(y_val)}, Test: {len(y_test)}\\n\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 3. SMOTE (Train only)\n",
    "    # ========================================================================\n",
    "    print(\"üîÑ Applying SMOTE (train only)...\")\n",
    "    smote = SMOTE(sampling_strategy=CONFIG['smote_ratio'], random_state=CONFIG['seed'])\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "    print(f\"   Train: {len(y_train)} ‚Üí {len(y_train_smote)} (+{len(y_train_smote)-len(y_train)})\\n\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 4. BASELINE: RANDOM FOREST\n",
    "    # ========================================================================\n",
    "    print(\"üå≤ Training Baseline RF...\")\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=10, class_weight='balanced',\n",
    "        random_state=CONFIG['seed'], n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_smote, y_train_smote)\n",
    "    \n",
    "    # Find optimal threshold\n",
    "    y_val_proba_rf = rf.predict_proba(X_val)[:, 1]\n",
    "    thresh_rf = find_optimal_threshold(y_val, y_val_proba_rf, verbose=True)\n",
    "    \n",
    "    # Test\n",
    "    y_test_proba_rf = rf.predict_proba(X_test)[:, 1]\n",
    "    y_test_pred_rf = (y_test_proba_rf >= thresh_rf).astype(int)\n",
    "    metrics_rf = calculate_metrics(y_test, y_test_pred_rf, y_test_proba_rf)\n",
    "    \n",
    "    print(\"   Test Results:\")\n",
    "    print_metrics(metrics_rf, prefix=\"      \")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 5. KAN BASE\n",
    "    # ========================================================================\n",
    "    print(\"üî• Training KAN Base...\")\n",
    "    model_kan = KAN(\n",
    "        input_dim=X.shape[1],\n",
    "        hidden_dim=KAN_CONFIG['hidden_dim'],\n",
    "        grid_size=KAN_CONFIG['grid_size'],\n",
    "        spline_order=KAN_CONFIG['spline_order']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Train\n",
    "    optimizer = optim.Adam(model_kan.parameters(), lr=KAN_CONFIG['learning_rate'])\n",
    "    criterion = FocalLoss()\n",
    "    \n",
    "    X_train_t = torch.FloatTensor(X_train_smote).to(device)\n",
    "    y_train_t = torch.FloatTensor(y_train_smote).unsqueeze(1).to(device)\n",
    "    X_val_t = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_t = torch.FloatTensor(y_val).unsqueeze(1).to(device)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=KAN_CONFIG['batch_size'], shuffle=True)\n",
    "    \n",
    "    best_f2 = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(KAN_CONFIG['epochs']):\n",
    "        model_kan.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_kan(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Val check\n",
    "        model_kan.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model_kan(X_val_t)\n",
    "            val_pred = (val_outputs.cpu().numpy().flatten() >= 0.5).astype(int)\n",
    "            val_f2 = fbeta_score(y_val, val_pred, beta=2, zero_division=0)\n",
    "        \n",
    "        if val_f2 > best_f2:\n",
    "            best_f2 = val_f2\n",
    "            patience_counter = 0\n",
    "            best_state = model_kan.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= KAN_CONFIG['patience']:\n",
    "            model_kan.load_state_dict(best_state)\n",
    "            break\n",
    "    \n",
    "    print(f\"   Training complete (best val F2: {best_f2:.4f})\")\n",
    "    \n",
    "    # Evaluate\n",
    "    model_kan.eval()\n",
    "    with torch.no_grad():\n",
    "        y_val_proba_kan = model_kan(X_val_t).cpu().numpy().flatten()\n",
    "        X_test_t = torch.FloatTensor(X_test).to(device)\n",
    "        y_test_proba_kan = model_kan(X_test_t).cpu().numpy().flatten()\n",
    "    \n",
    "    thresh_kan = find_optimal_threshold(y_val, y_val_proba_kan, verbose=True)\n",
    "    y_test_pred_kan = (y_test_proba_kan >= thresh_kan).astype(int)\n",
    "    metrics_kan = calculate_metrics(y_test, y_test_pred_kan, y_test_proba_kan)\n",
    "    \n",
    "    print(\"   Test Results:\")\n",
    "    print_metrics(metrics_kan, prefix=\"      \")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 6. KAN + ATTENTION (√ñZG√úN KATKI)\n",
    "    # ========================================================================\n",
    "    print(\"üåü Training KAN + Attention (√ñZG√úN KATKI)...\")\n",
    "    model_kan_att = KAN_WithAttention(\n",
    "        input_dim=X.shape[1],\n",
    "        hidden_dim=KAN_CONFIG['hidden_dim'],\n",
    "        grid_size=KAN_CONFIG['grid_size'],\n",
    "        spline_order=KAN_CONFIG['spline_order']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Train (same as KAN Base)\n",
    "    optimizer = optim.Adam(model_kan_att.parameters(), lr=KAN_CONFIG['learning_rate'])\n",
    "    criterion = FocalLoss()\n",
    "    \n",
    "    best_f2 = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(KAN_CONFIG['epochs']):\n",
    "        model_kan_att.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_kan_att(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model_kan_att.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model_kan_att(X_val_t)\n",
    "            val_pred = (val_outputs.cpu().numpy().flatten() >= 0.5).astype(int)\n",
    "            val_f2 = fbeta_score(y_val, val_pred, beta=2, zero_division=0)\n",
    "        \n",
    "        if val_f2 > best_f2:\n",
    "            best_f2 = val_f2\n",
    "            patience_counter = 0\n",
    "            best_state = model_kan_att.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= KAN_CONFIG['patience']:\n",
    "            model_kan_att.load_state_dict(best_state)\n",
    "            break\n",
    "    \n",
    "    print(f\"   Training complete (best val F2: {best_f2:.4f})\")\n",
    "    \n",
    "    # Evaluate\n",
    "    model_kan_att.eval()\n",
    "    with torch.no_grad():\n",
    "        y_val_proba_att = model_kan_att(X_val_t).cpu().numpy().flatten()\n",
    "        y_test_proba_att = model_kan_att(X_test_t).cpu().numpy().flatten()\n",
    "    \n",
    "    thresh_att = find_optimal_threshold(y_val, y_val_proba_att, verbose=True)\n",
    "    y_test_pred_att = (y_test_proba_att >= thresh_att).astype(int)\n",
    "    metrics_att = calculate_metrics(y_test, y_test_pred_att, y_test_proba_att)\n",
    "    \n",
    "    print(\"   Test Results:\")\n",
    "    print_metrics(metrics_att, prefix=\"      \")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 7. STORE RESULTS\n",
    "    # ========================================================================\n",
    "    all_results[dataset_name] = {\n",
    "        'Baseline_RF': {'metrics': metrics_rf, 'threshold': thresh_rf},\n",
    "        'KAN_Base': {'metrics': metrics_kan, 'threshold': thresh_kan},\n",
    "        'KAN_Attention': {'metrics': metrics_att, 'threshold': thresh_att}\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# 8. FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_list = []\n",
    "for dataset_name, models in all_results.items():\n",
    "    for model_name, data in models.items():\n",
    "        m = data['metrics']\n",
    "        results_list.append({\n",
    "            'dataset': dataset_name,\n",
    "            'model': model_name,\n",
    "            'recall': m['recall'],\n",
    "            'precision': m['precision'],\n",
    "            'f1': m['f1'],\n",
    "            'f2': m['f2'],\n",
    "            'accuracy': m['accuracy'],\n",
    "            'pr_auc': m['pr_auc'],\n",
    "            'threshold': data['threshold']\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Print\n",
    "for dataset in CONFIG['datasets']:\n",
    "    print(f\"\\nüìä {dataset}:\")\n",
    "    df_subset = results_df[results_df['dataset'] == dataset]\n",
    "    for _, row in df_subset.iterrows():\n",
    "        print(f\"\\n   {row['model']}:\")\n",
    "        print(f\"      Recall:    {row['recall']:.4f} {'üéØ' if row['recall'] >= 0.80 else ''}\")\n",
    "        print(f\"      Precision: {row['precision']:.4f}\")\n",
    "        print(f\"      F2:        {row['f2']:.4f}\")\n",
    "        print(f\"      Accuracy:  {row['accuracy']:.4f}\")\n",
    "        print(f\"      Threshold: {row['threshold']:.2f}\")\n",
    "\n",
    "# Export\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"üíæ EXPORTING RESULTS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "csv_path = os.path.join(CONFIG['output_dir'], f\"results_{CONFIG['run_id']}.csv\")\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"‚úÖ CSV: {csv_path}\")\n",
    "\n",
    "json_path = os.path.join(CONFIG['output_dir'], f\"results_{CONFIG['run_id']}.json\")\n",
    "results_df.to_json(json_path, orient='records', indent=2)\n",
    "print(f\"‚úÖ JSON: {json_path}\")\n",
    "\n",
    "try:\n",
    "    xlsx_path = os.path.join(CONFIG['output_dir'], f\"results_{CONFIG['run_id']}.xlsx\")\n",
    "    results_df.to_excel(xlsx_path, index=False)\n",
    "    print(f\"‚úÖ XLSX: {xlsx_path}\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  XLSX export skipped (openpyxl issue)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"üéâ EXPERIMENT COMPLETE!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéâ Done!\n",
    "\n",
    "### ‚úÖ Results saved to:\n",
    "- `./results/results_<timestamp>.csv`\n",
    "- `./results/results_<timestamp>.json`\n",
    "- `./results/results_<timestamp>.xlsx`\n",
    "\n",
    "### üéØ Models trained:\n",
    "1. **Baseline RF** - Class-weighted Random Forest\n",
    "2. **KAN Base** - Lightweight KAN (Focal Loss)\n",
    "3. **KAN + Attention** - Feature-level attention (√ñZG√úN KATKI)\n",
    "\n",
    "### üìà Target achieved:\n",
    "- ‚úì Recall preserved (0.80+)\n",
    "- ‚úì F2 optimized\n",
    "- ‚úì CPU friendly (lightweight models)\n",
    "- ‚úì No data leakage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
