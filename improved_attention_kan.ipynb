{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Improved Attention-KAN - 4 Strategies Combined\n",
    "## Better Accuracy + Better Recall\n",
    "\n",
    "**Previous Results:**\n",
    "- ‚ùå Accuracy: 45% (too low!)\n",
    "- ‚ùå Precision: 18% (too many false alarms)\n",
    "- ‚úÖ Recall: 81% (good but can be better)\n",
    "\n",
    "**4 Improvements:**\n",
    "1. ‚úÖ **Threshold Optimization**: Find best threshold (not just 0.5)\n",
    "2. ‚úÖ **Focal Loss**: Better handling of class imbalance\n",
    "3. ‚úÖ **pos_weight Tuning**: Optimize FN penalty via GWO (4th param)\n",
    "4. ‚úÖ **Mini Ensemble (2 models)**: More stable predictions\n",
    "\n",
    "**Target:**\n",
    "- Recall ‚â• 85%\n",
    "- Accuracy ‚â• 70%\n",
    "- Precision ‚â• 40%\n",
    "- F1 ‚â• 50%\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from io import StringIO\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, roc_auc_score, fbeta_score, balanced_accuracy_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"[INFO] All imports ready!\")\n",
    "print(f\"[INFO] PyTorch: {torch.__version__}\")\n",
    "print(f\"[INFO] Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ATTENTION-KAN (same as before)\n",
    "# ============================================================================\n",
    "\n",
    "class FeatureAttention(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(FeatureAttention, self).__init__()\n",
    "        hidden = max(in_features // 2, 8)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden, in_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.bn = nn.BatchNorm1d(in_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_norm = self.bn(x)\n",
    "        weights = self.attention(x_norm)\n",
    "        return x * weights, weights\n",
    "\n",
    "\n",
    "class KANLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, grid_size=5):\n",
    "        super(KANLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.grid_size = grid_size\n",
    "        \n",
    "        self.grid = nn.Parameter(torch.linspace(-1, 1, grid_size).unsqueeze(0).unsqueeze(0).repeat(out_features, in_features, 1))\n",
    "        self.coef = nn.Parameter(torch.randn(out_features, in_features, grid_size) * 0.1)\n",
    "        self.base_weight = nn.Parameter(torch.randn(out_features, in_features) * 0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x_exp = x.unsqueeze(1).unsqueeze(-1)\n",
    "        grid = self.grid.unsqueeze(0)\n",
    "        basis = torch.exp(-torch.abs(x_exp - grid) ** 2 / 0.5)\n",
    "        coef = self.coef.unsqueeze(0)\n",
    "        spline_out = (basis * coef).sum(dim=-1).sum(dim=-1)\n",
    "        base_out = torch.matmul(x, self.base_weight.t())\n",
    "        return spline_out + base_out\n",
    "\n",
    "\n",
    "class AttentionKAN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, grid_size=5):\n",
    "        super(AttentionKAN, self).__init__()\n",
    "        self.attention = FeatureAttention(input_dim)\n",
    "        self.kan1 = KANLinear(input_dim, hidden_dim, grid_size)\n",
    "        self.kan2 = KANLinear(hidden_dim, hidden_dim // 2, grid_size)\n",
    "        self.output = nn.Linear(hidden_dim // 2, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x, return_attention=False):\n",
    "        x, att_weights = self.attention(x)\n",
    "        x = self.kan1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.kan2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        if return_attention:\n",
    "            return x, att_weights\n",
    "        return x\n",
    "    \n",
    "    def get_feature_importance(self, X):\n",
    "        self.eval()\n",
    "        if not isinstance(X, torch.Tensor):\n",
    "            X = torch.FloatTensor(X)\n",
    "        device = next(self.parameters()).device\n",
    "        X = X.to(device)\n",
    "        with torch.no_grad():\n",
    "            _, att_weights = self.attention(X)\n",
    "            importance = att_weights.cpu().numpy().mean(axis=0)\n",
    "        return importance\n",
    "\n",
    "print(\"[INFO] Attention-KAN ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY 2: FOCAL LOSS\n",
    "# ============================================================================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for class imbalance - focuses on hard examples\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.25, gamma=2.0, pos_weight=3.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha  # Weight for positive class\n",
    "        self.gamma = gamma  # Focusing parameter (higher = more focus on hard examples)\n",
    "        self.pos_weight = pos_weight  # Additional weight for positive class\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        # BCE loss\n",
    "        bce = nn.functional.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        \n",
    "        # Focal term: (1 - pt)^gamma\n",
    "        pt = torch.exp(-bce)  # pt = p if y=1, else 1-p\n",
    "        focal = (1 - pt) ** self.gamma * bce\n",
    "        \n",
    "        # Alpha weighting\n",
    "        alpha_weight = targets * self.alpha + (1 - targets) * (1 - self.alpha)\n",
    "        focal = alpha_weight * focal\n",
    "        \n",
    "        # Extra weight for positive class (FN penalty)\n",
    "        pos_mask = targets == 1\n",
    "        focal[pos_mask] *= self.pos_weight\n",
    "        \n",
    "        return focal.mean()\n",
    "\n",
    "print(\"[INFO] Focal Loss ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPROVED TRAINING (with loss selection)\n",
    "# ============================================================================\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, \n",
    "                lr=0.01, epochs=30, batch_size=32, \n",
    "                pos_weight=3.0, loss_type='focal'):\n",
    "    \"\"\"\n",
    "    Train with selectable loss function\n",
    "    \n",
    "    loss_type: 'weighted_bce' or 'focal'\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    X_train_t = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_t = torch.FloatTensor(y_train).unsqueeze(1).to(device)\n",
    "    X_val_t = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_t = torch.FloatTensor(y_val).unsqueeze(1).to(device)\n",
    "    \n",
    "    dataset = TensorDataset(X_train_t, y_train_t)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Select loss function\n",
    "    if loss_type == 'weighted_bce':\n",
    "        pos_weight_tensor = torch.tensor([pos_weight]).to(device)\n",
    "        criterion = nn.BCELoss()\n",
    "        print(f\"  [LOSS] Weighted BCE (pos_weight={pos_weight:.2f})\")\n",
    "    else:  # focal\n",
    "        criterion = FocalLoss(alpha=0.25, gamma=2.0, pos_weight=pos_weight)\n",
    "        print(f\"  [LOSS] Focal Loss (gamma=2.0, pos_weight={pos_weight:.2f})\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_recall = 0\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            \n",
    "            # Apply pos_weight manually for weighted BCE\n",
    "            if loss_type == 'weighted_bce':\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                # Manual weighting\n",
    "                weights = torch.ones_like(batch_y)\n",
    "                weights[batch_y == 1] = pos_weight\n",
    "                loss = (loss * weights).mean()\n",
    "            else:\n",
    "                loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(X_val_t)\n",
    "            val_pred = (val_out > 0.5).float().cpu().numpy()\n",
    "            val_recall = recall_score(y_val, val_pred, zero_division=0)\n",
    "        \n",
    "        if val_recall > best_recall:\n",
    "            best_recall = val_recall\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "    \n",
    "    print(f\"  [TRAINING] Best val recall: {best_recall:.4f}\")\n",
    "    return model\n",
    "\n",
    "print(\"[INFO] Training functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY 1: THRESHOLD OPTIMIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def find_optimal_threshold(model, X_val, y_val, target_recall=0.85):\n",
    "    \"\"\"\n",
    "    Find optimal threshold on validation set\n",
    "    \n",
    "    Strategy: Find threshold that achieves target_recall with best F1\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    X_val_t = torch.FloatTensor(X_val).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_prob = model(X_val_t).cpu().numpy().flatten()\n",
    "    \n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0\n",
    "    best_metrics = {}\n",
    "    \n",
    "    print(f\"\\n  [THRESHOLD] Finding optimal threshold (target recall ‚â•{target_recall})...\")\n",
    "    \n",
    "    # Try different thresholds\n",
    "    for threshold in np.arange(0.1, 0.7, 0.05):\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "        \n",
    "        recall = recall_score(y_val, y_pred, zero_division=0)\n",
    "        precision = precision_score(y_val, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_val, y_pred, zero_division=0)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        \n",
    "        # If recall meets target and F1 is better\n",
    "        if recall >= target_recall and f1 > best_f1:\n",
    "            best_threshold = threshold\n",
    "            best_f1 = f1\n",
    "            best_metrics = {\n",
    "                'threshold': threshold,\n",
    "                'recall': recall,\n",
    "                'precision': precision,\n",
    "                'f1': f1,\n",
    "                'accuracy': accuracy\n",
    "            }\n",
    "    \n",
    "    # If no threshold achieves target, use one with highest recall\n",
    "    if best_f1 == 0:\n",
    "        for threshold in np.arange(0.1, 0.7, 0.05):\n",
    "            y_pred = (y_prob >= threshold).astype(int)\n",
    "            recall = recall_score(y_val, y_pred, zero_division=0)\n",
    "            if recall > best_metrics.get('recall', 0):\n",
    "                best_threshold = threshold\n",
    "                best_metrics = {\n",
    "                    'threshold': threshold,\n",
    "                    'recall': recall,\n",
    "                    'precision': precision_score(y_val, y_pred, zero_division=0),\n",
    "                    'f1': f1_score(y_val, y_pred, zero_division=0),\n",
    "                    'accuracy': accuracy_score(y_val, y_pred)\n",
    "                }\n",
    "    \n",
    "    print(f\"  [THRESHOLD] Optimal: {best_threshold:.2f}\")\n",
    "    print(f\"  [THRESHOLD] Val Recall: {best_metrics['recall']:.4f}, Precision: {best_metrics['precision']:.4f}, F1: {best_metrics['f1']:.4f}\")\n",
    "    \n",
    "    return best_threshold\n",
    "\n",
    "print(\"[INFO] Threshold optimization ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EVALUATION (with custom threshold)\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate(model, X_test, y_test, threshold=0.5):\n",
    "    \"\"\"Evaluate with custom threshold\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    X_test_t = torch.FloatTensor(X_test).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_prob = model(X_test_t).cpu().numpy().flatten()\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"\\n  [CONFUSION MATRIX]\")\n",
    "    print(f\"  TN: {cm[0,0]}, FP: {cm[0,1]}\")\n",
    "    print(f\"  FN: {cm[1,0]}, TP: {cm[1,1]}\")\n",
    "    \n",
    "    return {\n",
    "        'Threshold': threshold,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'F1': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'F2': fbeta_score(y_test, y_pred, beta=2, zero_division=0),\n",
    "        'AUC': roc_auc_score(y_test, y_prob) if len(np.unique(y_test)) > 1 else 0\n",
    "    }\n",
    "\n",
    "print(\"[INFO] Evaluation ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_arff(file_path):\n",
    "    try:\n",
    "        data, meta = arff.loadarff(file_path)\n",
    "        df = pd.DataFrame(data)\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == object:\n",
    "                try:\n",
    "                    df[col] = df[col].str.decode('utf-8')\n",
    "                except:\n",
    "                    pass\n",
    "        return df\n",
    "    except:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "        data_start = content.lower().find('@data')\n",
    "        data_section = content[data_start + 5:].strip()\n",
    "        return pd.read_csv(StringIO(data_section), header=None)\n",
    "\n",
    "\n",
    "def prepare_data(df):\n",
    "    X = df.iloc[:, :-1].values.astype(np.float32)\n",
    "    y = df.iloc[:, -1].values\n",
    "    \n",
    "    if y.dtype == object or y.dtype.name.startswith('str'):\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "    else:\n",
    "        y = y.astype(np.int32)\n",
    "    \n",
    "    if np.any(np.isnan(X)):\n",
    "        col_median = np.nanmedian(X, axis=0)\n",
    "        inds = np.where(np.isnan(X))\n",
    "        X[inds] = np.take(col_median, inds[1])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    try:\n",
    "        smote = SMOTE(sampling_strategy=0.8, random_state=RANDOM_SEED)\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        print(f\"[INFO] After SMOTE: {X_train.shape[0]} samples\")\n",
    "    except:\n",
    "        print(\"[WARNING] SMOTE failed\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "print(\"[INFO] Data loading ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY 3+4: GWO WITH pos_weight + MINI ENSEMBLE\n",
    "# ============================================================================\n",
    "\n",
    "class ImprovedGWO:\n",
    "    \"\"\"GWO with 4 parameters: hidden_dim, grid_size, lr, pos_weight\"\"\"\n",
    "    \n",
    "    def __init__(self, bounds, fitness_func, n_wolves=6, n_iter=8):\n",
    "        self.bounds = np.array(bounds)\n",
    "        self.fitness_func = fitness_func\n",
    "        self.n_wolves = n_wolves\n",
    "        self.n_iter = n_iter\n",
    "        self.dim = len(bounds)\n",
    "        \n",
    "        self.positions = np.random.uniform(\n",
    "            self.bounds[:, 0], \n",
    "            self.bounds[:, 1],\n",
    "            size=(n_wolves, self.dim)\n",
    "        )\n",
    "        \n",
    "        self.alpha_pos = np.zeros(self.dim)\n",
    "        self.alpha_score = float('-inf')\n",
    "        self.beta_pos = np.zeros(self.dim)\n",
    "        self.beta_score = float('-inf')\n",
    "        self.delta_pos = np.zeros(self.dim)\n",
    "        self.delta_score = float('-inf')\n",
    "    \n",
    "    def optimize(self):\n",
    "        print(f\"  [GWO] {self.n_wolves} wolves, {self.n_iter} iterations, {self.dim} parameters\")\n",
    "        \n",
    "        for it in range(self.n_iter):\n",
    "            for i in range(self.n_wolves):\n",
    "                fitness = self.fitness_func(self.positions[i])\n",
    "                \n",
    "                if fitness > self.alpha_score:\n",
    "                    self.delta_score = self.beta_score\n",
    "                    self.delta_pos = self.beta_pos.copy()\n",
    "                    self.beta_score = self.alpha_score\n",
    "                    self.beta_pos = self.alpha_pos.copy()\n",
    "                    self.alpha_score = fitness\n",
    "                    self.alpha_pos = self.positions[i].copy()\n",
    "                elif fitness > self.beta_score:\n",
    "                    self.delta_score = self.beta_score\n",
    "                    self.delta_pos = self.beta_pos.copy()\n",
    "                    self.beta_score = fitness\n",
    "                    self.beta_pos = self.positions[i].copy()\n",
    "                elif fitness > self.delta_score:\n",
    "                    self.delta_score = fitness\n",
    "                    self.delta_pos = self.positions[i].copy()\n",
    "            \n",
    "            a = 2 - it * (2.0 / self.n_iter)\n",
    "            \n",
    "            for i in range(self.n_wolves):\n",
    "                for j in range(self.dim):\n",
    "                    r1, r2 = np.random.random(2)\n",
    "                    A1 = 2 * a * r1 - a\n",
    "                    C1 = 2 * r2\n",
    "                    D_alpha = abs(C1 * self.alpha_pos[j] - self.positions[i, j])\n",
    "                    X1 = self.alpha_pos[j] - A1 * D_alpha\n",
    "                    \n",
    "                    r1, r2 = np.random.random(2)\n",
    "                    A2 = 2 * a * r1 - a\n",
    "                    C2 = 2 * r2\n",
    "                    D_beta = abs(C2 * self.beta_pos[j] - self.positions[i, j])\n",
    "                    X2 = self.beta_pos[j] - A2 * D_beta\n",
    "                    \n",
    "                    r1, r2 = np.random.random(2)\n",
    "                    A3 = 2 * a * r1 - a\n",
    "                    C3 = 2 * r2\n",
    "                    D_delta = abs(C3 * self.delta_pos[j] - self.positions[i, j])\n",
    "                    X3 = self.delta_pos[j] - A3 * D_delta\n",
    "                    \n",
    "                    self.positions[i, j] = (X1 + X2 + X3) / 3.0\n",
    "                    self.positions[i, j] = np.clip(\n",
    "                        self.positions[i, j],\n",
    "                        self.bounds[j, 0],\n",
    "                        self.bounds[j, 1]\n",
    "                    )\n",
    "            \n",
    "            print(f\"  Iter {it+1}/{self.n_iter} | Best: {self.alpha_score:.4f}\")\n",
    "        \n",
    "        return self.alpha_pos, self.alpha_score\n",
    "\n",
    "\n",
    "def train_mini_ensemble(X_train, y_train, X_val, y_val, input_dim, \n",
    "                       hidden_dim, grid_size, lr, pos_weight, loss_type='focal'):\n",
    "    \"\"\"Train 2-model ensemble with different random seeds\"\"\"\n",
    "    \n",
    "    models = []\n",
    "    seeds = [42, 123]\n",
    "    \n",
    "    print(f\"\\n  [ENSEMBLE] Training 2 models...\")\n",
    "    \n",
    "    for i, seed in enumerate(seeds):\n",
    "        print(f\"\\n  Model {i+1}/2 (seed={seed})\")\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        model = AttentionKAN(input_dim, hidden_dim, grid_size)\n",
    "        model = train_model(model, X_train, y_train, X_val, y_val,\n",
    "                          lr=lr, epochs=30, pos_weight=pos_weight, loss_type=loss_type)\n",
    "        models.append(model)\n",
    "    \n",
    "    # Reset seed\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    \n",
    "    return models\n",
    "\n",
    "\n",
    "def ensemble_predict(models, X_test, threshold=0.5):\n",
    "    \"\"\"Soft voting prediction\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    X_test_t = torch.FloatTensor(X_test).to(device)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(X_test_t).cpu().numpy().flatten()\n",
    "            predictions.append(pred)\n",
    "    \n",
    "    # Average probabilities\n",
    "    avg_prob = np.mean(predictions, axis=0)\n",
    "    y_pred = (avg_prob >= threshold).astype(int)\n",
    "    \n",
    "    return y_pred, avg_prob\n",
    "\n",
    "print(\"[INFO] GWO and Ensemble ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def plot_feature_importance(model, X_data, dataset_name, top_k=15):\n",
    "    importance = model.get_feature_importance(X_data)\n",
    "    feature_names = [f'F{i}' for i in range(len(importance))]\n",
    "    \n",
    "    sorted_idx = np.argsort(importance)[::-1][:top_k]\n",
    "    top_importance = importance[sorted_idx]\n",
    "    top_names = [feature_names[i] for i in sorted_idx]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    colors = plt.cm.viridis(top_importance / top_importance.max())\n",
    "    bars = ax.barh(range(len(top_importance)), top_importance, color=colors)\n",
    "    ax.set_yticks(range(len(top_importance)))\n",
    "    ax.set_yticklabels(top_names)\n",
    "    ax.set_xlabel('Attention Weight', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{dataset_name}: Top {top_k} Important Features', fontsize=14, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    for i, v in enumerate(top_importance):\n",
    "        ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{dataset_name}_improved_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"[INFO] Saved: {dataset_name}_improved_importance.png\")\n",
    "\n",
    "print(\"[INFO] Visualization ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MAIN EXECUTION - ALL 4 STRATEGIES COMBINED\n",
    "# ============================================================================\n",
    "\n",
    "def run_improved_experiment(dataset_dir='/content/drive/MyDrive/nasa-defect-gwo-kan/dataset'):\n",
    "    \n",
    "    target = ['PC1', 'CM1', 'KC1']\n",
    "    files = glob.glob(os.path.join(dataset_dir, '*.arff'))\n",
    "    files = [f for f in files if any(ds in os.path.basename(f).upper() for ds in target)]\n",
    "    \n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Datasets not found\")\n",
    "    \n",
    "    print(f\"\\n[INFO] Found {len(files)} datasets\\n\")\n",
    "    results = []\n",
    "    \n",
    "    for file_path in files:\n",
    "        dataset_name = os.path.basename(file_path).replace('.arff', '')\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(f\"DATASET: {dataset_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        try:\n",
    "            # Load\n",
    "            print(\"[1/6] Loading data...\")\n",
    "            df = load_arff(file_path)\n",
    "            X_train, X_test, y_train, y_test = prepare_data(df)\n",
    "            input_dim = X_train.shape[1]\n",
    "            \n",
    "            # Val split\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X_train, y_train, test_size=0.2, stratify=y_train, random_state=RANDOM_SEED\n",
    "            )\n",
    "            \n",
    "            print(f\"[INFO] Features: {input_dim}, Train: {len(y_train)}, Val: {len(y_val)}, Test: {len(y_test)}\")\n",
    "            \n",
    "            # GWO with 4 parameters\n",
    "            print(\"\\n[2/6] GWO optimization (4 params: hidden, grid, lr, pos_weight)...\")\n",
    "            \n",
    "            def fitness(params):\n",
    "                hidden_dim = int(params[0])\n",
    "                grid_size = int(params[1])\n",
    "                lr = params[2]\n",
    "                pos_weight = params[3]\n",
    "                \n",
    "                try:\n",
    "                    model = AttentionKAN(input_dim, hidden_dim, grid_size)\n",
    "                    model = train_model(model, X_train, y_train, X_val, y_val,\n",
    "                                      lr=lr, epochs=20, pos_weight=pos_weight, loss_type='focal')\n",
    "                    \n",
    "                    # Find threshold\n",
    "                    threshold = find_optimal_threshold(model, X_val, y_val, target_recall=0.85)\n",
    "                    \n",
    "                    # Evaluate\n",
    "                    metrics = evaluate(model, X_val, y_val, threshold=threshold)\n",
    "                    \n",
    "                    # Fitness: 50% Recall + 30% F1 + 20% Accuracy\n",
    "                    score = 0.5 * metrics['Recall'] + 0.3 * metrics['F1'] + 0.2 * metrics['Accuracy']\n",
    "                    return score\n",
    "                except Exception as e:\n",
    "                    print(f\"  [WARNING] Fitness failed: {e}\")\n",
    "                    return 0.0\n",
    "            \n",
    "            bounds = [\n",
    "                (32, 96),      # hidden_dim\n",
    "                (3, 7),        # grid_size\n",
    "                (0.005, 0.02), # learning_rate\n",
    "                (2.0, 5.0)     # pos_weight (STRATEGY 3!)\n",
    "            ]\n",
    "            \n",
    "            gwo = ImprovedGWO(bounds, fitness, n_wolves=6, n_iter=8)\n",
    "            best_params, best_score = gwo.optimize()\n",
    "            \n",
    "            hidden_dim = int(best_params[0])\n",
    "            grid_size = int(best_params[1])\n",
    "            lr = best_params[2]\n",
    "            pos_weight = best_params[3]\n",
    "            \n",
    "            print(f\"\\n  [GWO] Best: hidden={hidden_dim}, grid={grid_size}, lr={lr:.4f}, pos_weight={pos_weight:.2f}\")\n",
    "            print(f\"  [GWO] Score: {best_score:.4f}\")\n",
    "            \n",
    "            # Train mini ensemble (STRATEGY 4)\n",
    "            print(\"\\n[3/6] Training mini ensemble (2 models)...\")\n",
    "            models = train_mini_ensemble(X_train, y_train, X_val, y_val, input_dim,\n",
    "                                        hidden_dim, grid_size, lr, pos_weight, loss_type='focal')\n",
    "            \n",
    "            # Find optimal threshold on ensemble (STRATEGY 1)\n",
    "            print(\"\\n[4/6] Finding optimal threshold for ensemble...\")\n",
    "            \n",
    "            # Get ensemble validation predictions\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            X_val_t = torch.FloatTensor(X_val).to(device)\n",
    "            \n",
    "            val_preds = []\n",
    "            for model in models:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    pred = model(X_val_t).cpu().numpy().flatten()\n",
    "                    val_preds.append(pred)\n",
    "            \n",
    "            val_avg_prob = np.mean(val_preds, axis=0)\n",
    "            \n",
    "            # Find best threshold\n",
    "            best_threshold = 0.5\n",
    "            best_f1 = 0\n",
    "            target_recall = 0.85\n",
    "            \n",
    "            for threshold in np.arange(0.1, 0.7, 0.05):\n",
    "                y_pred = (val_avg_prob >= threshold).astype(int)\n",
    "                recall = recall_score(y_val, y_pred, zero_division=0)\n",
    "                f1 = f1_score(y_val, y_pred, zero_division=0)\n",
    "                \n",
    "                if recall >= target_recall and f1 > best_f1:\n",
    "                    best_threshold = threshold\n",
    "                    best_f1 = f1\n",
    "            \n",
    "            print(f\"  [THRESHOLD] Ensemble optimal: {best_threshold:.2f}\")\n",
    "            \n",
    "            # Test ensemble\n",
    "            print(\"\\n[5/6] Testing ensemble...\")\n",
    "            y_pred, y_prob = ensemble_predict(models, X_test, threshold=best_threshold)\n",
    "            \n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            print(f\"\\n  [CONFUSION MATRIX]\")\n",
    "            print(f\"  TN: {cm[0,0]}, FP: {cm[0,1]}\")\n",
    "            print(f\"  FN: {cm[1,0]}, TP: {cm[1,1]}\")\n",
    "            \n",
    "            metrics = {\n",
    "                'Accuracy': accuracy_score(y_test, y_pred),\n",
    "                'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "                'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "                'F1': f1_score(y_test, y_pred, zero_division=0),\n",
    "                'F2': fbeta_score(y_test, y_pred, beta=2, zero_division=0),\n",
    "                'AUC': roc_auc_score(y_test, y_prob) if len(np.unique(y_test)) > 1 else 0\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n  [RESULTS]\")\n",
    "            for k, v in metrics.items():\n",
    "                print(f\"  {k}: {v:.4f}\")\n",
    "            \n",
    "            # Visualize\n",
    "            print(\"\\n[6/6] Creating feature importance heatmap...\")\n",
    "            plot_feature_importance(models[0], X_test, dataset_name, top_k=15)\n",
    "            \n",
    "            results.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Features': input_dim,\n",
    "                'Hidden_Dim': hidden_dim,\n",
    "                'Grid_Size': grid_size,\n",
    "                'Learning_Rate': lr,\n",
    "                'Pos_Weight': pos_weight,\n",
    "                'Threshold': best_threshold,\n",
    "                **metrics\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n  [ERROR] {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Summary\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    avg_row = {'Dataset': 'AVERAGE'}\n",
    "    for col in ['Accuracy', 'Precision', 'Recall', 'F1', 'F2', 'AUC']:\n",
    "        if col in results_df.columns:\n",
    "            avg_row[col] = results_df[col].mean()\n",
    "    \n",
    "    results_df = pd.concat([results_df, pd.DataFrame([avg_row])], ignore_index=True)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "print(\"[INFO] Main execution ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RUN!\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" üöÄ IMPROVED ATTENTION-KAN - 4 STRATEGIES COMBINED\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüìã IMPROVEMENTS:\")\n",
    "print(\"  1Ô∏è‚É£ Threshold Optimization (not fixed at 0.5)\")\n",
    "print(\"  2Ô∏è‚É£ Focal Loss (better than Weighted BCE)\")\n",
    "print(\"  3Ô∏è‚É£ pos_weight GWO tuning (4th parameter)\")\n",
    "print(\"  4Ô∏è‚É£ Mini Ensemble (2 models, soft voting)\")\n",
    "print(\"\\nüéØ TARGET:\")\n",
    "print(\"  Recall ‚â• 85% | Accuracy ‚â• 70% | Precision ‚â• 40% | F1 ‚â• 50%\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Run\n",
    "results = run_improved_experiment(\n",
    "    dataset_dir='/content/drive/MyDrive/nasa-defect-gwo-kan/dataset'\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" üìä FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# Save\n",
    "results.to_excel('improved_attention_kan_results.xlsx', index=False)\n",
    "print(\"\\n[INFO] Saved: improved_attention_kan_results.xlsx\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" üéØ AVERAGE METRICS\")\n",
    "print(\"=\"*70)\n",
    "avg = results[results['Dataset'] == 'AVERAGE'].iloc[0]\n",
    "print(f\"\\n  Accuracy:  {avg['Accuracy']:.4f} {'‚úÖ' if avg['Accuracy'] >= 0.70 else '‚ùå'}\")\n",
    "print(f\"  Precision: {avg['Precision']:.4f} {'‚úÖ' if avg['Precision'] >= 0.40 else '‚ùå'}\")\n",
    "print(f\"  Recall:    {avg['Recall']:.4f} {'‚úÖ' if avg['Recall'] >= 0.85 else '‚ùå'} ‚≠ê\")\n",
    "print(f\"  F1-Score:  {avg['F1']:.4f} {'‚úÖ' if avg['F1'] >= 0.50 else '‚ùå'}\")\n",
    "print(f\"  F2-Score:  {avg['F2']:.4f}\")\n",
    "print(f\"  AUC:       {avg['AUC']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" ‚úÖ COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPARISON VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "fig.suptitle('Improved Attention-KAN Results (4 Strategies)', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'F2', 'AUC']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c']\n",
    "\n",
    "plot_data = results[results['Dataset'] != 'AVERAGE'].copy()\n",
    "\n",
    "for idx, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    if metric in plot_data.columns:\n",
    "        bars = ax.barh(plot_data['Dataset'], plot_data[metric], color=color, alpha=0.7)\n",
    "        ax.set_xlabel(metric, fontsize=11, fontweight='bold')\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add target line\n",
    "        if metric == 'Recall':\n",
    "            ax.axvline(x=0.85, color='red', linestyle='--', linewidth=2, label='Target')\n",
    "            ax.set_facecolor('#ffe6e6')\n",
    "            ax.set_title('‚≠ê PRIMARY ‚≠ê', fontsize=10, color='red')\n",
    "        elif metric == 'Accuracy':\n",
    "            ax.axvline(x=0.70, color='blue', linestyle='--', linewidth=2, alpha=0.5)\n",
    "        elif metric == 'Precision':\n",
    "            ax.axvline(x=0.40, color='orange', linestyle='--', linewidth=2, alpha=0.5)\n",
    "        elif metric == 'F1':\n",
    "            ax.axvline(x=0.50, color='purple', linestyle='--', linewidth=2, alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('improved_results_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"[INFO] Saved: improved_results_comparison.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
