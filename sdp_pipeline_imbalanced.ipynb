{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Defect Prediction - Meta-Heuristic Feature Selection Pipeline\n",
    "## NASA MDP D'' Dataset - Imbalanced Classification\n",
    "\n",
    "**Author:** Complete Reproducible Experiment\n",
    "\n",
    "**Strategy:**\n",
    "- âœ… RepeatedStratifiedKFold (10x5)\n",
    "- âœ… MCC as Primary Metric\n",
    "- âœ… 3 Baseline Models (RF, XGB, CatBoost)\n",
    "- âœ… Novel: Binary Grey Wolf Optimizer for Feature Selection\n",
    "- âœ… Statistical Testing (Wilcoxon)\n",
    "- âœ… Complete Reproducibility (fixed seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS AND DEPENDENCIES\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from scipy.stats import wilcoxon\n",
    "from io import StringIO\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    matthews_corrcoef, f1_score, recall_score, roc_auc_score,\n",
    "    make_scorer, confusion_matrix\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Fixed random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"[INFO] All dependencies loaded successfully!\")\n",
    "print(f\"[INFO] Random seed: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GOOGLE DRIVE MOUNTING\n",
    "# ============================================================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"âœ… Google Drive mounted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_arff_data(file_path):\n",
    "    \"\"\"Load ARFF file and convert to DataFrame\"\"\"\n",
    "    try:\n",
    "        data, meta = arff.loadarff(file_path)\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Decode byte strings\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == object:\n",
    "                try:\n",
    "                    df[col] = df[col].str.decode('utf-8')\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] scipy.io.arff failed: {e}\")\n",
    "        # Fallback: manual parsing\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "        data_start = content.lower().find('@data')\n",
    "        data_section = content[data_start + 5:].strip()\n",
    "        df = pd.read_csv(StringIO(data_section), header=None)\n",
    "        return df\n",
    "\n",
    "\n",
    "def preprocess_dataset(df, target_col='defective'):\n",
    "    \"\"\"\n",
    "    Preprocess dataset:\n",
    "    - Separate features and target\n",
    "    - Handle missing values\n",
    "    - Convert target to binary (0/1)\n",
    "    \"\"\"\n",
    "    # Assume last column is target\n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[:, -1].values\n",
    "    \n",
    "    # Convert to numeric\n",
    "    X = X.astype(np.float32)\n",
    "    \n",
    "    # Handle missing values (median imputation)\n",
    "    if np.any(np.isnan(X)):\n",
    "        col_median = np.nanmedian(X, axis=0)\n",
    "        inds = np.where(np.isnan(X))\n",
    "        X[inds] = np.take(col_median, inds[1])\n",
    "    \n",
    "    # Convert target to binary (0 = non-defective, 1 = defective)\n",
    "    if y.dtype == object or y.dtype.name.startswith('str'):\n",
    "        # Map 'true'/'false' or 'Y'/'N' to 1/0\n",
    "        y_clean = []\n",
    "        for val in y:\n",
    "            if isinstance(val, bytes):\n",
    "                val = val.decode('utf-8')\n",
    "            val = str(val).strip().lower()\n",
    "            if val in ['true', 'yes', 'y', '1']:\n",
    "                y_clean.append(1)\n",
    "            else:\n",
    "                y_clean.append(0)\n",
    "        y = np.array(y_clean, dtype=np.int32)\n",
    "    else:\n",
    "        y = y.astype(np.int32)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def load_datasets(dataset_dir):\n",
    "    \"\"\"\n",
    "    Load all ARFF datasets from directory\n",
    "    Returns: list of (dataset_name, X, y) tuples\n",
    "    \"\"\"\n",
    "    arff_files = glob.glob(os.path.join(dataset_dir, '*.arff'))\n",
    "    \n",
    "    if not arff_files:\n",
    "        raise FileNotFoundError(f\"No ARFF files found in {dataset_dir}\")\n",
    "    \n",
    "    datasets = []\n",
    "    \n",
    "    for file_path in arff_files:\n",
    "        dataset_name = os.path.basename(file_path).replace('.arff', '')\n",
    "        \n",
    "        try:\n",
    "            df = load_arff_data(file_path)\n",
    "            X, y = preprocess_dataset(df)\n",
    "            \n",
    "            print(f\"[LOADED] {dataset_name}: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "            print(f\"         Class distribution: {np.bincount(y)}\")\n",
    "            \n",
    "            datasets.append((dataset_name, X, y))\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load {dataset_name}: {e}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "\n",
    "print(\"[INFO] Data loading functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# METRICS FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_pred_proba=None):\n",
    "    \"\"\"\n",
    "    Calculate all required metrics\n",
    "    \n",
    "    Returns:\n",
    "        dict: {MCC, F1, Recall, AUC}\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # MCC (Primary Metric)\n",
    "    metrics['MCC'] = matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    # F1-Score\n",
    "    metrics['F1'] = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # Recall\n",
    "    metrics['Recall'] = recall_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # ROC-AUC (if probabilities provided)\n",
    "    if y_pred_proba is not None:\n",
    "        try:\n",
    "            # Handle binary classification\n",
    "            if len(np.unique(y_true)) > 1:\n",
    "                metrics['AUC'] = roc_auc_score(y_true, y_pred_proba)\n",
    "            else:\n",
    "                metrics['AUC'] = 0.0\n",
    "        except:\n",
    "            metrics['AUC'] = 0.0\n",
    "    else:\n",
    "        metrics['AUC'] = 0.0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"[INFO] Metrics function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BASELINE MODELS\n",
    "# ============================================================================\n",
    "\n",
    "def create_baseline_models(n_positive, n_negative):\n",
    "    \"\"\"\n",
    "    Create baseline models with imbalance handling\n",
    "    \n",
    "    Args:\n",
    "        n_positive: number of positive samples\n",
    "        n_negative: number of negative samples\n",
    "    \n",
    "    Returns:\n",
    "        dict: {model_name: model_instance}\n",
    "    \"\"\"\n",
    "    # Calculate scale_pos_weight for XGBoost\n",
    "    scale_pos_weight = n_negative / n_positive if n_positive > 0 else 1.0\n",
    "    \n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            class_weight='balanced',\n",
    "            random_state=RANDOM_SEED,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'XGBoost': XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_state=RANDOM_SEED,\n",
    "            eval_metric='logloss',\n",
    "            use_label_encoder=False\n",
    "        ),\n",
    "        \n",
    "        'CatBoost': CatBoostClassifier(\n",
    "            iterations=100,\n",
    "            auto_class_weights='Balanced',\n",
    "            random_seed=RANDOM_SEED,\n",
    "            logging_level='Silent',\n",
    "            allow_writing_files=False\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return models\n",
    "\n",
    "\n",
    "print(\"[INFO] Baseline models defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BINARY GREY WOLF OPTIMIZER (BGWO)\n",
    "# ============================================================================\n",
    "\n",
    "class BinaryGreyWolfOptimizer:\n",
    "    \"\"\"\n",
    "    Binary Grey Wolf Optimizer for Feature Selection\n",
    "    \n",
    "    Optimizes feature subset using GWO with sigmoid transfer function\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_wolves=10, n_iterations=20, random_state=42):\n",
    "        self.n_wolves = n_wolves\n",
    "        self.n_iterations = n_iterations\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        # Best solutions\n",
    "        self.alpha_pos = None\n",
    "        self.alpha_score = -float('inf')\n",
    "        self.beta_pos = None\n",
    "        self.beta_score = -float('inf')\n",
    "        self.delta_pos = None\n",
    "        self.delta_score = -float('inf')\n",
    "        \n",
    "        self.convergence_curve = []\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid transfer function\"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-10 * (x - 0.5)))\n",
    "    \n",
    "    def initialize_population(self, n_features):\n",
    "        \"\"\"Initialize binary population\"\"\"\n",
    "        # Start with random feature subsets (30-70% features)\n",
    "        population = np.random.rand(self.n_wolves, n_features) > 0.5\n",
    "        \n",
    "        # Ensure at least 3 features are selected in each wolf\n",
    "        for i in range(self.n_wolves):\n",
    "            if population[i].sum() < 3:\n",
    "                indices = np.random.choice(n_features, 3, replace=False)\n",
    "                population[i, indices] = True\n",
    "        \n",
    "        return population.astype(float)\n",
    "    \n",
    "    def optimize(self, X, y, fitness_func, verbose=True):\n",
    "        \"\"\"\n",
    "        Run BGWO optimization\n",
    "        \n",
    "        Args:\n",
    "            X: feature matrix\n",
    "            y: target vector\n",
    "            fitness_func: function(X_subset, y) -> score\n",
    "            verbose: print progress\n",
    "        \n",
    "        Returns:\n",
    "            best_features: boolean array of selected features\n",
    "            best_score: fitness score\n",
    "        \"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # Initialize population\n",
    "        positions = self.initialize_population(n_features)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n[BGWO] Starting optimization...\")\n",
    "            print(f\"       Wolves: {self.n_wolves}, Iterations: {self.n_iterations}\")\n",
    "        \n",
    "        # Main optimization loop\n",
    "        for iteration in range(self.n_iterations):\n",
    "            # Evaluate fitness for each wolf\n",
    "            for i in range(self.n_wolves):\n",
    "                # Convert to binary mask\n",
    "                feature_mask = positions[i] > 0.5\n",
    "                \n",
    "                # Ensure at least 3 features\n",
    "                if feature_mask.sum() < 3:\n",
    "                    indices = np.random.choice(n_features, 3, replace=False)\n",
    "                    feature_mask[indices] = True\n",
    "                    positions[i] = feature_mask.astype(float)\n",
    "                \n",
    "                # Calculate fitness\n",
    "                try:\n",
    "                    fitness = fitness_func(X[:, feature_mask], y)\n",
    "                except:\n",
    "                    fitness = 0.0\n",
    "                \n",
    "                # Update alpha, beta, delta\n",
    "                if fitness > self.alpha_score:\n",
    "                    self.delta_score = self.beta_score\n",
    "                    self.delta_pos = self.beta_pos.copy() if self.beta_pos is not None else None\n",
    "                    self.beta_score = self.alpha_score\n",
    "                    self.beta_pos = self.alpha_pos.copy() if self.alpha_pos is not None else None\n",
    "                    self.alpha_score = fitness\n",
    "                    self.alpha_pos = positions[i].copy()\n",
    "                elif fitness > self.beta_score:\n",
    "                    self.delta_score = self.beta_score\n",
    "                    self.delta_pos = self.beta_pos.copy() if self.beta_pos is not None else None\n",
    "                    self.beta_score = fitness\n",
    "                    self.beta_pos = positions[i].copy()\n",
    "                elif fitness > self.delta_score:\n",
    "                    self.delta_score = fitness\n",
    "                    self.delta_pos = positions[i].copy()\n",
    "            \n",
    "            # Update coefficient a (linearly decreases from 2 to 0)\n",
    "            a = 2.0 - iteration * (2.0 / self.n_iterations)\n",
    "            \n",
    "            # Update positions\n",
    "            for i in range(self.n_wolves):\n",
    "                for j in range(n_features):\n",
    "                    # Calculate distance to alpha, beta, delta\n",
    "                    r1, r2 = np.random.random(2)\n",
    "                    A1 = 2 * a * r1 - a\n",
    "                    C1 = 2 * r2\n",
    "                    D_alpha = abs(C1 * self.alpha_pos[j] - positions[i, j])\n",
    "                    X1 = self.alpha_pos[j] - A1 * D_alpha\n",
    "                    \n",
    "                    r1, r2 = np.random.random(2)\n",
    "                    A2 = 2 * a * r1 - a\n",
    "                    C2 = 2 * r2\n",
    "                    D_beta = abs(C2 * self.beta_pos[j] - positions[i, j])\n",
    "                    X2 = self.beta_pos[j] - A2 * D_beta\n",
    "                    \n",
    "                    r1, r2 = np.random.random(2)\n",
    "                    A3 = 2 * a * r1 - a\n",
    "                    C3 = 2 * r2\n",
    "                    D_delta = abs(C3 * self.delta_pos[j] - positions[i, j])\n",
    "                    X3 = self.delta_pos[j] - A3 * D_delta\n",
    "                    \n",
    "                    # Update position (average of X1, X2, X3)\n",
    "                    new_pos = (X1 + X2 + X3) / 3.0\n",
    "                    \n",
    "                    # Apply sigmoid and convert to binary\n",
    "                    positions[i, j] = self.sigmoid(new_pos)\n",
    "            \n",
    "            # Store convergence\n",
    "            self.convergence_curve.append(self.alpha_score)\n",
    "            \n",
    "            if verbose and (iteration + 1) % 5 == 0:\n",
    "                n_features_selected = (self.alpha_pos > 0.5).sum()\n",
    "                print(f\"       Iter {iteration + 1}/{self.n_iterations} | \"\n",
    "                      f\"Best MCC: {self.alpha_score:.4f} | \"\n",
    "                      f\"Features: {n_features_selected}/{n_features}\")\n",
    "        \n",
    "        # Final best solution\n",
    "        best_features = self.alpha_pos > 0.5\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n[BGWO] Optimization complete!\")\n",
    "            print(f\"       Best MCC: {self.alpha_score:.4f}\")\n",
    "            print(f\"       Selected: {best_features.sum()}/{n_features} features\")\n",
    "        \n",
    "        return best_features, self.alpha_score\n",
    "\n",
    "\n",
    "print(\"[INFO] Binary GWO implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# META-HEURISTIC FEATURE SELECTION WRAPPER\n",
    "# ============================================================================\n",
    "\n",
    "class MetaHeuristicSelector:\n",
    "    \"\"\"\n",
    "    Meta-Heuristic Feature Selection Wrapper for CatBoost\n",
    "    \n",
    "    Uses Binary GWO to find optimal feature subset,\n",
    "    then trains CatBoost on selected features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_wolves=10, n_iterations=20, random_state=42):\n",
    "        self.n_wolves = n_wolves\n",
    "        self.n_iterations = n_iterations\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.optimizer = None\n",
    "        self.selected_features_ = None\n",
    "        self.model_ = None\n",
    "        self.scaler_ = None\n",
    "    \n",
    "    def _fitness_function(self, X_subset, y):\n",
    "        \"\"\"\n",
    "        Fitness function: MCC score using 3-fold CV\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        \n",
    "        # Quick 3-fold CV for speed\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        mcc_scores = []\n",
    "        \n",
    "        for train_idx, val_idx in cv.split(X_subset, y):\n",
    "            X_train, X_val = X_subset[train_idx], X_subset[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            # Calculate class weights\n",
    "            n_positive = np.sum(y_train == 1)\n",
    "            n_negative = np.sum(y_train == 0)\n",
    "            \n",
    "            # Train CatBoost\n",
    "            model = CatBoostClassifier(\n",
    "                iterations=50,\n",
    "                auto_class_weights='Balanced',\n",
    "                random_seed=self.random_state,\n",
    "                logging_level='Silent',\n",
    "                allow_writing_files=False\n",
    "            )\n",
    "            \n",
    "            model.fit(X_train, y_train, verbose=False)\n",
    "            y_pred = model.predict(X_val)\n",
    "            \n",
    "            # Calculate MCC\n",
    "            mcc = matthews_corrcoef(y_val, y_pred)\n",
    "            mcc_scores.append(mcc)\n",
    "        \n",
    "        # Return mean MCC\n",
    "        return np.mean(mcc_scores)\n",
    "    \n",
    "    def fit(self, X, y, verbose=True):\n",
    "        \"\"\"\n",
    "        Fit the selector:\n",
    "        1. Run BGWO to find best features\n",
    "        2. Train final CatBoost on selected features\n",
    "        \"\"\"\n",
    "        # Standardize features\n",
    "        self.scaler_ = StandardScaler()\n",
    "        X_scaled = self.scaler_.fit_transform(X)\n",
    "        \n",
    "        # Run BGWO\n",
    "        self.optimizer = BinaryGreyWolfOptimizer(\n",
    "            n_wolves=self.n_wolves,\n",
    "            n_iterations=self.n_iterations,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        self.selected_features_, best_score = self.optimizer.optimize(\n",
    "            X_scaled, y,\n",
    "            fitness_func=self._fitness_function,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        # Train final model on selected features\n",
    "        X_selected = X_scaled[:, self.selected_features_]\n",
    "        \n",
    "        self.model_ = CatBoostClassifier(\n",
    "            iterations=100,\n",
    "            auto_class_weights='Balanced',\n",
    "            random_seed=self.random_state,\n",
    "            logging_level='Silent',\n",
    "            allow_writing_files=False\n",
    "        )\n",
    "        \n",
    "        self.model_.fit(X_selected, y, verbose=False)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using selected features\"\"\"\n",
    "        X_scaled = self.scaler_.transform(X)\n",
    "        X_selected = X_scaled[:, self.selected_features_]\n",
    "        return self.model_.predict(X_selected)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities using selected features\"\"\"\n",
    "        X_scaled = self.scaler_.transform(X)\n",
    "        X_selected = X_scaled[:, self.selected_features_]\n",
    "        return self.model_.predict_proba(X_selected)[:, 1]\n",
    "\n",
    "\n",
    "print(\"[INFO] MetaHeuristicSelector implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def run_experiment(dataset_name, X, y, n_splits=10, n_repeats=5):\n",
    "    \"\"\"\n",
    "    Run complete experiment:\n",
    "    - RepeatedStratifiedKFold CV\n",
    "    - 3 Baselines + 1 Novel Approach\n",
    "    - Store all metrics\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"DATASET: {dataset_name}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Samples: {X.shape[0]}, Features: {X.shape[1]}\")\n",
    "    print(f\"Class distribution: {np.bincount(y)}\")\n",
    "    \n",
    "    # Calculate class weights\n",
    "    n_positive = np.sum(y == 1)\n",
    "    n_negative = np.sum(y == 0)\n",
    "    \n",
    "    # Create baseline models\n",
    "    baseline_models = create_baseline_models(n_positive, n_negative)\n",
    "    \n",
    "    # Add novel approach\n",
    "    all_models = baseline_models.copy()\n",
    "    all_models['BGWO-CatBoost'] = MetaHeuristicSelector(\n",
    "        n_wolves=8,\n",
    "        n_iterations=15,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    # Setup CV\n",
    "    cv = RepeatedStratifiedKFold(\n",
    "        n_splits=n_splits,\n",
    "        n_repeats=n_repeats,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Standardize features (for baseline models)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Run CV\n",
    "    fold_idx = 0\n",
    "    total_folds = n_splits * n_repeats\n",
    "    \n",
    "    print(f\"\\nRunning {total_folds} folds ({n_splits} splits Ã— {n_repeats} repeats)...\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        fold_idx += 1\n",
    "        \n",
    "        # Split data\n",
    "        X_train_raw, X_test_raw = X[train_idx], X[test_idx]\n",
    "        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Show progress every 10 folds\n",
    "        if fold_idx % 10 == 0 or fold_idx == 1:\n",
    "            print(f\"Fold {fold_idx}/{total_folds}...\")\n",
    "        \n",
    "        # Test each model\n",
    "        for model_name, model in all_models.items():\n",
    "            try:\n",
    "                # For MetaHeuristic, use raw data (it has its own scaler)\n",
    "                if model_name == 'BGWO-CatBoost':\n",
    "                    # Only show verbose for first fold\n",
    "                    verbose = (fold_idx == 1)\n",
    "                    model.fit(X_train_raw, y_train, verbose=verbose)\n",
    "                    y_pred = model.predict(X_test_raw)\n",
    "                    y_pred_proba = model.predict_proba(X_test_raw)\n",
    "                else:\n",
    "                    # Baseline models use scaled data\n",
    "                    model.fit(X_train, y_train)\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    \n",
    "                    # Get probabilities\n",
    "                    if hasattr(model, 'predict_proba'):\n",
    "                        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "                    else:\n",
    "                        y_pred_proba = y_pred\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics = calculate_metrics(y_test, y_pred, y_pred_proba)\n",
    "                \n",
    "                # Store results\n",
    "                result_row = {\n",
    "                    'Dataset': dataset_name,\n",
    "                    'Model': model_name,\n",
    "                    'Fold': fold_idx,\n",
    "                    'MCC': metrics['MCC'],\n",
    "                    'F1': metrics['F1'],\n",
    "                    'Recall': metrics['Recall'],\n",
    "                    'AUC': metrics['AUC']\n",
    "                }\n",
    "                results.append(result_row)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {model_name} failed on fold {fold_idx}: {e}\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    print(f\"âœ“ Completed {total_folds} folds for {len(all_models)} models\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "print(\"[INFO] Experiment function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# REPORTING & VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def generate_report(results_df):\n",
    "    \"\"\"\n",
    "    Generate summary statistics and visualizations\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Calculate mean and std for each model\n",
    "    summary = results_df.groupby('Model')[['MCC', 'F1', 'Recall', 'AUC']].agg(['mean', 'std'])\n",
    "    \n",
    "    # Flatten column names\n",
    "    summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "    \n",
    "    # Sort by MCC (primary metric)\n",
    "    summary = summary.sort_values('MCC_mean', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + summary.to_string())\n",
    "    \n",
    "    # Boxplot\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GENERATING BOXPLOT...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Boxplot for MCC scores\n",
    "    results_df.boxplot(column='MCC', by='Model', ax=ax, patch_artist=True)\n",
    "    \n",
    "    ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('MCC Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('MCC Score Comparison (Primary Metric)', fontsize=14, fontweight='bold')\n",
    "    plt.suptitle('')  # Remove default title\n",
    "    \n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig('mcc_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ“ Boxplot saved: mcc_comparison.png\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "def statistical_test(results_df, baseline_model, novel_model):\n",
    "    \"\"\"\n",
    "    Perform Wilcoxon Signed-Rank Test\n",
    "    \n",
    "    Tests if novel approach is significantly better than baseline\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STATISTICAL SIGNIFICANCE TEST\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Baseline: {baseline_model}\")\n",
    "    print(f\"Novel:    {novel_model}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Get MCC scores\n",
    "    baseline_scores = results_df[results_df['Model'] == baseline_model]['MCC'].values\n",
    "    novel_scores = results_df[results_df['Model'] == novel_model]['MCC'].values\n",
    "    \n",
    "    # Wilcoxon test\n",
    "    statistic, p_value = wilcoxon(novel_scores, baseline_scores, alternative='greater')\n",
    "    \n",
    "    print(f\"\\nWilcoxon Signed-Rank Test:\")\n",
    "    print(f\"  Statistic: {statistic:.4f}\")\n",
    "    print(f\"  P-value:   {p_value:.6f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(f\"\\nâœ“ SIGNIFICANT: {novel_model} is significantly better (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"\\nâœ— NOT SIGNIFICANT: No significant difference (p >= 0.05)\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return statistic, p_value\n",
    "\n",
    "\n",
    "print(\"[INFO] Reporting functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# MAIN EXECUTION - HER DATASET AYRI AYRI Ä°ÅžLENÄ°R\n# ============================================================================\n\n# Set dataset directory\nDATASET_DIR = '/content/drive/MyDrive/nasa-defect-gwo-kan/dataset'\n\nprint(\"\\n\" + \"#\"*70)\nprint(\"# SOFTWARE DEFECT PREDICTION - META-HEURISTIC PIPELINE\")\nprint(\"# NASA MDP D'' Dataset - Imbalanced Classification\")\nprint(\"# HER DATASET Ä°Ã‡Ä°N AYRI AYRI SONUÃ‡LAR GÃ–STERÄ°LECEK\")\nprint(\"#\"*70)\n\n# Load datasets\nprint(\"\\n[STEP 1] Loading datasets...\")\ndatasets = load_datasets(DATASET_DIR)\n\nprint(f\"\\n[INFO] Loaded {len(datasets)} datasets\")\nprint(f\"[INFO] Her dataset iÃ§in ayrÄ± ayrÄ± iÅŸlem yapÄ±lacak ve sonuÃ§lar gÃ¶sterilecek\")\n\n# Store all results for final summary\nall_results = []\ndataset_summaries = []\n\n# Run experiments on each dataset SEPARATELY\nfor idx, (dataset_name, X, y) in enumerate(datasets, 1):\n    print(\"\\n\" + \"â–ˆ\"*70)\n    print(f\"â–ˆ DATASET {idx}/{len(datasets)}: {dataset_name}\")\n    print(\"â–ˆ\"*70)\n\n    # Skip very small datasets\n    if X.shape[0] < 100:\n        print(f\"\\n[SKIP] {dataset_name}: Too small ({X.shape[0]} samples)\")\n        print(\"â–ˆ\"*70 + \"\\n\")\n        continue\n\n    # Run experiment for this dataset\n    print(f\"\\n[RUNNING] Experiment baÅŸlatÄ±ldÄ±: {dataset_name}\")\n    results_df = run_experiment(dataset_name, X, y, n_splits=10, n_repeats=5)\n    all_results.append(results_df)\n\n    # IMMEDIATELY show results for THIS dataset\n    print(\"\\n\" + \"â–¼\"*70)\n    print(f\"â–¼ SONUÃ‡LAR: {dataset_name}\")\n    print(\"â–¼\"*70)\n\n    # Calculate summary for this dataset\n    summary = results_df.groupby('Model')[['MCC', 'F1', 'Recall', 'AUC']].agg(['mean', 'std'])\n    summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n    summary = summary.sort_values('MCC_mean', ascending=False)\n\n    # Store for final comparison\n    dataset_summaries.append({\n        'dataset': dataset_name,\n        'summary': summary\n    })\n\n    # Print results\n    print(\"\\nðŸ“Š Ã–ZET Ä°STATÄ°STÄ°KLER:\")\n    print(summary.to_string())\n\n    # Find best model for this dataset\n    best_model = summary.index[0]\n    best_mcc = summary.loc[best_model, 'MCC_mean']\n\n    print(f\"\\nðŸ† EN Ä°YÄ° MODEL: {best_model}\")\n    print(f\"   MCC (mean Â± std): {best_mcc:.4f} Â± {summary.loc[best_model, 'MCC_std']:.4f}\")\n\n    # Show how BGWO-CatBoost performed\n    if 'BGWO-CatBoost' in summary.index:\n        bgwo_mcc = summary.loc['BGWO-CatBoost', 'MCC_mean']\n        bgwo_std = summary.loc['BGWO-CatBoost', 'MCC_std']\n        bgwo_rank = list(summary.index).index('BGWO-CatBoost') + 1\n\n        print(f\"\\nðŸ”¬ BGWO-CatBoost (Novel Method):\")\n        print(f\"   MCC (mean Â± std): {bgwo_mcc:.4f} Â± {bgwo_std:.4f}\")\n        print(f\"   SÄ±ralama: {bgwo_rank}/{len(summary)} model arasÄ±nda\")\n\n    print(\"\\n\" + \"â–ˆ\"*70)\n    print(f\"â–ˆ âœ… TAMAMLANDI: {dataset_name} ({idx}/{len(datasets)})\")\n    print(\"â–ˆ\"*70 + \"\\n\\n\")\n\n# ============================================================================\n# FINAL SUMMARY - ALL DATASETS COMBINED\n# ============================================================================\n\nif len(all_results) > 0:\n    print(\"\\n\" + \"ðŸŽ¯\"*35)\n    print(\"ðŸŽ¯ TÃœM DATASET'LER Ä°Ã‡Ä°N GENEL Ã–ZET\")\n    print(\"ðŸŽ¯\"*35)\n\n    # Combine all results\n    final_results = pd.concat(all_results, ignore_index=True)\n\n    # Overall statistics\n    print(\"\\nðŸ“Š GENEL Ä°STATÄ°STÄ°KLER (TÃ¼m Dataset'ler BirleÅŸtirildi):\")\n    print(\"=\"*70)\n\n    overall_summary = final_results.groupby('Model')[['MCC', 'F1', 'Recall', 'AUC']].agg(['mean', 'std'])\n    overall_summary.columns = ['_'.join(col).strip() for col in overall_summary.columns.values]\n    overall_summary = overall_summary.sort_values('MCC_mean', ascending=False)\n\n    print(overall_summary.to_string())\n\n    # Statistical test (compare best baseline vs novel)\n    print(\"\\n\" + \"=\"*70)\n    print(\"ðŸ“ˆ Ä°STATÄ°STÄ°KSEL TEST\")\n    print(\"=\"*70)\n\n    # Find best baseline (highest mean MCC)\n    baseline_models = ['Random Forest', 'XGBoost', 'CatBoost']\n    baseline_means = {}\n    for model in baseline_models:\n        mean_mcc = final_results[final_results['Model'] == model]['MCC'].mean()\n        baseline_means[model] = mean_mcc\n\n    best_baseline = max(baseline_means, key=baseline_means.get)\n\n    print(f\"\\nEn Ä°yi Baseline: {best_baseline} (MCC: {baseline_means[best_baseline]:.4f})\")\n\n    # Test against novel approach\n    if 'BGWO-CatBoost' in final_results['Model'].values:\n        stat, p_val = statistical_test(final_results, best_baseline, 'BGWO-CatBoost')\n\n    # Generate overall boxplot\n    print(\"\\n\" + \"=\"*70)\n    print(\"ðŸ“Š GENEL BOXPLOT OLUÅžTURULUYOR...\")\n    print(\"=\"*70)\n\n    fig, ax = plt.subplots(figsize=(12, 6))\n    final_results.boxplot(column='MCC', by='Model', ax=ax, patch_artist=True)\n    ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n    ax.set_ylabel('MCC Score', fontsize=12, fontweight='bold')\n    ax.set_title('MCC Score Comparison - All Datasets', fontsize=14, fontweight='bold')\n    plt.suptitle('')\n    ax.grid(axis='y', alpha=0.3)\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    plt.savefig('mcc_comparison_all_datasets.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n    print(\"âœ“ Boxplot kaydedildi: mcc_comparison_all_datasets.png\")\n\n    # Save results\n    print(\"\\n\" + \"=\"*70)\n    print(\"ðŸ’¾ SONUÃ‡LAR KAYDEDÄ°LÄ°YOR...\")\n    print(\"=\"*70)\n\n    final_results.to_csv('sdp_results_detailed_all.csv', index=False)\n    print(\"âœ“ DetaylÄ± sonuÃ§lar: sdp_results_detailed_all.csv\")\n\n    overall_summary.to_csv('sdp_results_summary_all.csv')\n    print(\"âœ“ Genel Ã¶zet: sdp_results_summary_all.csv\")\n\n    # Save individual dataset summaries\n    with open('sdp_results_by_dataset.txt', 'w') as f:\n        f.write(\"=\"*70 + \"\\n\")\n        f.write(\"DATASET'E GÃ–RE SONUÃ‡LAR\\n\")\n        f.write(\"=\"*70 + \"\\n\\n\")\n\n        for item in dataset_summaries:\n            f.write(f\"\\n{'='*70}\\n\")\n            f.write(f\"Dataset: {item['dataset']}\\n\")\n            f.write(f\"{'='*70}\\n\")\n            f.write(item['summary'].to_string())\n            f.write(\"\\n\\n\")\n\n    print(\"âœ“ Dataset bazlÄ± Ã¶zet: sdp_results_by_dataset.txt\")\n\n    print(\"\\n\" + \"ðŸŽ¯\"*35)\n    print(\"ðŸŽ¯ TÃœM DENEYLER TAMAMLANDI!\")\n    print(\"ðŸŽ¯\"*35)\n\n    print(f\"\\nâœ… Ä°ÅŸlenen Dataset SayÄ±sÄ±: {len(all_results)}\")\n    print(f\"âœ… Toplam Fold SayÄ±sÄ±: {len(final_results)}\")\n    print(f\"âœ… Test Edilen Model SayÄ±sÄ±: {len(overall_summary)}\")\n\nelse:\n    print(\"\\n[ERROR] HiÃ§bir dataset iÅŸlenemedi!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}