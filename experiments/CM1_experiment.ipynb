{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ NASA Defect Prediction: CM1\n",
    "\n",
    "**Dataset:** CM1\n",
    "**Method:** Baseline RF â†’ KAN Base â†’ KAN + Attention\n",
    "**Goal:** F2 & Recall optimization (defect detection)\n",
    "\n",
    "**âœ… Self-contained:** Run all cells in order, no dependencies!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Step 1: Setup & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print('âœ… Google Drive mounted!')\n",
    "except ImportError:\n",
    "    print('âš ï¸  Not on Colab - skipping mount')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import sys\n",
    "!{sys.executable} -m pip install imbalanced-learn scipy scikit-learn torch matplotlib seaborn pandas numpy openpyxl -q\n",
    "print('âœ… Packages installed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from io import StringIO\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    fbeta_score, confusion_matrix, average_precision_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print('âœ… Imports complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATASET_NAME = 'CM1'\n",
    "DATASET_PATH = '/content/drive/MyDrive/nasa-defect-gwo-kan/dataset'\n",
    "OUTPUT_DIR = f'./results_{DATASET_NAME}'\n",
    "SEED = 42\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device('cpu')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "RUN_ID = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "print(f'âœ… Configuration complete!')\n",
    "print(f'ðŸ“Š Dataset: CM1')\n",
    "print(f'ðŸ–¥ï¸  Device: {device}')\n",
    "print(f'ðŸ“ Output: {OUTPUT_DIR}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Step 2: Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for data loading and metrics\n",
    "\n",
    "def load_arff(file_path):\n",
    "    \"\"\"Load ARFF file and return pandas DataFrame.\"\"\"\n",
    "    try:\n",
    "        data, _ = arff.loadarff(file_path)\n",
    "        df = pd.DataFrame(data)\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == object:\n",
    "                try:\n",
    "                    df[col] = df[col].str.decode('utf-8')\n",
    "                except:\n",
    "                    pass\n",
    "        return df\n",
    "    except:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "        data_start = content.lower().find('@data')\n",
    "        data_section = content[data_start + 5:].strip()\n",
    "        return pd.read_csv(StringIO(data_section), header=None)\n",
    "\n",
    "def calc_metrics(y_true, y_pred, y_proba=None):\n",
    "    \"\"\"Calculate comprehensive metrics for defect prediction.\"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    m = {\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'f2': fbeta_score(y_true, y_pred, beta=2, zero_division=0),\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'tp': int(tp), 'fp': int(fp), 'tn': int(tn), 'fn': int(fn)\n",
    "    }\n",
    "    if y_proba is not None:\n",
    "        try:\n",
    "            m['pr_auc'] = average_precision_score(y_true, y_proba)\n",
    "        except:\n",
    "            m['pr_auc'] = 0.0\n",
    "    else:\n",
    "        m['pr_auc'] = 0.0\n",
    "    return m\n",
    "\n",
    "def find_threshold(y_true, y_proba):\n",
    "    \"\"\"Find optimal threshold for F2 score.\"\"\"\n",
    "    best_score, best_t = -1, 0.5\n",
    "    for t in np.arange(0.05, 0.96, 0.05):\n",
    "        y_pred = (y_proba >= t).astype(int)\n",
    "        m = calc_metrics(y_true, y_pred)\n",
    "        score = m['f2'] if m['accuracy'] >= 0.5 else 0\n",
    "        if score > best_score:\n",
    "            best_score, best_t = score, t\n",
    "    return best_t\n",
    "\n",
    "print('âœ… Utility functions defined!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Step 3: Define KAN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KANLinear: Core KAN layer with spline-based activation\n",
    "\n",
    "class KANLinear(nn.Module):\n",
    "    \"\"\"Kolmogorov-Arnold Network Linear Layer with learnable spline activations.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, grid_size=3, spline_order=2):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "        \n",
    "        # Learnable grid points\n",
    "        self.grid = nn.Parameter(\n",
    "            torch.linspace(-1, 1, grid_size)\n",
    "            .unsqueeze(0).unsqueeze(0)\n",
    "            .repeat(out_features, in_features, 1)\n",
    "        )\n",
    "        \n",
    "        # Spline coefficients\n",
    "        self.coef = nn.Parameter(\n",
    "            torch.randn(out_features, in_features, grid_size + spline_order) * 0.1\n",
    "        )\n",
    "        \n",
    "        # Base weights\n",
    "        self.base_weight = nn.Parameter(\n",
    "            torch.randn(out_features, in_features) * 0.1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Expand input for broadcasting\n",
    "        x_expanded = x.unsqueeze(1).unsqueeze(-1)  # [B, 1, in_f, 1]\n",
    "        \n",
    "        # Compute distances to grid points\n",
    "        distances = torch.abs(x_expanded - self.grid.unsqueeze(0))\n",
    "        \n",
    "        # Build basis functions\n",
    "        basis = torch.zeros(\n",
    "            batch_size, self.out_features, self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "            device=x.device\n",
    "        )\n",
    "        \n",
    "        # RBF basis for grid points\n",
    "        for i in range(self.grid_size):\n",
    "            basis[:, :, :, i] = torch.exp(-distances[:, :, :, i] ** 2 / 0.5)\n",
    "        \n",
    "        # Polynomial basis\n",
    "        for i in range(self.spline_order):\n",
    "            basis[:, :, :, self.grid_size + i] = x_expanded.squeeze(-1) ** (i + 1)\n",
    "        \n",
    "        # Compute spline output\n",
    "        spline_output = (basis * self.coef.unsqueeze(0)).sum(dim=-1).sum(dim=-1)\n",
    "        \n",
    "        # Add base transformation\n",
    "        base_output = torch.matmul(x, self.base_weight.t())\n",
    "        \n",
    "        return spline_output + base_output\n",
    "\n",
    "print('âœ… KANLinear layer defined!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# KAN: Complete KAN model for defect prediction\n\nclass KAN(nn.Module):\n    \"\"\"Kolmogorov-Arnold Network for binary defect classification.\"\"\"\n    \n    def __init__(self, input_dim, hidden_dim=64, grid_size=5, spline_order=3):\n        super().__init__()\n        # Deeper architecture with 3 KAN layers\n        self.kan1 = KANLinear(input_dim, hidden_dim, grid_size, spline_order)\n        self.kan2 = KANLinear(hidden_dim, hidden_dim, grid_size, spline_order)\n        self.kan3 = KANLinear(hidden_dim, hidden_dim // 2, grid_size, spline_order)\n        self.output_layer = nn.Linear(hidden_dim // 2, 1)\n        \n        self.bn1 = nn.BatchNorm1d(hidden_dim)\n        self.bn2 = nn.BatchNorm1d(hidden_dim)\n        self.bn3 = nn.BatchNorm1d(hidden_dim // 2)\n        self.dropout = nn.Dropout(0.2)\n    \n    def forward(self, x):\n        x = self.kan1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        \n        x = self.kan2(x)\n        x = self.bn2(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        \n        x = self.kan3(x)\n        x = self.bn3(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        \n        x = self.output_layer(x)\n        return torch.sigmoid(x)\n\nprint('âœ… KAN model defined!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Multi-Head Attention and Enhanced KAN with Attention\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multi-head attention mechanism for feature importance.\"\"\"\n    \n    def __init__(self, input_dim, num_heads=4, attention_dim=32):\n        super().__init__()\n        self.num_heads = num_heads\n        self.attention_dim = attention_dim\n        \n        # Multiple attention heads\n        self.heads = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(input_dim, attention_dim),\n                nn.ReLU(),\n                nn.Dropout(0.1),\n                nn.Linear(attention_dim, input_dim),\n                nn.Sigmoid()\n            ) for _ in range(num_heads)\n        ])\n        \n        # Combine heads\n        self.combine = nn.Linear(input_dim * num_heads, input_dim)\n        self.layer_norm = nn.LayerNorm(input_dim)\n    \n    def forward(self, x):\n        # Apply all attention heads\n        attended_outputs = []\n        for head in self.heads:\n            attention_weights = head(x)\n            attended_outputs.append(x * attention_weights)\n        \n        # Concatenate and combine\n        combined = torch.cat(attended_outputs, dim=-1)\n        output = self.combine(combined)\n        \n        # Residual connection + layer norm\n        output = self.layer_norm(x + output)\n        \n        return output\n\n\nclass KAN_Attention(nn.Module):\n    \"\"\"Enhanced KAN with Multi-Head Attention for superior defect detection.\"\"\"\n    \n    def __init__(self, input_dim, hidden_dim=128, grid_size=5, spline_order=3):\n        super().__init__()\n        \n        # Multi-head attention with 4 heads\n        self.attention = MultiHeadAttention(input_dim, num_heads=4, attention_dim=64)\n        \n        # Deeper KAN architecture - 4 layers\n        self.kan1 = KANLinear(input_dim, hidden_dim, grid_size, spline_order)\n        self.kan2 = KANLinear(hidden_dim, hidden_dim, grid_size, spline_order)\n        self.kan3 = KANLinear(hidden_dim, hidden_dim // 2, grid_size, spline_order)\n        self.kan4 = KANLinear(hidden_dim // 2, hidden_dim // 4, grid_size, spline_order)\n        \n        # Batch normalization for each layer\n        self.bn1 = nn.BatchNorm1d(hidden_dim)\n        self.bn2 = nn.BatchNorm1d(hidden_dim)\n        self.bn3 = nn.BatchNorm1d(hidden_dim // 2)\n        self.bn4 = nn.BatchNorm1d(hidden_dim // 4)\n        \n        # Dropout for regularization\n        self.dropout1 = nn.Dropout(0.2)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.15)\n        self.dropout4 = nn.Dropout(0.1)\n        \n        # Output layer\n        self.output_layer = nn.Linear(hidden_dim // 4, 1)\n        \n        # Feature importance tracking\n        self.feature_importance = None\n    \n    def forward(self, x):\n        # Apply multi-head attention with residual\n        x_attended = self.attention(x)\n        \n        # Deep KAN layers with residual connections\n        x1 = self.kan1(x_attended)\n        x1 = self.bn1(x1)\n        x1 = F.relu(x1)\n        x1 = self.dropout1(x1)\n        \n        x2 = self.kan2(x1)\n        x2 = self.bn2(x2)\n        x2 = F.relu(x2)\n        x2 = self.dropout2(x2)\n        \n        # Residual connection from x1 to x2\n        x2 = x2 + x1\n        \n        x3 = self.kan3(x2)\n        x3 = self.bn3(x3)\n        x3 = F.relu(x3)\n        x3 = self.dropout3(x3)\n        \n        x4 = self.kan4(x3)\n        x4 = self.bn4(x4)\n        x4 = F.relu(x4)\n        x4 = self.dropout4(x4)\n        \n        # Final prediction\n        output = self.output_layer(x4)\n        return torch.sigmoid(output)\n\nprint('âœ… Multi-Head Attention and Enhanced KAN_Attention defined!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Focal Loss for imbalanced classification\n\nclass FocalLoss(nn.Module):\n    \"\"\"Focal Loss to handle class imbalance - optimized for minority class.\"\"\"\n    \n    def __init__(self, alpha=0.75, gamma=3.0):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n    \n    def forward(self, inputs, targets):\n        bce_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-bce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n        return focal_loss.mean()\n\nprint('âœ… Optimized Focal Loss defined!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 4: Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print('\\n' + '='*70)\n",
    "print('ðŸš€ LOADING DATASET: CM1')\n",
    "print('='*70 + '\\n')\n",
    "\n",
    "file_path = os.path.join(DATASET_PATH, 'CM1.arff')\n",
    "df = load_arff(file_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.iloc[:, :-1].values.astype(np.float32)\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Encode labels if needed\n",
    "if y.dtype == object:\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "else:\n",
    "    y = y.astype(int)\n",
    "\n",
    "print(f'âœ… Dataset loaded successfully!')\n",
    "print(f'   Total samples: {len(y)}')\n",
    "print(f'   Features: {X.shape[1]}')\n",
    "print(f'   Defective samples: {np.sum(y==1)} ({np.mean(y==1):.2%})')\n",
    "print(f'   Non-defective samples: {np.sum(y==0)} ({np.mean(y==0):.2%})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values (NaN imputation)\n",
    "if np.any(np.isnan(X)):\n",
    "    print('âš ï¸  Found NaN values, imputing with column medians...')\n",
    "    col_medians = np.nanmedian(X, axis=0)\n",
    "    nan_indices = np.where(np.isnan(X))\n",
    "    X[nan_indices] = np.take(col_medians, nan_indices[1])\n",
    "    print('âœ… NaN values imputed!')\n",
    "else:\n",
    "    print('âœ… No NaN values found!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (train/val/test) - leakage-free splits\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, stratify=y_train_full, random_state=SEED\n",
    ")\n",
    "\n",
    "print('âœ… Data split complete!')\n",
    "print(f'   Training samples: {len(y_train)}')\n",
    "print(f'   Validation samples: {len(y_val)}')\n",
    "print(f'   Test samples: {len(y_test)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling (fit on train, transform all sets)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print('âœ… Feature scaling complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply SMOTE to training data only (prevent data leakage)\nsmote = SMOTE(sampling_strategy=1.0, random_state=SEED)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\nprint('âœ… SMOTE resampling complete!')\nprint(f'   Before: {len(y_train)} samples')\nprint(f'   After: {len(y_train_resampled)} samples')\nprint(f'   Added: {len(y_train_resampled) - len(y_train)} synthetic samples')\nprint(f'   Class balance: {np.sum(y_train_resampled==1)}/{np.sum(y_train_resampled==0)} (defective/normal)')\n\n# Initialize results dictionary\nresults = {}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒ² Step 5: Train Baseline Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline Random Forest classifier\n",
    "print('\\n' + '='*70)\n",
    "print('ðŸŒ² TRAINING BASELINE: RANDOM FOREST')\n",
    "print('='*70 + '\\n')\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    class_weight='balanced',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "print('âœ… Random Forest training complete!')\n",
    "\n",
    "# Find optimal threshold on validation set\n",
    "y_val_proba_rf = rf_model.predict_proba(X_val)[:, 1]\n",
    "threshold_rf = find_threshold(y_val, y_val_proba_rf)\n",
    "print(f'   Optimal threshold: {threshold_rf:.2f}')\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "y_test_pred_rf = (y_test_proba_rf >= threshold_rf).astype(int)\n",
    "metrics_rf = calc_metrics(y_test, y_test_pred_rf, y_test_proba_rf)\n",
    "\n",
    "print(f'\\nðŸ“Š Test Set Results:')\n",
    "print(f'   Recall:    {metrics_rf[\"recall\"]:.4f}')\n",
    "print(f'   Precision: {metrics_rf[\"precision\"]:.4f}')\n",
    "print(f'   F1 Score:  {metrics_rf[\"f1\"]:.4f}')\n",
    "print(f'   F2 Score:  {metrics_rf[\"f2\"]:.4f}')\n",
    "print(f'   Accuracy:  {metrics_rf[\"accuracy\"]:.4f}')\n",
    "print(f'   PR-AUC:    {metrics_rf[\"pr_auc\"]:.4f}')\n",
    "\n",
    "results['Baseline_RF'] = {'metrics': metrics_rf, 'threshold': threshold_rf}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¥ Step 6: Train KAN Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train KAN base model\nprint('\\n' + '='*70)\nprint('ðŸ”¥ TRAINING KAN BASE MODEL')\nprint('='*70 + '\\n')\n\n# Initialize model\nmodel_kan = KAN(\n    input_dim=X.shape[1],\n    hidden_dim=64,\n    grid_size=5,\n    spline_order=3\n).to(device)\n\noptimizer = optim.AdamW(model_kan.parameters(), lr=0.01, weight_decay=1e-4)\ncriterion = FocalLoss(alpha=0.75, gamma=3.0)\n\n# Learning rate scheduler with warmup\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-5)\n\n# Prepare data loaders\nX_train_tensor = torch.FloatTensor(X_train_resampled).to(device)\ny_train_tensor = torch.FloatTensor(y_train_resampled).unsqueeze(1).to(device)\nX_val_tensor = torch.FloatTensor(X_val).to(device)\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# Training loop with early stopping\nbest_val_f2 = 0\npatience_counter = 0\nmax_patience = 20\n\nfor epoch in range(100):\n    model_kan.train()\n    epoch_loss = 0\n    \n    for batch_X, batch_y in train_loader:\n        optimizer.zero_grad()\n        outputs = model_kan(batch_X)\n        loss = criterion(outputs, batch_y)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n    \n    scheduler.step()\n    \n    # Validation\n    model_kan.eval()\n    with torch.no_grad():\n        val_outputs = model_kan(X_val_tensor).cpu().numpy().flatten()\n        val_predictions = (val_outputs >= 0.5).astype(int)\n        val_f2 = fbeta_score(y_val, val_predictions, beta=2, zero_division=0)\n    \n    if val_f2 > best_val_f2:\n        best_val_f2 = val_f2\n        best_model_state = model_kan.state_dict().copy()\n        patience_counter = 0\n    else:\n        patience_counter += 1\n    \n    if (epoch + 1) % 10 == 0:\n        print(f'   Epoch {epoch + 1}/100 - Loss: {epoch_loss/len(train_loader):.4f} - Val F2: {val_f2:.4f} - LR: {scheduler.get_last_lr()[0]:.6f}')\n    \n    if patience_counter >= max_patience:\n        print(f'   Early stopping at epoch {epoch + 1}')\n        model_kan.load_state_dict(best_model_state)\n        break\n\nprint(f'âœ… KAN training complete!')\nprint(f'   Best validation F2: {best_val_f2:.4f}')\n\n# Evaluate on test set\nmodel_kan.eval()\nwith torch.no_grad():\n    y_val_proba_kan = model_kan(X_val_tensor).cpu().numpy().flatten()\n    X_test_tensor = torch.FloatTensor(X_test).to(device)\n    y_test_proba_kan = model_kan(X_test_tensor).cpu().numpy().flatten()\n\nthreshold_kan = find_threshold(y_val, y_val_proba_kan)\nprint(f'   Optimal threshold: {threshold_kan:.2f}')\n\ny_test_pred_kan = (y_test_proba_kan >= threshold_kan).astype(int)\nmetrics_kan = calc_metrics(y_test, y_test_pred_kan, y_test_proba_kan)\n\nprint(f'\\nðŸ“Š Test Set Results:')\nprint(f'   Recall:    {metrics_kan[\"recall\"]:.4f}')\nprint(f'   Precision: {metrics_kan[\"precision\"]:.4f}')\nprint(f'   F1 Score:  {metrics_kan[\"f1\"]:.4f}')\nprint(f'   F2 Score:  {metrics_kan[\"f2\"]:.4f}')\nprint(f'   Accuracy:  {metrics_kan[\"accuracy\"]:.4f}')\nprint(f'   PR-AUC:    {metrics_kan[\"pr_auc\"]:.4f}')\n\nresults['KAN_Base'] = {'metrics': metrics_kan, 'threshold': threshold_kan}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Step 7: Train KAN + Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train KAN with Attention mechanism - ENHANCED VERSION\nprint('\\n' + '='*70)\nprint('ðŸŒŸ TRAINING ENHANCED KAN + MULTI-HEAD ATTENTION MODEL')\nprint('='*70 + '\\n')\n\n# Initialize enhanced model with larger capacity\nmodel_kan_att = KAN_Attention(\n    input_dim=X.shape[1],\n    hidden_dim=128,  # Much larger capacity\n    grid_size=5,\n    spline_order=3\n).to(device)\n\nprint(f'ðŸ“Š Model Parameters: {sum(p.numel() for p in model_kan_att.parameters()):,}')\n\n# Optimizer with lower learning rate for better convergence\noptimizer = optim.AdamW(model_kan_att.parameters(), lr=0.005, weight_decay=1e-3)\n\n# Stronger focal loss for minority class\ncriterion = FocalLoss(alpha=0.85, gamma=4.0)\n\n# Warmup + Cosine Annealing scheduler\ndef get_lr(epoch, warmup_epochs=5, total_epochs=150):\n    if epoch < warmup_epochs:\n        return 0.005 * (epoch + 1) / warmup_epochs\n    else:\n        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n        return 1e-5 + (0.005 - 1e-5) * 0.5 * (1 + np.cos(np.pi * progress))\n\n# Training loop with advanced techniques\nbest_val_f2 = 0\nbest_val_recall = 0\npatience_counter = 0\nmax_patience = 30\n\ntraining_history = {'loss': [], 'val_f2': [], 'val_recall': []}\n\nfor epoch in range(150):\n    model_kan_att.train()\n    epoch_loss = 0\n    \n    for batch_X, batch_y in train_loader:\n        # Update learning rate\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = get_lr(epoch)\n        \n        optimizer.zero_grad()\n        outputs = model_kan_att(batch_X)\n        loss = criterion(outputs, batch_y)\n        \n        loss.backward()\n        \n        # Gradient clipping to prevent exploding gradients\n        torch.nn.utils.clip_grad_norm_(model_kan_att.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        epoch_loss += loss.item()\n    \n    # Validation\n    model_kan_att.eval()\n    with torch.no_grad():\n        val_outputs = model_kan_att(X_val_tensor).cpu().numpy().flatten()\n        val_predictions = (val_outputs >= 0.5).astype(int)\n        val_f2 = fbeta_score(y_val, val_predictions, beta=2, zero_division=0)\n        val_recall = recall_score(y_val, val_predictions, zero_division=0)\n    \n    training_history['loss'].append(epoch_loss / len(train_loader))\n    training_history['val_f2'].append(val_f2)\n    training_history['val_recall'].append(val_recall)\n    \n    # Save best model based on F2 score\n    if val_f2 > best_val_f2:\n        best_val_f2 = val_f2\n        best_val_recall = val_recall\n        best_model_state = model_kan_att.state_dict().copy()\n        patience_counter = 0\n    else:\n        patience_counter += 1\n    \n    if (epoch + 1) % 10 == 0:\n        current_lr = get_lr(epoch)\n        print(f'   Epoch {epoch + 1}/150 - Loss: {epoch_loss/len(train_loader):.4f} - Val F2: {val_f2:.4f} - Val Recall: {val_recall:.4f} - LR: {current_lr:.6f}')\n    \n    if patience_counter >= max_patience:\n        print(f'   Early stopping at epoch {epoch + 1}')\n        model_kan_att.load_state_dict(best_model_state)\n        break\n\nprint(f'\\nâœ… KAN + Attention training complete!')\nprint(f'   Best validation F2: {best_val_f2:.4f}')\nprint(f'   Best validation Recall: {best_val_recall:.4f}')\n\n# Evaluate on test set with aggressive recall-focused threshold optimization\nmodel_kan_att.eval()\nwith torch.no_grad():\n    y_val_proba_att = model_kan_att(X_val_tensor).cpu().numpy().flatten()\n    y_test_proba_att = model_kan_att(X_test_tensor).cpu().numpy().flatten()\n\n# Custom threshold optimization focusing on RECALL\nprint(f'\\nðŸŽ¯ Finding optimal threshold (recall-focused)...')\nbest_score = -1\nbest_threshold = 0.5\nbest_metrics = None\n\nfor t in np.arange(0.05, 0.80, 0.02):  # Lower thresholds for better recall\n    y_val_pred_temp = (y_val_proba_att >= t).astype(int)\n    temp_metrics = calc_metrics(y_val, y_val_pred_temp)\n    \n    # Weighted score: 60% recall + 40% F2\n    score = 0.6 * temp_metrics['recall'] + 0.4 * temp_metrics['f2']\n    \n    # Must maintain minimum accuracy\n    if temp_metrics['accuracy'] >= 0.55 and score > best_score:\n        best_score = score\n        best_threshold = t\n        best_metrics = temp_metrics\n\nthreshold_att = best_threshold\nprint(f'   Optimal threshold: {threshold_att:.3f}')\nprint(f'   At this threshold - Val Recall: {best_metrics[\"recall\"]:.4f}, Val F2: {best_metrics[\"f2\"]:.4f}')\n\ny_test_pred_att = (y_test_proba_att >= threshold_att).astype(int)\nmetrics_att = calc_metrics(y_test, y_test_pred_att, y_test_proba_att)\n\nprint(f'\\nðŸ“Š Test Set Results:')\nprint(f'   Recall:    {metrics_att[\"recall\"]:.4f} {\"ðŸŽ¯ EXCELLENT!\" if metrics_att[\"recall\"] >= 0.85 else \"\"}')\nprint(f'   Precision: {metrics_att[\"precision\"]:.4f}')\nprint(f'   F1 Score:  {metrics_att[\"f1\"]:.4f}')\nprint(f'   F2 Score:  {metrics_att[\"f2\"]:.4f} {\"â­ EXCELLENT!\" if metrics_att[\"f2\"] >= 0.70 else \"\"}')\nprint(f'   Accuracy:  {metrics_att[\"accuracy\"]:.4f}')\nprint(f'   PR-AUC:    {metrics_att[\"pr_auc\"]:.4f}')\nprint(f'   Confusion Matrix: TP={metrics_att[\"tp\"]}, FP={metrics_att[\"fp\"]}, TN={metrics_att[\"tn\"]}, FN={metrics_att[\"fn\"]}')\n\nresults['KAN_Attention'] = {'metrics': metrics_att, 'threshold': threshold_att}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Step 8: Compare Results & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and display final results\n",
    "print('\\n' + '='*70)\n",
    "print('ðŸ“Š FINAL RESULTS - CM1')\n",
    "print('='*70 + '\\n')\n",
    "\n",
    "results_list = []\n",
    "for model_name, data in results.items():\n",
    "    m = data['metrics']\n",
    "    results_list.append({\n",
    "        'dataset': 'CM1',\n",
    "        'model': model_name,\n",
    "        'recall': m['recall'],\n",
    "        'precision': m['precision'],\n",
    "        'f1': m['f1'],\n",
    "        'f2': m['f2'],\n",
    "        'accuracy': m['accuracy'],\n",
    "        'pr_auc': m['pr_auc'],\n",
    "        'threshold': data['threshold']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Display results\n",
    "print('\\nðŸ“‹ Model Comparison:\\n')\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"{row['model']}:\")\n",
    "    print(f\"   Recall:    {row['recall']:.4f} {'ðŸŽ¯' if row['recall'] >= 0.80 else ''}\")\n",
    "    print(f\"   Precision: {row['precision']:.4f}\")\n",
    "    print(f\"   F2 Score:  {row['f2']:.4f} {'â­' if row['f2'] >= 0.75 else ''}\")\n",
    "    print(f\"   Accuracy:  {row['accuracy']:.4f}\")\n",
    "    print(f\"   Threshold: {row['threshold']:.2f}\\n\")\n",
    "\n",
    "# Export results\n",
    "csv_path = os.path.join(OUTPUT_DIR, f'results_{RUN_ID}.csv')\n",
    "json_path = os.path.join(OUTPUT_DIR, f'results_{RUN_ID}.json')\n",
    "\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "results_df.to_json(json_path, orient='records', indent=2)\n",
    "\n",
    "print('\\nðŸ’¾ Results saved to:')\n",
    "print(f'   CSV:  {csv_path}')\n",
    "print(f'   JSON: {json_path}')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('âœ… EXPERIMENT COMPLETE - CM1')\n",
    "print('='*70 + '\\n')\n",
    "\n",
    "# Display summary DataFrame\n",
    "print('\\nðŸ“Š Summary Table:')\n",
    "display(results_df[['model', 'recall', 'precision', 'f1', 'f2', 'accuracy', 'pr_auc']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}