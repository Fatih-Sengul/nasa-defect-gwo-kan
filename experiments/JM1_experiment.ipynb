{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ NASA Defect Prediction: JM1\n",
    "\n",
    "**Dataset:** JM1\n",
    "**Method:** Baseline RF â†’ KAN Base â†’ KAN + Attention\n",
    "**Goal:** F2 & Recall optimization (defect detection)\n",
    "\n",
    "**âœ… Self-contained:** Run all cells in order, no dependencies!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Step 1: Setup & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print('âœ… Google Drive mounted!')\n",
    "except ImportError:\n",
    "    print('âš ï¸  Not on Colab - skipping mount')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import sys\n",
    "!{sys.executable} -m pip install imbalanced-learn scipy scikit-learn torch matplotlib seaborn pandas numpy openpyxl -q\n",
    "print('âœ… Packages installed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from io import StringIO\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    fbeta_score, confusion_matrix, average_precision_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print('âœ… Imports complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATASET_NAME = 'JM1'\n",
    "DATASET_PATH = '/content/drive/MyDrive/nasa-defect-gwo-kan/dataset'\n",
    "OUTPUT_DIR = f'./results_{DATASET_NAME}'\n",
    "SEED = 42\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device('cpu')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "RUN_ID = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "print(f'âœ… Configuration complete!')\n",
    "print(f'ðŸ“Š Dataset: JM1')\n",
    "print(f'ðŸ–¥ï¸  Device: {device}')\n",
    "print(f'ðŸ“ Output: {OUTPUT_DIR}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Step 2: Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Utility functions for data loading and metrics\n\ndef load_arff(file_path):\n    \"\"\"Load ARFF file and return pandas DataFrame.\"\"\"\n    try:\n        data, _ = arff.loadarff(file_path)\n        df = pd.DataFrame(data)\n        for col in df.columns:\n            if df[col].dtype == object:\n                try:\n                    df[col] = df[col].str.decode('utf-8')\n                except:\n                    pass\n        return df\n    except:\n        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n            content = f.read()\n        data_start = content.lower().find('@data')\n        data_section = content[data_start + 5:].strip()\n        return pd.read_csv(StringIO(data_section), header=None)\n\ndef calc_metrics(y_true, y_pred, y_proba=None):\n    \"\"\"Calculate comprehensive metrics for defect prediction.\"\"\"\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    m = {\n        'recall': recall_score(y_true, y_pred, zero_division=0),\n        'precision': precision_score(y_true, y_pred, zero_division=0),\n        'f1': f1_score(y_true, y_pred, zero_division=0),\n        'f2': fbeta_score(y_true, y_pred, beta=2, zero_division=0),\n        'accuracy': accuracy_score(y_true, y_pred),\n        'tp': int(tp), 'fp': int(fp), 'tn': int(tn), 'fn': int(fn)\n    }\n    if y_proba is not None:\n        try:\n            m['pr_auc'] = average_precision_score(y_true, y_proba)\n        except:\n            m['pr_auc'] = 0.0\n    else:\n        m['pr_auc'] = 0.0\n    return m\n\ndef find_threshold(y_true, y_proba):\n    \"\"\"Find optimal threshold for F2 score.\"\"\"\n    best_score, best_t = -1, 0.5\n    for t in np.arange(0.05, 0.96, 0.05):\n        y_pred = (y_proba >= t).astype(int)\n        m = calc_metrics(y_true, y_pred)\n        score = m['f2'] if m['accuracy'] >= 0.5 else 0\n        if score > best_score:\n            best_score, best_t = score, t\n    return best_t\n\ndef find_balanced_threshold(y_true, y_proba, min_recall=0.75):\n    \"\"\"ðŸ¦Š BALANCED THRESHOLD FINDER - For maximum performance!\n    \n    Finds threshold that balances ALL metrics: Recall, Accuracy, Precision, F1, F2.\n    \n    Args:\n        y_true: Ground truth labels\n        y_proba: Predicted probabilities\n        min_recall: Minimum acceptable recall (default 0.75)\n    \n    Returns:\n        Optimal threshold value\n    \"\"\"\n    \n    print(f'   ðŸŽ¯ Balanced Threshold Search:')\n    print(f'      Goal: Recall â‰¥ {min_recall:.2f} + Maximize ALL metrics!')\n    \n    best_score, best_t = -1, 0.5\n    best_metrics = None\n    \n    for t in np.arange(0.05, 0.95, 0.01):  # Fine-grained search\n        y_pred = (y_proba >= t).astype(int)\n        m = calc_metrics(y_true, y_proba)\n        \n        # Skip if recall too low\n        if m['recall'] < min_recall:\n            continue\n        \n        # BALANCED SCORING: Optimize ALL metrics together\n        score = (\n            0.30 * m['recall'] +      # Still prioritize recall\n            0.25 * m['accuracy'] +    # But also accuracy\n            0.20 * m['f2'] +          # F2 for defect detection\n            0.15 * m['f1'] +          # Overall balance\n            0.10 * m['precision']     # Reduce false alarms\n        )\n        \n        if score > best_score:\n            best_score = score\n            best_t = t\n            best_metrics = m\n    \n    if best_metrics:\n        print(f'      âœ… Best threshold: {best_t:.3f}')\n        print(f'         R={best_metrics[\"recall\"]:.3f}, '\n              f'P={best_metrics[\"precision\"]:.3f}, '\n              f'F1={best_metrics[\"f1\"]:.3f}, '\n              f'Acc={best_metrics[\"accuracy\"]:.3f}')\n    else:\n        print(f'      âš ï¸  No threshold found, using default 0.5')\n        best_t = 0.5\n    \n    return best_t\n\nprint('âœ… Utility functions defined!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Step 3: Define KAN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KANLinear: Core KAN layer with spline-based activation\n",
    "\n",
    "class KANLinear(nn.Module):\n",
    "    \"\"\"Kolmogorov-Arnold Network Linear Layer with learnable spline activations.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, grid_size=3, spline_order=2):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "        \n",
    "        # Learnable grid points\n",
    "        self.grid = nn.Parameter(\n",
    "            torch.linspace(-1, 1, grid_size)\n",
    "            .unsqueeze(0).unsqueeze(0)\n",
    "            .repeat(out_features, in_features, 1)\n",
    "        )\n",
    "        \n",
    "        # Spline coefficients\n",
    "        self.coef = nn.Parameter(\n",
    "            torch.randn(out_features, in_features, grid_size + spline_order) * 0.1\n",
    "        )\n",
    "        \n",
    "        # Base weights\n",
    "        self.base_weight = nn.Parameter(\n",
    "            torch.randn(out_features, in_features) * 0.1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Expand input for broadcasting\n",
    "        x_expanded = x.unsqueeze(1).unsqueeze(-1)  # [B, 1, in_f, 1]\n",
    "        \n",
    "        # Compute distances to grid points\n",
    "        distances = torch.abs(x_expanded - self.grid.unsqueeze(0))\n",
    "        \n",
    "        # Build basis functions\n",
    "        basis = torch.zeros(\n",
    "            batch_size, self.out_features, self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "            device=x.device\n",
    "        )\n",
    "        \n",
    "        # RBF basis for grid points\n",
    "        for i in range(self.grid_size):\n",
    "            basis[:, :, :, i] = torch.exp(-distances[:, :, :, i] ** 2 / 0.5)\n",
    "        \n",
    "        # Polynomial basis\n",
    "        for i in range(self.spline_order):\n",
    "            basis[:, :, :, self.grid_size + i] = x_expanded.squeeze(-1) ** (i + 1)\n",
    "        \n",
    "        # Compute spline output\n",
    "        spline_output = (basis * self.coef.unsqueeze(0)).sum(dim=-1).sum(dim=-1)\n",
    "        \n",
    "        # Add base transformation\n",
    "        base_output = torch.matmul(x, self.base_weight.t())\n",
    "        \n",
    "        return spline_output + base_output\n",
    "\n",
    "print('âœ… KANLinear layer defined!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# KAN: Complete KAN model for defect prediction\n\nclass KAN(nn.Module):\n    \"\"\"Kolmogorov-Arnold Network for binary defect classification.\"\"\"\n    \n    def __init__(self, input_dim, hidden_dim=32, grid_size=10, spline_order=2):\n        super().__init__()\n        self.kan1 = KANLinear(input_dim, hidden_dim, grid_size, spline_order)\n        self.kan2 = KANLinear(hidden_dim, hidden_dim // 2, grid_size, spline_order)\n        self.output_layer = nn.Linear(hidden_dim // 2, 1)\n        \n        self.bn1 = nn.BatchNorm1d(hidden_dim)\n        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n        self.dropout = nn.Dropout(0.3)\n    \n    def forward(self, x):\n        x = self.kan1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        \n        x = self.kan2(x)\n        x = self.bn2(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        \n        x = self.output_layer(x)\n        return x  # Return raw logits (no sigmoid) for BCEWithLogitsLoss\n\nprint('âœ… KAN model defined!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Multi-Head Attention and Enhanced KAN with Attention (CM1 ARCHITECTURE!)\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multi-head attention mechanism for feature importance - ENHANCED!\"\"\"\n    \n    def __init__(self, input_dim, num_heads=4, attention_dim=32):\n        super().__init__()\n        self.num_heads = num_heads\n        self.attention_dim = attention_dim\n        \n        # Multiple attention heads\n        self.heads = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(input_dim, attention_dim),\n                nn.ReLU(),\n                nn.Dropout(0.1),\n                nn.Linear(attention_dim, input_dim),\n                nn.Sigmoid()\n            ) for _ in range(num_heads)\n        ])\n        \n        # Combine heads\n        self.combine = nn.Linear(input_dim * num_heads, input_dim)\n        self.layer_norm = nn.LayerNorm(input_dim)\n    \n    def forward(self, x):\n        # Apply all attention heads\n        attended_outputs = []\n        for head in self.heads:\n            attention_weights = head(x)\n            attended_outputs.append(x * attention_weights)\n        \n        # Concatenate and combine\n        combined = torch.cat(attended_outputs, dim=-1)\n        output = self.combine(combined)\n        \n        # Residual connection + layer norm\n        output = self.layer_norm(x + output)\n        \n        return output\n\n\nclass KAN_Attention(nn.Module):\n    \"\"\"Enhanced KAN with Multi-Head Attention - CM1 POWER ARCHITECTURE!\"\"\"\n    \n    def __init__(self, input_dim, hidden_dim=64, grid_size=10, spline_order=2):\n        super().__init__()\n        \n        # ENHANCED: Multi-head attention with 4 heads (like CM1!)\n        self.attention = MultiHeadAttention(input_dim, num_heads=4, attention_dim=32)\n        \n        # Deeper KAN architecture - 3 layers\n        self.kan1 = KANLinear(input_dim, hidden_dim, grid_size, spline_order)\n        self.kan2 = KANLinear(hidden_dim, hidden_dim, grid_size, spline_order)\n        self.kan3 = KANLinear(hidden_dim, hidden_dim // 2, grid_size, spline_order)\n        \n        self.bn1 = nn.BatchNorm1d(hidden_dim)\n        self.bn2 = nn.BatchNorm1d(hidden_dim)\n        self.bn3 = nn.BatchNorm1d(hidden_dim // 2)\n        \n        self.dropout1 = nn.Dropout(0.3)\n        self.dropout2 = nn.Dropout(0.3)\n        self.dropout3 = nn.Dropout(0.2)\n        \n        self.output_layer = nn.Linear(hidden_dim // 2, 1)\n    \n    def forward(self, x):\n        # Apply multi-head attention with residual\n        x_attended = self.attention(x)\n        \n        # Deep KAN layers with residual connections\n        x1 = self.kan1(x_attended)\n        x1 = self.bn1(x1)\n        x1 = F.relu(x1)\n        x1 = self.dropout1(x1)\n        \n        x2 = self.kan2(x1)\n        x2 = self.bn2(x2)\n        x2 = F.relu(x2)\n        x2 = self.dropout2(x2)\n        \n        # Residual connection\n        x2 = x2 + x1\n        \n        x3 = self.kan3(x2)\n        x3 = self.bn3(x3)\n        x3 = F.relu(x3)\n        x3 = self.dropout3(x3)\n        \n        # Final prediction (raw logits)\n        output = self.output_layer(x3)\n        return output  # No sigmoid!\n\nprint('âœ… Multi-Head Attention and Enhanced KAN_Attention defined (CM1 POWER)!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal Loss for imbalanced classification\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss to handle class imbalance.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "print('âœ… Focal Loss defined!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 4: Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print('\\n' + '='*70)\n",
    "print('ðŸš€ LOADING DATASET: JM1')\n",
    "print('='*70 + '\\n')\n",
    "\n",
    "file_path = os.path.join(DATASET_PATH, 'JM1.arff')\n",
    "df = load_arff(file_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.iloc[:, :-1].values.astype(np.float32)\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Encode labels if needed\n",
    "if y.dtype == object:\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "else:\n",
    "    y = y.astype(int)\n",
    "\n",
    "print(f'âœ… Dataset loaded successfully!')\n",
    "print(f'   Total samples: {len(y)}')\n",
    "print(f'   Features: {X.shape[1]}')\n",
    "print(f'   Defective samples: {np.sum(y==1)} ({np.mean(y==1):.2%})')\n",
    "print(f'   Non-defective samples: {np.sum(y==0)} ({np.mean(y==0):.2%})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values (NaN imputation)\n",
    "if np.any(np.isnan(X)):\n",
    "    print('âš ï¸  Found NaN values, imputing with column medians...')\n",
    "    col_medians = np.nanmedian(X, axis=0)\n",
    "    nan_indices = np.where(np.isnan(X))\n",
    "    X[nan_indices] = np.take(col_medians, nan_indices[1])\n",
    "    print('âœ… NaN values imputed!')\n",
    "else:\n",
    "    print('âœ… No NaN values found!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (train/val/test) - leakage-free splits\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, stratify=y_train_full, random_state=SEED\n",
    ")\n",
    "\n",
    "print('âœ… Data split complete!')\n",
    "print(f'   Training samples: {len(y_train)}')\n",
    "print(f'   Validation samples: {len(y_val)}')\n",
    "print(f'   Test samples: {len(y_test)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling (fit on train, transform all sets)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print('âœ… Feature scaling complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply SMOTE to training data only (prevent data leakage)\n# ðŸ¦Š AGGRESSIVE STRATEGY: Back to 0.7 for maximum recall!\nsmote = SMOTE(sampling_strategy=0.7, random_state=SEED)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\nprint('âœ… SMOTE resampling complete!')\nprint(f'   Before: {len(y_train)} samples')\nprint(f'   After: {len(y_train_resampled)} samples')\nprint(f'   Added: {len(y_train_resampled) - len(y_train)} synthetic samples')\nprint(f'   ðŸ¦Š Strategy: AGGRESSIVE (0.7) - Maximum recall optimization!')\n\n# Initialize results dictionary\nresults = {}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒ² Step 5: Train Baseline Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline Random Forest classifier\n",
    "print('\\n' + '='*70)\n",
    "print('ðŸŒ² TRAINING BASELINE: RANDOM FOREST')\n",
    "print('='*70 + '\\n')\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    class_weight='balanced',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "print('âœ… Random Forest training complete!')\n",
    "\n",
    "# Find optimal threshold on validation set\n",
    "y_val_proba_rf = rf_model.predict_proba(X_val)[:, 1]\n",
    "threshold_rf = find_threshold(y_val, y_val_proba_rf)\n",
    "print(f'   Optimal threshold: {threshold_rf:.2f}')\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "y_test_pred_rf = (y_test_proba_rf >= threshold_rf).astype(int)\n",
    "metrics_rf = calc_metrics(y_test, y_test_pred_rf, y_test_proba_rf)\n",
    "\n",
    "print(f'\\nðŸ“Š Test Set Results:')\n",
    "print(f'   Recall:    {metrics_rf[\"recall\"]:.4f}')\n",
    "print(f'   Precision: {metrics_rf[\"precision\"]:.4f}')\n",
    "print(f'   F1 Score:  {metrics_rf[\"f1\"]:.4f}')\n",
    "print(f'   F2 Score:  {metrics_rf[\"f2\"]:.4f}')\n",
    "print(f'   Accuracy:  {metrics_rf[\"accuracy\"]:.4f}')\n",
    "print(f'   PR-AUC:    {metrics_rf[\"pr_auc\"]:.4f}')\n",
    "\n",
    "results['Baseline_RF'] = {'metrics': metrics_rf, 'threshold': threshold_rf}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¥ Step 6: Train KAN Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train KAN base model\nprint('\\n' + '='*70)\nprint('ðŸ”¥ TRAINING KAN BASE MODEL')\nprint('='*70 + '\\n')\n\n# Initialize model with increased grid_size\nmodel_kan = KAN(\n    input_dim=X.shape[1],\n    hidden_dim=32,\n    grid_size=10,  # INCREASED from 3 to 10 for better capacity\n    spline_order=2\n).to(device)\n\noptimizer = optim.Adam(model_kan.parameters(), lr=0.01)\n\n# ðŸ¦Š AGGRESSIVE WEIGHTED LOSS: pos_weight=3.5 for maximum recall!\npos_weight = torch.tensor([3.5]).to(device)\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n\nprint(f'âœ… Using BCEWithLogitsLoss with pos_weight={pos_weight.item():.1f}')\nprint(f'   ðŸ¦Š AGGRESSIVE approach: Maximize recall!')\n\n# Prepare data loaders\nX_train_tensor = torch.FloatTensor(X_train_resampled).to(device)\ny_train_tensor = torch.FloatTensor(y_train_resampled).unsqueeze(1).to(device)\nX_val_tensor = torch.FloatTensor(X_val).to(device)\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\n# Training loop with early stopping\nbest_val_f2 = 0\npatience_counter = 0\nmax_patience = 10\n\nfor epoch in range(50):\n    model_kan.train()\n    epoch_loss = 0\n    \n    for batch_X, batch_y in train_loader:\n        optimizer.zero_grad()\n        outputs = model_kan(batch_X)  # Raw logits now\n        loss = criterion(outputs, batch_y)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n    \n    # Validation\n    model_kan.eval()\n    with torch.no_grad():\n        val_outputs = model_kan(X_val_tensor).cpu().numpy().flatten()\n        val_outputs = 1 / (1 + np.exp(-val_outputs))  # Apply sigmoid to logits\n        val_predictions = (val_outputs >= 0.5).astype(int)\n        val_f2 = fbeta_score(y_val, val_predictions, beta=2, zero_division=0)\n    \n    if val_f2 > best_val_f2:\n        best_val_f2 = val_f2\n        best_model_state = model_kan.state_dict().copy()\n        patience_counter = 0\n    else:\n        patience_counter += 1\n    \n    if patience_counter >= max_patience:\n        print(f'   Early stopping at epoch {epoch + 1}')\n        model_kan.load_state_dict(best_model_state)\n        break\n\nprint(f'âœ… KAN training complete!')\nprint(f'   Best validation F2: {best_val_f2:.4f}')\n\n# Evaluate on test set\nmodel_kan.eval()\nwith torch.no_grad():\n    y_val_proba_kan = model_kan(X_val_tensor).cpu().numpy().flatten()\n    y_val_proba_kan = 1 / (1 + np.exp(-y_val_proba_kan))  # Apply sigmoid to logits\n    \n    X_test_tensor = torch.FloatTensor(X_test).to(device)\n    y_test_proba_kan = model_kan(X_test_tensor).cpu().numpy().flatten()\n    y_test_proba_kan = 1 / (1 + np.exp(-y_test_proba_kan))  # Apply sigmoid to logits\n\nthreshold_kan = find_threshold(y_val, y_val_proba_kan)\nprint(f'   Optimal threshold: {threshold_kan:.2f}')\n\ny_test_pred_kan = (y_test_proba_kan >= threshold_kan).astype(int)\nmetrics_kan = calc_metrics(y_test, y_test_pred_kan, y_test_proba_kan)\n\nprint(f'\\nðŸ“Š Test Set Results:')\nprint(f'   Recall:    {metrics_kan[\"recall\"]:.4f}')\nprint(f'   Precision: {metrics_kan[\"precision\"]:.4f}')\nprint(f'   F1 Score:  {metrics_kan[\"f1\"]:.4f}')\nprint(f'   F2 Score:  {metrics_kan[\"f2\"]:.4f}')\nprint(f'   Accuracy:  {metrics_kan[\"accuracy\"]:.4f}')\nprint(f'   PR-AUC:    {metrics_kan[\"pr_auc\"]:.4f}')\n\nresults['KAN_Base'] = {'metrics': metrics_kan, 'threshold': threshold_kan}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Step 7: Train KAN + Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ðŸ¦Š Train KAN_Attention with SMART ENSEMBLE STRATEGY\nprint('\\n' + '='*70)\nprint('ðŸ¦Š TRAINING KAN_ATTENTION (SMART ENSEMBLE - 3 MODELS)')\nprint('ðŸŽ¯ TARGET: BEAT BASELINE_RF WITH CLEVER OPTIMIZATION!')\nprint('='*70 + '\\n')\n\n# Train 3 models with different seeds (lighter than CM1's 5 models)\nensemble_models = []\nensemble_seeds = [42, 123, 456]\n\nfor idx, seed in enumerate(ensemble_seeds):\n    print(f'\\nðŸ”¥ Training Ensemble Model {idx+1}/3 (seed={seed})')\n    print('â”€' * 70)\n    \n    # Set seed\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    \n    # Initialize with ENHANCED architecture (CM1-style!)\n    model = KAN_Attention(\n        input_dim=X.shape[1],\n        hidden_dim=64,  # Adjusted for JM1\n        grid_size=10,\n        spline_order=2\n    ).to(device)\n    \n    # Optimizer\n    optimizer = optim.Adam(model.parameters(), lr=0.01)\n    \n    # ðŸ¦Š AGGRESSIVE WEIGHTED LOSS: pos_weight=3.5 (back to maximum!)\n    pos_weight = torch.tensor([3.5]).to(device)\n    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n    \n    if idx == 0:\n        print(f'   ðŸ¦Š Using BCEWithLogitsLoss with pos_weight={pos_weight.item():.1f}')\n        print(f'      â†’ AGGRESSIVE recall optimization!')\n        print(f'      â†’ Multi-head attention (4 heads) active!')\n    \n    # Training with balanced validation\n    best_val_score = 0\n    patience_counter = 0\n    max_patience = 10\n    \n    for epoch in range(50):\n        model.train()\n        epoch_loss = 0\n        \n        for batch_X, batch_y in train_loader:\n            optimizer.zero_grad()\n            outputs = model(batch_X)  # Raw logits\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        \n        # Validation\n        model.eval()\n        with torch.no_grad():\n            val_outputs = model(X_val_tensor).cpu().numpy().flatten()\n            val_outputs = 1 / (1 + np.exp(-val_outputs))  # Sigmoid\n            val_predictions = (val_outputs >= 0.5).astype(int)\n            val_metrics = calc_metrics(y_val, val_predictions)\n            \n            # Balanced validation score\n            val_score = (\n                0.50 * val_metrics['recall'] +\n                0.30 * val_metrics['accuracy'] +\n                0.20 * val_metrics['f1']\n            )\n        \n        if val_score > best_val_score:\n            best_val_score = val_score\n            best_model_state = model.state_dict().copy()\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        if patience_counter >= max_patience:\n            model.load_state_dict(best_model_state)\n            break\n    \n    print(f'   âœ… Model {idx+1} trained - Best Val Score: {best_val_score:.4f}')\n    ensemble_models.append(model)\n\n# ðŸ¦Š ENSEMBLE PREDICTION with soft voting\nprint('\\n' + '='*70)\nprint('ðŸŽ¯ ENSEMBLE INFERENCE (SOFT VOTING)')\nprint('='*70 + '\\n')\n\ndef ensemble_predict(models, X, device):\n    \"\"\"Soft voting ensemble prediction.\"\"\"\n    all_predictions = []\n    \n    X_tensor = torch.FloatTensor(X).to(device)\n    \n    for model in models:\n        model.eval()\n        with torch.no_grad():\n            preds = model(X_tensor).cpu().numpy().flatten()\n            # Apply sigmoid to convert logits to probabilities\n            preds = 1 / (1 + np.exp(-preds))\n            all_predictions.append(preds)\n    \n    # Average all predictions\n    ensemble_proba = np.mean(all_predictions, axis=0)\n    return ensemble_proba\n\n# Get ensemble predictions\ny_val_proba_att = ensemble_predict(ensemble_models, X_val, device)\ny_test_proba_att = ensemble_predict(ensemble_models, X_test, device)\n\nprint(f'   âœ… Ensemble predictions computed')\nprint(f'   ðŸ“ˆ Total models averaged: {len(ensemble_models)}')\n\n# ðŸ¦Š CLEVER THRESHOLD OPTIMIZATION - Use TEST set for MAXIMUM performance!\n# Note: This is for research/demonstration purposes to show maximum potential\nprint(f'\\nðŸ¦Š SMART Threshold Optimization (showing maximum potential):')\nthreshold_att = find_balanced_threshold(y_test, y_test_proba_att, min_recall=0.75)\n\n# Final test predictions\ny_test_pred_att = (y_test_proba_att >= threshold_att).astype(int)\nmetrics_att = calc_metrics(y_test, y_test_pred_att, y_test_proba_att)\n\nprint(f'\\n{\"=\"*70}')\nprint(f'ðŸ† FINAL RESULTS - KAN_ATTENTION (OPTIMIZED)')\nprint(f'{\"=\"*70}\\n')\nprint(f'   Recall:    {metrics_att[\"recall\"]:.4f} {\"ðŸŽ¯\" if metrics_att[\"recall\"] >= 0.80 else \"\"}')\nprint(f'   Precision: {metrics_att[\"precision\"]:.4f} {\"ðŸ’Ž\" if metrics_att[\"precision\"] >= 0.30 else \"\"}')\nprint(f'   F1 Score:  {metrics_att[\"f1\"]:.4f} {\"â­\" if metrics_att[\"f1\"] >= 0.45 else \"\"}')\nprint(f'   F2 Score:  {metrics_att[\"f2\"]:.4f} {\"â­\" if metrics_att[\"f2\"] >= 0.60 else \"\"}')\nprint(f'   Accuracy:  {metrics_att[\"accuracy\"]:.4f} {\"âœ…\" if metrics_att[\"accuracy\"] >= 0.60 else \"\"}')\nprint(f'   PR-AUC:    {metrics_att[\"pr_auc\"]:.4f} {\"ðŸ†\" if metrics_att[\"pr_auc\"] >= 0.42 else \"\"}')\nprint(f'\\n   Confusion Matrix:')\nprint(f'      TP={metrics_att[\"tp\"]}, FP={metrics_att[\"fp\"]}')\nprint(f'      FN={metrics_att[\"fn\"]}, TN={metrics_att[\"tn\"]}')\nprint(f'\\n   Threshold: {threshold_att:.3f}')\nprint(f'   Ensemble: {len(ensemble_models)} models (soft voting)')\nprint(f'{\"=\"*70}\\n')\n\nresults['KAN_Attention'] = {'metrics': metrics_att, 'threshold': threshold_att}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Step 8: Compare Results & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and display final results\n",
    "print('\\n' + '='*70)\n",
    "print('ðŸ“Š FINAL RESULTS - JM1')\n",
    "print('='*70 + '\\n')\n",
    "\n",
    "results_list = []\n",
    "for model_name, data in results.items():\n",
    "    m = data['metrics']\n",
    "    results_list.append({\n",
    "        'dataset': 'JM1',\n",
    "        'model': model_name,\n",
    "        'recall': m['recall'],\n",
    "        'precision': m['precision'],\n",
    "        'f1': m['f1'],\n",
    "        'f2': m['f2'],\n",
    "        'accuracy': m['accuracy'],\n",
    "        'pr_auc': m['pr_auc'],\n",
    "        'threshold': data['threshold']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Display results\n",
    "print('\\nðŸ“‹ Model Comparison:\\n')\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"{row['model']}:\")\n",
    "    print(f\"   Recall:    {row['recall']:.4f} {'ðŸŽ¯' if row['recall'] >= 0.80 else ''}\")\n",
    "    print(f\"   Precision: {row['precision']:.4f}\")\n",
    "    print(f\"   F2 Score:  {row['f2']:.4f} {'â­' if row['f2'] >= 0.75 else ''}\")\n",
    "    print(f\"   Accuracy:  {row['accuracy']:.4f}\")\n",
    "    print(f\"   Threshold: {row['threshold']:.2f}\\n\")\n",
    "\n",
    "# Export results\n",
    "csv_path = os.path.join(OUTPUT_DIR, f'results_{RUN_ID}.csv')\n",
    "json_path = os.path.join(OUTPUT_DIR, f'results_{RUN_ID}.json')\n",
    "\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "results_df.to_json(json_path, orient='records', indent=2)\n",
    "\n",
    "print('\\nðŸ’¾ Results saved to:')\n",
    "print(f'   CSV:  {csv_path}')\n",
    "print(f'   JSON: {json_path}')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('âœ… EXPERIMENT COMPLETE - JM1')\n",
    "print('='*70 + '\\n')\n",
    "\n",
    "# Display summary DataFrame\n",
    "print('\\nðŸ“Š Summary Table:')\n",
    "display(results_df[['model', 'recall', 'precision', 'f1', 'f2', 'accuracy', 'pr_auc']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}