{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Simple Attention-KAN for NASA Defect Prediction\n",
    "## Clean, Focused, Explainable\n",
    "\n",
    "**Strategy:** Simple and effective instead of complex\n",
    "\n",
    "**Key Features:**\n",
    "- ‚úÖ **Attention-KAN**: Built-in feature importance (XAI)\n",
    "- ‚úÖ **Weighted BCE Loss**: Simple but effective (FN cost = 3x)\n",
    "- ‚úÖ **Fast GWO**: Only 3 hyperparameters\n",
    "- ‚úÖ **Heatmaps**: Visualize what the model learns\n",
    "\n",
    "**Datasets:** PC1, CM1, KC1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from io import StringIO\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, roc_auc_score, fbeta_score, balanced_accuracy_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Random seeds\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"[INFO] Dependencies loaded!\")\n",
    "print(f\"[INFO] PyTorch: {torch.__version__}\")\n",
    "print(f\"[INFO] Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ATTENTION-KAN ARCHITECTURE\n",
    "# ============================================================================\n",
    "\n",
    "class FeatureAttention(nn.Module):\n",
    "    \"\"\"Learn which features are important\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features):\n",
    "        super(FeatureAttention, self).__init__()\n",
    "        hidden = max(in_features // 2, 8)\n",
    "        \n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden, in_features),\n",
    "            nn.Sigmoid()  # Attention weights [0,1]\n",
    "        )\n",
    "        self.bn = nn.BatchNorm1d(in_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_norm = self.bn(x)\n",
    "        weights = self.attention(x_norm)  # Learn importance\n",
    "        return x * weights, weights  # Weighted features, attention weights\n",
    "\n",
    "\n",
    "class KANLinear(nn.Module):\n",
    "    \"\"\"KAN layer with spline functions\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, grid_size=5):\n",
    "        super(KANLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.grid_size = grid_size\n",
    "        \n",
    "        # Learnable parameters\n",
    "        self.grid = nn.Parameter(torch.linspace(-1, 1, grid_size).unsqueeze(0).unsqueeze(0).repeat(out_features, in_features, 1))\n",
    "        self.coef = nn.Parameter(torch.randn(out_features, in_features, grid_size) * 0.1)\n",
    "        self.base_weight = nn.Parameter(torch.randn(out_features, in_features) * 0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x_exp = x.unsqueeze(1).unsqueeze(-1)\n",
    "        grid = self.grid.unsqueeze(0)\n",
    "        \n",
    "        # RBF basis\n",
    "        basis = torch.exp(-torch.abs(x_exp - grid) ** 2 / 0.5)\n",
    "        \n",
    "        # Spline output\n",
    "        coef = self.coef.unsqueeze(0)\n",
    "        spline_out = (basis * coef).sum(dim=-1).sum(dim=-1)\n",
    "        \n",
    "        # Base linear\n",
    "        base_out = torch.matmul(x, self.base_weight.t())\n",
    "        \n",
    "        return spline_out + base_out\n",
    "\n",
    "\n",
    "class AttentionKAN(nn.Module):\n",
    "    \"\"\"Simple KAN with Attention for XAI\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim=64, grid_size=5):\n",
    "        super(AttentionKAN, self).__init__()\n",
    "        \n",
    "        # Feature Attention (XAI)\n",
    "        self.attention = FeatureAttention(input_dim)\n",
    "        \n",
    "        # KAN layers\n",
    "        self.kan1 = KANLinear(input_dim, hidden_dim, grid_size)\n",
    "        self.kan2 = KANLinear(hidden_dim, hidden_dim // 2, grid_size)\n",
    "        \n",
    "        # Output\n",
    "        self.output = nn.Linear(hidden_dim // 2, 1)\n",
    "        \n",
    "        # Batch norm & dropout\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x, return_attention=False):\n",
    "        # Attention\n",
    "        x, att_weights = self.attention(x)\n",
    "        \n",
    "        # KAN layers\n",
    "        x = self.kan1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.kan2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Output\n",
    "        x = self.output(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        if return_attention:\n",
    "            return x, att_weights\n",
    "        return x\n",
    "    \n",
    "    def get_feature_importance(self, X):\n",
    "        \"\"\"Get global feature importance\"\"\"\n",
    "        self.eval()\n",
    "        if not isinstance(X, torch.Tensor):\n",
    "            X = torch.FloatTensor(X)\n",
    "        \n",
    "        device = next(self.parameters()).device\n",
    "        X = X.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, att_weights = self.attention(X)\n",
    "            importance = att_weights.cpu().numpy().mean(axis=0)\n",
    "        \n",
    "        return importance\n",
    "\n",
    "print(\"[INFO] Attention-KAN architecture ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# XAI VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def plot_feature_importance(model, X_data, dataset_name, top_k=15):\n",
    "    \"\"\"Plot feature importance heatmap\"\"\"\n",
    "    \n",
    "    importance = model.get_feature_importance(X_data)\n",
    "    feature_names = [f'F{i}' for i in range(len(importance))]\n",
    "    \n",
    "    # Sort by importance\n",
    "    sorted_idx = np.argsort(importance)[::-1][:top_k]\n",
    "    top_importance = importance[sorted_idx]\n",
    "    top_names = [feature_names[i] for i in sorted_idx]\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    colors = plt.cm.viridis(top_importance / top_importance.max())\n",
    "    bars = ax.barh(range(len(top_importance)), top_importance, color=colors)\n",
    "    \n",
    "    ax.set_yticks(range(len(top_importance)))\n",
    "    ax.set_yticklabels(top_names)\n",
    "    ax.set_xlabel('Attention Weight', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{dataset_name}: Top {top_k} Important Features', fontsize=14, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add values\n",
    "    for i, v in enumerate(top_importance):\n",
    "        ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{dataset_name}_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"[INFO] Saved: {dataset_name}_importance.png\")\n",
    "\n",
    "print(\"[INFO] XAI visualization ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_arff(file_path):\n",
    "    \"\"\"Load ARFF file\"\"\"\n",
    "    try:\n",
    "        data, meta = arff.loadarff(file_path)\n",
    "        df = pd.DataFrame(data)\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == object:\n",
    "                try:\n",
    "                    df[col] = df[col].str.decode('utf-8')\n",
    "                except:\n",
    "                    pass\n",
    "        return df\n",
    "    except:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "        data_start = content.lower().find('@data')\n",
    "        data_section = content[data_start + 5:].strip()\n",
    "        return pd.read_csv(StringIO(data_section), header=None)\n",
    "\n",
    "\n",
    "def prepare_data(df):\n",
    "    \"\"\"Prepare data for training\"\"\"\n",
    "    X = df.iloc[:, :-1].values.astype(np.float32)\n",
    "    y = df.iloc[:, -1].values\n",
    "    \n",
    "    # Encode labels\n",
    "    if y.dtype == object or y.dtype.name.startswith('str'):\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "    else:\n",
    "        y = y.astype(np.int32)\n",
    "    \n",
    "    # Handle NaN\n",
    "    if np.any(np.isnan(X)):\n",
    "        col_median = np.nanmedian(X, axis=0)\n",
    "        inds = np.where(np.isnan(X))\n",
    "        X[inds] = np.take(col_median, inds[1])\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    # Normalize\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # SMOTE (simple)\n",
    "    try:\n",
    "        smote = SMOTE(sampling_strategy=0.8, random_state=RANDOM_SEED)\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        print(f\"[INFO] After SMOTE: {X_train.shape[0]} samples, {np.bincount(y_train)}\")\n",
    "    except:\n",
    "        print(\"[WARNING] SMOTE failed, using original data\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "print(\"[INFO] Data loading ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING (SIMPLE WEIGHTED BCE)\n",
    "# ============================================================================\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, \n",
    "                lr=0.01, epochs=30, batch_size=32, fn_weight=3.0):\n",
    "    \"\"\"Train with Weighted BCE Loss\"\"\"\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    X_train_t = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_t = torch.FloatTensor(y_train).unsqueeze(1).to(device)\n",
    "    X_val_t = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_t = torch.FloatTensor(y_val).unsqueeze(1).to(device)\n",
    "    \n",
    "    dataset = TensorDataset(X_train_t, y_train_t)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Weighted BCE Loss (FN cost = fn_weight)\n",
    "    pos_weight = torch.tensor([fn_weight]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    print(f\"  [TRAINING] Weighted BCE Loss (FN weight={fn_weight})\")\n",
    "    \n",
    "    best_recall = 0\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            # Need logits for BCEWithLogitsLoss\n",
    "            logits = model.output(model.dropout(model.bn2(torch.relu(model.kan2(model.dropout(model.bn1(torch.relu(model.kan1(model.attention(batch_X)[0])))))))))\n",
    "            loss = criterion(logits, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(X_val_t)\n",
    "            val_pred = (val_out > 0.5).float().cpu().numpy()\n",
    "            val_recall = recall_score(y_val, val_pred, zero_division=0)\n",
    "        \n",
    "        if val_recall > best_recall:\n",
    "            best_recall = val_recall\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "    \n",
    "    print(f\"  [TRAINING] Best val recall: {best_recall:.4f}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(model, X_test, y_test):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    X_test_t = torch.FloatTensor(X_test).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_prob = model(X_test_t).cpu().numpy()\n",
    "        y_pred = (y_prob > 0.5).astype(int).flatten()\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"\\n  [CONFUSION MATRIX]\")\n",
    "    print(f\"  TN: {cm[0,0]}, FP: {cm[0,1]}\")\n",
    "    print(f\"  FN: {cm[1,0]}, TP: {cm[1,1]}\")\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'F1': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'F2': fbeta_score(y_test, y_pred, beta=2, zero_division=0),\n",
    "        'AUC': roc_auc_score(y_test, y_prob) if len(np.unique(y_test)) > 1 else 0\n",
    "    }\n",
    "\n",
    "print(\"[INFO] Training functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SIMPLE GWO (3 PARAMETERS ONLY)\n",
    "# ============================================================================\n",
    "\n",
    "class SimpleGWO:\n",
    "    \"\"\"Fast GWO for 3 hyperparameters\"\"\"\n",
    "    \n",
    "    def __init__(self, bounds, fitness_func, n_wolves=5, n_iter=6):\n",
    "        self.bounds = np.array(bounds)\n",
    "        self.fitness_func = fitness_func\n",
    "        self.n_wolves = n_wolves\n",
    "        self.n_iter = n_iter\n",
    "        self.dim = len(bounds)\n",
    "        \n",
    "        # Initialize\n",
    "        self.positions = np.random.uniform(\n",
    "            self.bounds[:, 0], \n",
    "            self.bounds[:, 1],\n",
    "            size=(n_wolves, self.dim)\n",
    "        )\n",
    "        \n",
    "        self.alpha_pos = np.zeros(self.dim)\n",
    "        self.alpha_score = float('-inf')\n",
    "        self.beta_pos = np.zeros(self.dim)\n",
    "        self.beta_score = float('-inf')\n",
    "        self.delta_pos = np.zeros(self.dim)\n",
    "        self.delta_score = float('-inf')\n",
    "    \n",
    "    def optimize(self):\n",
    "        print(f\"  [GWO] {self.n_wolves} wolves, {self.n_iter} iterations\")\n",
    "        \n",
    "        for it in range(self.n_iter):\n",
    "            # Evaluate fitness\n",
    "            for i in range(self.n_wolves):\n",
    "                fitness = self.fitness_func(self.positions[i])\n",
    "                \n",
    "                if fitness > self.alpha_score:\n",
    "                    self.delta_score = self.beta_score\n",
    "                    self.delta_pos = self.beta_pos.copy()\n",
    "                    self.beta_score = self.alpha_score\n",
    "                    self.beta_pos = self.alpha_pos.copy()\n",
    "                    self.alpha_score = fitness\n",
    "                    self.alpha_pos = self.positions[i].copy()\n",
    "                elif fitness > self.beta_score:\n",
    "                    self.delta_score = self.beta_score\n",
    "                    self.delta_pos = self.beta_pos.copy()\n",
    "                    self.beta_score = fitness\n",
    "                    self.beta_pos = self.positions[i].copy()\n",
    "                elif fitness > self.delta_score:\n",
    "                    self.delta_score = fitness\n",
    "                    self.delta_pos = self.positions[i].copy()\n",
    "            \n",
    "            # Update positions\n",
    "            a = 2 - it * (2.0 / self.n_iter)\n",
    "            \n",
    "            for i in range(self.n_wolves):\n",
    "                for j in range(self.dim):\n",
    "                    r1, r2 = np.random.random(2)\n",
    "                    A1 = 2 * a * r1 - a\n",
    "                    C1 = 2 * r2\n",
    "                    D_alpha = abs(C1 * self.alpha_pos[j] - self.positions[i, j])\n",
    "                    X1 = self.alpha_pos[j] - A1 * D_alpha\n",
    "                    \n",
    "                    r1, r2 = np.random.random(2)\n",
    "                    A2 = 2 * a * r1 - a\n",
    "                    C2 = 2 * r2\n",
    "                    D_beta = abs(C2 * self.beta_pos[j] - self.positions[i, j])\n",
    "                    X2 = self.beta_pos[j] - A2 * D_beta\n",
    "                    \n",
    "                    r1, r2 = np.random.random(2)\n",
    "                    A3 = 2 * a * r1 - a\n",
    "                    C3 = 2 * r2\n",
    "                    D_delta = abs(C3 * self.delta_pos[j] - self.positions[i, j])\n",
    "                    X3 = self.delta_pos[j] - A3 * D_delta\n",
    "                    \n",
    "                    self.positions[i, j] = (X1 + X2 + X3) / 3.0\n",
    "                    self.positions[i, j] = np.clip(\n",
    "                        self.positions[i, j],\n",
    "                        self.bounds[j, 0],\n",
    "                        self.bounds[j, 1]\n",
    "                    )\n",
    "            \n",
    "            print(f\"  Iter {it+1}/{self.n_iter} | Best: {self.alpha_score:.4f}\")\n",
    "        \n",
    "        return self.alpha_pos, self.alpha_score\n",
    "\n",
    "print(\"[INFO] Simple GWO ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MAIN EXECUTION - 3 DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "def run_experiment(dataset_dir='/content/drive/MyDrive/nasa-defect-gwo-kan/dataset'):\n",
    "    \"\"\"Run on PC1, CM1, KC1\"\"\"\n",
    "    \n",
    "    # Find datasets\n",
    "    target = ['PC1', 'CM1', 'KC1']\n",
    "    files = glob.glob(os.path.join(dataset_dir, '*.arff'))\n",
    "    files = [f for f in files if any(ds in os.path.basename(f).upper() for ds in target)]\n",
    "    \n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Datasets not found in {dataset_dir}\")\n",
    "    \n",
    "    print(f\"\\n[INFO] Found {len(files)} datasets\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for file_path in files:\n",
    "        dataset_name = os.path.basename(file_path).replace('.arff', '')\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(f\"DATASET: {dataset_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        try:\n",
    "            # Load data\n",
    "            print(\"[1/5] Loading data...\")\n",
    "            df = load_arff(file_path)\n",
    "            X_train, X_test, y_train, y_test = prepare_data(df)\n",
    "            \n",
    "            input_dim = X_train.shape[1]\n",
    "            print(f\"[INFO] Features: {input_dim}, Train: {len(y_train)}, Test: {len(y_test)}\")\n",
    "            \n",
    "            # Validation split\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X_train, y_train, test_size=0.2, stratify=y_train, random_state=RANDOM_SEED\n",
    "            )\n",
    "            \n",
    "            # GWO optimization (3 params only: hidden_dim, grid_size, lr)\n",
    "            print(\"\\n[2/5] GWO optimization...\")\n",
    "            \n",
    "            def fitness(params):\n",
    "                hidden_dim = int(params[0])\n",
    "                grid_size = int(params[1])\n",
    "                lr = params[2]\n",
    "                \n",
    "                try:\n",
    "                    model = AttentionKAN(input_dim, hidden_dim, grid_size)\n",
    "                    model = train_model(model, X_train, y_train, X_val, y_val, \n",
    "                                      lr=lr, epochs=20, fn_weight=3.0)\n",
    "                    metrics = evaluate(model, X_val, y_val)\n",
    "                    \n",
    "                    # Fitness: 60% Recall + 30% F1 + 10% Acc\n",
    "                    score = 0.6 * metrics['Recall'] + 0.3 * metrics['F1'] + 0.1 * metrics['Accuracy']\n",
    "                    return score\n",
    "                except:\n",
    "                    return 0.0\n",
    "            \n",
    "            bounds = [\n",
    "                (32, 96),      # hidden_dim\n",
    "                (3, 7),        # grid_size\n",
    "                (0.005, 0.02)  # learning_rate\n",
    "            ]\n",
    "            \n",
    "            gwo = SimpleGWO(bounds, fitness, n_wolves=5, n_iter=6)\n",
    "            best_params, best_score = gwo.optimize()\n",
    "            \n",
    "            hidden_dim = int(best_params[0])\n",
    "            grid_size = int(best_params[1])\n",
    "            lr = best_params[2]\n",
    "            \n",
    "            print(f\"\\n  [GWO] Best params: hidden={hidden_dim}, grid={grid_size}, lr={lr:.4f}\")\n",
    "            print(f\"  [GWO] Best score: {best_score:.4f}\")\n",
    "            \n",
    "            # Train final model\n",
    "            print(\"\\n[3/5] Training final model...\")\n",
    "            model = AttentionKAN(input_dim, hidden_dim, grid_size)\n",
    "            model = train_model(model, X_train, y_train, X_val, y_val, \n",
    "                              lr=lr, epochs=30, fn_weight=3.0)\n",
    "            \n",
    "            # Evaluate\n",
    "            print(\"\\n[4/5] Evaluating...\")\n",
    "            metrics = evaluate(model, X_test, y_test)\n",
    "            \n",
    "            print(f\"\\n  [RESULTS]\")\n",
    "            for k, v in metrics.items():\n",
    "                print(f\"  {k}: {v:.4f}\")\n",
    "            \n",
    "            # Visualize feature importance\n",
    "            print(\"\\n[5/5] Creating heatmap...\")\n",
    "            plot_feature_importance(model, X_test, dataset_name, top_k=15)\n",
    "            \n",
    "            # Save results\n",
    "            results.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Features': input_dim,\n",
    "                'Hidden_Dim': hidden_dim,\n",
    "                'Grid_Size': grid_size,\n",
    "                'Learning_Rate': lr,\n",
    "                **metrics\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n  [ERROR] {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Summary\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Average\n",
    "    avg_row = {'Dataset': 'AVERAGE'}\n",
    "    for col in ['Accuracy', 'Precision', 'Recall', 'F1', 'F2', 'AUC']:\n",
    "        if col in results_df.columns:\n",
    "            avg_row[col] = results_df[col].mean()\n",
    "    \n",
    "    results_df = pd.concat([results_df, pd.DataFrame([avg_row])], ignore_index=True)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "print(\"[INFO] Main execution ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RUN!\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" üöÄ SIMPLE ATTENTION-KAN - NASA DEFECT PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüìã APPROACH:\")\n",
    "print(\"  ‚úÖ Attention-KAN (built-in XAI)\")\n",
    "print(\"  ‚úÖ Weighted BCE Loss (FN cost=3x)\")\n",
    "print(\"  ‚úÖ Fast GWO (3 hyperparameters)\")\n",
    "print(\"  ‚úÖ Feature importance heatmaps\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Run experiment\n",
    "results = run_experiment(\n",
    "    dataset_dir='/content/drive/MyDrive/nasa-defect-gwo-kan/dataset'\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" üìä FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# Save\n",
    "results.to_excel('simple_attention_kan_results.xlsx', index=False)\n",
    "print(\"\\n[INFO] Results saved: simple_attention_kan_results.xlsx\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" üéØ AVERAGE METRICS\")\n",
    "print(\"=\"*70)\n",
    "avg = results[results['Dataset'] == 'AVERAGE'].iloc[0]\n",
    "print(f\"\\n  Accuracy:  {avg['Accuracy']:.4f}\")\n",
    "print(f\"  Precision: {avg['Precision']:.4f}\")\n",
    "print(f\"  Recall:    {avg['Recall']:.4f} ‚≠ê\")\n",
    "print(f\"  F1-Score:  {avg['F1']:.4f}\")\n",
    "print(f\"  F2-Score:  {avg['F2']:.4f}\")\n",
    "print(f\"  AUC:       {avg['AUC']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" ‚úÖ COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION - COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "fig.suptitle('Simple Attention-KAN Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'F2', 'AUC']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c']\n",
    "\n",
    "plot_data = results[results['Dataset'] != 'AVERAGE'].copy()\n",
    "\n",
    "for idx, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    if metric in plot_data.columns:\n",
    "        ax.barh(plot_data['Dataset'], plot_data[metric], color=color, alpha=0.7)\n",
    "        ax.set_xlabel(metric, fontsize=11, fontweight='bold')\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        if metric == 'Recall':\n",
    "            ax.set_facecolor('#ffe6e6')\n",
    "            ax.set_title('‚≠ê PRIMARY ‚≠ê', fontsize=10, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('simple_results_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"[INFO] Saved: simple_results_comparison.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
