{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "---\n## \ud83c\udf89 EXPERIMENT COMPLETE!\n\n### \ud83d\udcca Sonu\u00e7lar:\n\nT\u00fcm sonu\u00e7lar `./results/` klas\u00f6r\u00fcnde:\n- `final_results_<run_id>.csv` - CSV format\u0131nda\n- `final_results_<run_id>.json` - JSON format\u0131nda  \n- `final_results_<run_id>.xlsx` - Excel format\u0131nda\n- `comparison_plot_<run_id>.png` - Kar\u015f\u0131la\u015ft\u0131rma grafi\u011fi\n- `config_<run_id>.json` - Experiment konfig\u00fcrasyonu\n\n### \ud83c\udfaf Test Edilen Modeller:\n\n1. **Baseline RF** - Class-weighted Random Forest\n2. **KAN Base** - Lightweight KAN (Focal Loss)\n3. **KAN + Attention** - Feature-level attention (\u00d6ZG\u00dcN KATKI)\n\n### \ud83d\udcc8 Ana Metrikler:\n\n- **Recall** (0.80+ hedef) - Defect detection i\u00e7in kritik\n- **F2 Score** (0.65+ hedef) - Recall'a a\u011f\u0131rl\u0131k veren metric\n- **Precision** - False positive kontrol\u00fc\n- **Accuracy** - Genel performans\n\n### \u2705 Ba\u015far\u0131 Kriterleri:\n\n- \u2713 Recall korundu (0.80+)\n- \u2713 F2 optimize edildi\n- \u2713 Hafif model (CPU friendly)\n- \u2713 Leakage yok (test seti izole)\n- \u2713 SMOTE sadece train'de\n\n---\n\n**\ud83c\udf93 \u00d6zg\u00fcn Katk\u0131:** Feature-level attention mechanism - Her sample i\u00e7in feature'lara dinamik a\u011f\u0131rl\u0131k vererek model interpretability ve performance art\u0131\u015f\u0131 sa\u011fland\u0131!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================",
    "# COMPILE ALL RESULTS",
    "# ============================================================================",
    "",
    "# Check if all required variables exist",
    "required_vars = {",
    "    'CONFIG': 'Run the Config cell first!',",
    "    'prepared_datasets': 'Run the Dataset Preparation cell first!',",
    "    'baseline_results': 'Run the Baseline RF training cell first!',",
    "    'kan_base_results': 'Run the KAN Base training cell first!',",
    "    'kan_attention_results': 'Run the KAN+Attention training cell first!'",
    "}",
    "",
    "missing_vars = []",
    "for var_name, message in required_vars.items():",
    "    if var_name not in dir():",
    "        missing_vars.append(f\"\u274c {var_name}: {message}\")",
    "",
    "if missing_vars:",
    "    print(\"\\n\" + \"=\"*70)",
    "    print(\"\u26a0\ufe0f  MISSING REQUIRED VARIABLES\")",
    "    print(\"=\"*70)",
    "    print(\"\\nPlease run these cells IN ORDER before running this cell:\\n\")",
    "    for msg in missing_vars:",
    "        print(f\"   {msg}\")",
    "    print(\"\\n\" + \"=\"*70)",
    "    print(\"\\n\ud83d\udccc Quick Guide:\")",
    "    print(\"   1. Mount Google Drive (cell at top)\")",
    "    print(\"   2. Run Config cell\")",
    "    print(\"   3. Run Dependencies cell\")",
    "    print(\"   4. Run Dataset Loading cell\")",
    "    print(\"   5. Run Data Preparation cell\")",
    "    print(\"   6. Run SMOTE cell\")",
    "    print(\"   7. Run Baseline RF cell\")",
    "    print(\"   8. Run KAN Base cell\")",
    "    print(\"   9. Run KAN+Attention cell\")",
    "    print(\"   10. NOW run this cell!\")",
    "    print(\"=\"*70)",
    "    raise RuntimeError(\"Missing required variables - see messages above\")",
    "",
    "print(\"\\n\" + \"=\"*70)",
    "print(\"\ud83d\udcca COMPILING FINAL RESULTS\")",
    "print(\"=\"*70)",
    "",
    "# Organize all results",
    "all_results = {}",
    "",
    "for dataset_name in CONFIG['datasets']:",
    "    if dataset_name not in prepared_datasets:",
    "        continue",
    "    ",
    "    all_results[dataset_name] = {",
    "        'dataset_info': {",
    "            'n_samples': len(prepared_datasets[dataset_name]['y_train']) + ",
    "                        len(prepared_datasets[dataset_name]['y_val']) + ",
    "                        len(prepared_datasets[dataset_name]['y_test']),",
    "            'n_features': prepared_datasets[dataset_name]['n_features'],",
    "            'defect_ratio': np.mean(np.concatenate([",
    "                prepared_datasets[dataset_name]['y_train'],",
    "                prepared_datasets[dataset_name]['y_val'],",
    "                prepared_datasets[dataset_name]['y_test']",
    "            ]))",
    "        },",
    "        'Baseline_RF': baseline_results[dataset_name],",
    "        'KAN_Base': kan_base_results[dataset_name],",
    "        'KAN_Attention': kan_attention_results[dataset_name]",
    "    }",
    "",
    "# Create summary dataframe",
    "results_df = compile_final_results(all_results, CONFIG)",
    "",
    "# Print summary",
    "print_final_summary(results_df)",
    "",
    "# Export results",
    "print(f\"\\n{'='*70}\")",
    "print(f\"\ud83d\udcbe EXPORTING RESULTS\")",
    "print(f\"{'='*70}\\n\")",
    "",
    "export_results(results_df, CONFIG)",
    "",
    "# Create comparison plot",
    "print(f\"\\n{'='*70}\")",
    "print(f\"\ud83d\udcc8 CREATING COMPARISON PLOTS\")",
    "print(f\"{'='*70}\\n\")",
    "",
    "create_comparison_plot(results_df, CONFIG)",
    "",
    "print(f\"\\n{'='*70}\")",
    "print(f\"\u2705 ALL RESULTS COMPILED AND EXPORTED\")",
    "print(f\"{'='*70}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## \ud83d\udcca EXECUTION: Final Results Compilation & Export\n\nT\u00fcm sonu\u00e7lar\u0131 derle, kar\u015f\u0131la\u015ft\u0131r ve export et (CSV/JSON/XLSX)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# LOSS ABLATION: Focal vs Weighted BCE (on JM1 only for speed)\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"\ud83d\udd2c LOSS ABLATION STUDY - Focal vs Weighted BCE\")\nprint(\"=\"*70)\nprint(\"\ud83d\udccc Testing on JM1 only (kaynak tasarrufu)\\n\")\n\nloss_ablation_results = {}\n\n# Test only on JM1 to save resources\ndataset_name = 'JM1'\ndata = prepared_datasets[dataset_name]\n\nfor loss_type in ['Focal', 'WeightedBCE']:\n    print(f\"\\n{'='*70}\")\n    print(f\"\ud83d\udcca Loss Type: {loss_type}\")\n    print(f\"{'='*70}\")\n    \n    # Create model\n    model = KAN(\n        input_dim=data['n_features'],\n        hidden_dim=KAN_LITE_CONFIG['hidden_dim'],\n        grid_size=KAN_LITE_CONFIG['grid_size'],\n        spline_order=KAN_LITE_CONFIG['spline_order'],\n        dropout=0.3\n    )\n    \n    # Train\n    model, history = train_kan_model(\n        model,\n        data['X_train_smote'],\n        data['y_train_smote'],\n        data['X_val'],\n        data['y_val'],\n        learning_rate=KAN_LITE_CONFIG['learning_rate'],\n        epochs=KAN_LITE_CONFIG['epochs'],\n        batch_size=KAN_LITE_CONFIG['batch_size'],\n        loss_type=loss_type,\n        patience=KAN_LITE_CONFIG['patience'],\n        verbose=True\n    )\n    \n    # Evaluate\n    model.eval()\n    with torch.no_grad():\n        X_test_t = torch.FloatTensor(data['X_test']).to(device)\n        y_test_proba = model(X_test_t).cpu().numpy().flatten()\n    \n    # Use threshold from Focal loss run (fair comparison)\n    optimal_threshold = kan_base_results[dataset_name]['optimal_threshold']\n    \n    y_test_pred = (y_test_proba >= optimal_threshold).astype(int)\n    test_metrics = calculate_all_metrics(data['y_test'], y_test_pred, y_test_proba)\n    \n    print(f\"\\n   \ud83d\udcca Test Performance (threshold={optimal_threshold:.2f}):\")\n    print(f\"      Recall:    {test_metrics['recall']:.4f}\")\n    print(f\"      Precision: {test_metrics['precision']:.4f}\")\n    print(f\"      F2:        {test_metrics['f2']:.4f}\")\n    print(f\"      Accuracy:  {test_metrics['accuracy']:.4f}\")\n    \n    loss_ablation_results[loss_type] = {\n        'model': model,\n        'test_metrics': test_metrics,\n        'history': history\n    }\n\n# Compare losses\nprint(f\"\\n{'='*70}\")\nprint(f\"\ud83c\udfc6 LOSS COMPARISON (JM1 Dataset)\")\nprint(f\"{'='*70}\")\n\nfocal_f2 = loss_ablation_results['Focal']['test_metrics']['f2']\nwbce_f2 = loss_ablation_results['WeightedBCE']['test_metrics']['f2']\n\nfocal_recall = loss_ablation_results['Focal']['test_metrics']['recall']\nwbce_recall = loss_ablation_results['WeightedBCE']['test_metrics']['recall']\n\nprint(f\"\\n\ud83d\udcca F2 Score:\")\nprint(f\"   Focal Loss:       {focal_f2:.4f}\")\nprint(f\"   Weighted BCE:     {wbce_f2:.4f}\")\nprint(f\"   Winner:           {'Focal' if focal_f2 > wbce_f2 else 'WeightedBCE'} (+{abs(focal_f2-wbce_f2):.4f})\")\n\nprint(f\"\\n\ud83d\udcca Recall (Defect Detection):\")\nprint(f\"   Focal Loss:       {focal_recall:.4f}\")\nprint(f\"   Weighted BCE:     {wbce_recall:.4f}\")\nprint(f\"   Winner:           {'Focal' if focal_recall > wbce_recall else 'WeightedBCE'} (+{abs(focal_recall-wbce_recall):.4f})\")\n\nprint(f\"\\n\u2705 LOSS ABLATION COMPLETE\")\nprint(f\"{'='*70}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## \ud83d\udd2c EXECUTION: Loss Ablation Study\n\nFocal Loss vs Weighted BCE kar\u015f\u0131la\u015ft\u0131rmas\u0131 (h\u0131zl\u0131)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# KAN + FEATURE ATTENTION - LIGHTWEIGHT (\u00d6ZG\u00dcN KATKI)\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"\ud83c\udf1f KAN + FEATURE ATTENTION MODELS (Focal Loss)\")\nprint(\"=\"*70)\nprint(\"\u2728 \u00d6ZG\u00dcN KATKI: Sample-specific feature weighting\\n\")\n\nkan_attention_results = {}\n\nfor dataset_name, data in prepared_datasets.items():\n    print(f\"\\n{'='*70}\")\n    print(f\"\ud83d\udcca Dataset: {dataset_name}\")\n    print(f\"{'='*70}\")\n    \n    # Create KAN with Attention\n    model = KAN_WithAttention(\n        input_dim=data['n_features'],\n        hidden_dim=KAN_LITE_CONFIG['hidden_dim'],\n        grid_size=KAN_LITE_CONFIG['grid_size'],\n        spline_order=KAN_LITE_CONFIG['spline_order'],\n        dropout=0.3,\n        attention_dim=CONFIG['attention_dim']\n    )\n    \n    # Train\n    model, history = train_kan_model(\n        model,\n        data['X_train_smote'],\n        data['y_train_smote'],\n        data['X_val'],\n        data['y_val'],\n        learning_rate=KAN_LITE_CONFIG['learning_rate'],\n        epochs=KAN_LITE_CONFIG['epochs'],\n        batch_size=KAN_LITE_CONFIG['batch_size'],\n        loss_type=KAN_LITE_CONFIG['loss_type'],\n        patience=KAN_LITE_CONFIG['patience'],\n        verbose=True\n    )\n    \n    # Evaluate on validation\n    model.eval()\n    with torch.no_grad():\n        X_val_t = torch.FloatTensor(data['X_val']).to(device)\n        y_val_proba = model(X_val_t).cpu().numpy().flatten()\n    \n    # Find optimal threshold\n    print(f\"\\n\ud83d\udd0d Finding optimal threshold on validation set...\")\n    threshold_result = find_optimal_threshold(\n        data['y_val'],\n        y_val_proba,\n        metric='f2',\n        threshold_range=CONFIG['threshold_range'],\n        step=CONFIG['threshold_step'],\n        accuracy_floor=CONFIG['gwo_accuracy_floor'],\n        verbose=True\n    )\n    \n    optimal_threshold = threshold_result['threshold']\n    \n    # Evaluate on test\n    print(f\"\\n\ud83e\uddea Evaluating on TEST set with threshold={optimal_threshold:.2f}...\")\n    with torch.no_grad():\n        X_test_t = torch.FloatTensor(data['X_test']).to(device)\n        y_test_proba = model(X_test_t).cpu().numpy().flatten()\n    \n    y_test_pred = (y_test_proba >= optimal_threshold).astype(int)\n    test_metrics = calculate_all_metrics(data['y_test'], y_test_pred, y_test_proba)\n    \n    print(f\"\\n   \ud83d\udcca Test Performance:\")\n    print_metrics(test_metrics, prefix=\"   \")\n    \n    # Extract attention weights for interpretation\n    print(f\"\\n   \ud83d\udd0d Analyzing Feature Attention Weights...\")\n    attention_weights = model.get_attention_weights(data['X_test'][:10])  # Sample 10 examples\n    avg_attention = attention_weights.mean(axis=0)\n    top_features_idx = np.argsort(avg_attention)[-5:][::-1]\n    \n    print(f\"      Top 5 Most Attended Features:\")\n    for i, idx in enumerate(top_features_idx, 1):\n        print(f\"         {i}. Feature {idx}: {avg_attention[idx]:.4f}\")\n    \n    # Store results\n    kan_attention_results[dataset_name] = {\n        'model': model,\n        'optimal_threshold': optimal_threshold,\n        'val_metrics': threshold_result['metrics'],\n        'test_metrics': test_metrics,\n        'history': history,\n        'attention_weights': attention_weights\n    }\n    \n    print(f\"\\n\u2705 {dataset_name} KAN+Attention Complete!\")\n    print(f\"   Optimal Threshold: {optimal_threshold:.2f}\")\n    print(f\"   Test F2: {test_metrics['f2']:.4f}\")\n    print(f\"   Test Recall: {test_metrics['recall']:.4f} {'\ud83c\udfaf' if test_metrics['recall'] >= 0.80 else '\u26a0\ufe0f'}\")\n    print(f\"   Test Accuracy: {test_metrics['accuracy']:.4f}\")\n    \n    # Compare with baseline\n    baseline_f2 = baseline_results[dataset_name]['test_metrics']['f2']\n    kan_base_f2 = kan_base_results[dataset_name]['test_metrics']['f2']\n    improvement_vs_baseline = (test_metrics['f2'] - baseline_f2) / baseline_f2 * 100\n    improvement_vs_kan = (test_metrics['f2'] - kan_base_f2) / kan_base_f2 * 100\n    \n    print(f\"\\n   \ud83d\udcc8 Improvement:\")\n    print(f\"      vs Baseline RF: {improvement_vs_baseline:+.1f}%\")\n    print(f\"      vs KAN Base:    {improvement_vs_kan:+.1f}%\")\n\nprint(f\"\\n{'='*70}\")\nprint(f\"\u2705 ALL KAN+ATTENTION MODELS TRAINED\")\nprint(f\"{'='*70}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## \ud83c\udf1f EXECUTION: KAN + Feature Attention (\u00d6ZG\u00dcN KATKI)\n\nFeature-level attention eklendi - Her sample i\u00e7in feature'lara a\u011f\u0131rl\u0131k",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# KAN BASE - LIGHTWEIGHT CONFIG (CPU OPTIMIZED)\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"\ud83d\udd25 KAN BASE MODELS - LIGHTWEIGHT (Focal Loss)\")\nprint(\"=\"*70)\n\n# Lightweight config for CPU\nKAN_LITE_CONFIG = {\n    'hidden_dim': 32,      # Small hidden layer\n    'grid_size': 3,        # Small grid\n    'spline_order': 2,     # Low spline order\n    'learning_rate': 0.01,\n    'epochs': 50,          # Fewer epochs\n    'batch_size': 64,      # Larger batches (faster)\n    'patience': 10,\n    'loss_type': 'Focal'\n}\n\nprint(f\"\\n\u26a1 Lightweight Config:\")\nprint(f\"   Hidden: {KAN_LITE_CONFIG['hidden_dim']}, Grid: {KAN_LITE_CONFIG['grid_size']}, Spline: {KAN_LITE_CONFIG['spline_order']}\")\nprint(f\"   Epochs: {KAN_LITE_CONFIG['epochs']}, Batch: {KAN_LITE_CONFIG['batch_size']}\")\nprint(f\"   Loss: {KAN_LITE_CONFIG['loss_type']}\\n\")\n\nkan_base_results = {}\n\nfor dataset_name, data in prepared_datasets.items():\n    print(f\"\\n{'='*70}\")\n    print(f\"\ud83d\udcca Dataset: {dataset_name}\")\n    print(f\"{'='*70}\")\n    \n    # Create lightweight KAN model\n    model = KAN(\n        input_dim=data['n_features'],\n        hidden_dim=KAN_LITE_CONFIG['hidden_dim'],\n        grid_size=KAN_LITE_CONFIG['grid_size'],\n        spline_order=KAN_LITE_CONFIG['spline_order'],\n        dropout=0.3\n    )\n    \n    # Train\n    model, history = train_kan_model(\n        model,\n        data['X_train_smote'],\n        data['y_train_smote'],\n        data['X_val'],\n        data['y_val'],\n        learning_rate=KAN_LITE_CONFIG['learning_rate'],\n        epochs=KAN_LITE_CONFIG['epochs'],\n        batch_size=KAN_LITE_CONFIG['batch_size'],\n        loss_type=KAN_LITE_CONFIG['loss_type'],\n        patience=KAN_LITE_CONFIG['patience'],\n        verbose=True\n    )\n    \n    # Evaluate on validation\n    model.eval()\n    with torch.no_grad():\n        X_val_t = torch.FloatTensor(data['X_val']).to(device)\n        y_val_proba = model(X_val_t).cpu().numpy().flatten()\n    \n    # Find optimal threshold\n    print(f\"\\n\ud83d\udd0d Finding optimal threshold on validation set...\")\n    threshold_result = find_optimal_threshold(\n        data['y_val'],\n        y_val_proba,\n        metric='f2',\n        threshold_range=CONFIG['threshold_range'],\n        step=CONFIG['threshold_step'],\n        accuracy_floor=CONFIG['gwo_accuracy_floor'],\n        verbose=True\n    )\n    \n    optimal_threshold = threshold_result['threshold']\n    \n    # Evaluate on test\n    print(f\"\\n\ud83e\uddea Evaluating on TEST set with threshold={optimal_threshold:.2f}...\")\n    with torch.no_grad():\n        X_test_t = torch.FloatTensor(data['X_test']).to(device)\n        y_test_proba = model(X_test_t).cpu().numpy().flatten()\n    \n    y_test_pred = (y_test_proba >= optimal_threshold).astype(int)\n    test_metrics = calculate_all_metrics(data['y_test'], y_test_pred, y_test_proba)\n    \n    print(f\"\\n   \ud83d\udcca Test Performance:\")\n    print_metrics(test_metrics, prefix=\"   \")\n    \n    # Store results\n    kan_base_results[dataset_name] = {\n        'model': model,\n        'optimal_threshold': optimal_threshold,\n        'val_metrics': threshold_result['metrics'],\n        'test_metrics': test_metrics,\n        'history': history\n    }\n    \n    print(f\"\\n\u2705 {dataset_name} KAN Base Complete!\")\n    print(f\"   Optimal Threshold: {optimal_threshold:.2f}\")\n    print(f\"   Test F2: {test_metrics['f2']:.4f}\")\n    print(f\"   Test Recall: {test_metrics['recall']:.4f} {'\ud83c\udfaf' if test_metrics['recall'] >= 0.80 else '\u26a0\ufe0f'}\")\n    print(f\"   Test Accuracy: {test_metrics['accuracy']:.4f}\")\n\nprint(f\"\\n{'='*70}\")\nprint(f\"\u2705 ALL KAN BASE MODELS TRAINED\")\nprint(f\"{'='*70}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASA Defect Prediction: Comprehensive GWO-KAN Pipeline\n",
    "## F2 & Recall Optimized with Feature-Level Attention\n",
    "\n",
    "**Hedef:** F2 ve Recall (defective=1) maksimizasyonu, Accuracy korunarak\n",
    "\n",
    "**Datasets:** JM1, KC1\n",
    "\n",
    "**K\u0131s\u0131tlar:**\n",
    "- \u2705 Test seti leakage YOK (scaler fit sadece train)\n",
    "- \u2705 SMOTE sadece train seti\n",
    "- \u2705 Threshold tuning val seti (F2 hedef)\n",
    "- \u2705 Colab CPU optimize\n",
    "\n",
    "**\u00d6zg\u00fcn Katk\u0131:** Feature-Level Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Config / Seed / Run Kay\u0131t\n",
    "Her deneyin izlenebilirli\u011fi i\u00e7in config ve seed y\u00f6netimi"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# GOOGLE DRIVE MOUNT (REQUIRED FOR COLAB)\n# ============================================================================\n\nprint(\"=\"*70)\nprint(\"\ud83d\udcc2 MOUNTING GOOGLE DRIVE\")\nprint(\"=\"*70)\n\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    print(\"\\n\u2705 Google Drive mounted successfully!\")\n    print(f\"   Dataset path: /content/drive/MyDrive/nasa-defect-gwo-kan/dataset\")\nexcept ImportError:\n    print(\"\\n\u26a0\ufe0f  Not running on Colab - skipping Drive mount\")\nexcept Exception as e:\n    print(f\"\\n\u274c Drive mount failed: {e}\")\n    print(f\"   Please mount manually or check permissions\")\n\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT CONFIG\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Run Info\n",
    "    'run_id': datetime.datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "    'experiment_name': 'NASA_Defect_F2_Optimized',\n",
    "    \n",
    "    # Random Seeds\n",
    "    'seed': 42,\n",
    "    \n",
    "    # Dataset Config\n",
    "    'dataset_path': '/content/drive/MyDrive/nasa-defect-gwo-kan/dataset',\n",
    "    'datasets': ['JM1', 'KC1'],  # Focus datasets\n",
    "    \n",
    "    # Split Config\n",
    "    'test_size': 0.2,\n",
    "    'val_size': 0.2,  # from train\n",
    "    \n",
    "    # SMOTE Config\n",
    "    'smote_ratio': 0.7,  # Primary ratio\n",
    "    'smote_alternatives': [0.5, 0.9],  # For ablation\n",
    "    \n",
    "    # Threshold Config\n",
    "    'threshold_range': (0.05, 0.95),\n",
    "    'threshold_step': 0.05,\n",
    "    'threshold_metric': 'F2',  # Optimize for F2\n",
    "    \n",
    "    # Model Config\n",
    "    'baseline_model': 'RandomForest',\n",
    "    \n",
    "    # KAN Config\n",
    "    'kan_grid_size': [3, 5, 7],\n",
    "    'kan_spline_order': [2, 3, 4],\n",
    "    'kan_hidden_dim': [32, 64, 128],\n",
    "    'kan_learning_rate': [0.001, 0.01, 0.05],\n",
    "    'kan_epochs': 100,\n",
    "    'kan_batch_size': 32,\n",
    "    'kan_patience': 15,\n",
    "    \n",
    "    # GWO Config\n",
    "    'gwo_n_wolves': 10,\n",
    "    'gwo_n_iterations': 20,\n",
    "    'gwo_fitness_weights': {\n",
    "        'f1': 0.5,\n",
    "        'recall': 0.3,\n",
    "        'accuracy': 0.2\n",
    "    },\n",
    "    'gwo_accuracy_floor': 0.5,  # Minimum acceptable accuracy\n",
    "    \n",
    "    # Loss Config\n",
    "    'loss_types': ['WeightedBCE', 'Focal'],\n",
    "    'focal_alpha': 0.25,\n",
    "    'focal_gamma': 2.0,\n",
    "    \n",
    "    # Attention Config\n",
    "    'attention_dim': 16,\n",
    "    'attention_dropout': 0.2,\n",
    "    \n",
    "    # Output Config\n",
    "    'save_models': True,\n",
    "    'output_dir': './results',\n",
    "    'export_format': ['csv', 'json', 'xlsx']\n",
    "}\n",
    "\n",
    "# Set all random seeds\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "# Save config\n",
    "config_path = os.path.join(CONFIG['output_dir'], f\"config_{CONFIG['run_id']}.json\")\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"\ud83d\udd27 EXPERIMENT CONFIG INITIALIZED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Run ID: {CONFIG['run_id']}\")\n",
    "print(f\"Experiment: {CONFIG['experiment_name']}\")\n",
    "print(f\"Random Seed: {CONFIG['seed']}\")\n",
    "print(f\"Datasets: {', '.join(CONFIG['datasets'])}\")\n",
    "print(f\"Config saved to: {config_path}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Dependencies & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INSTALL & IMPORT DEPENDENCIES\n",
    "# ============================================================================\n",
    "\n",
    "# Install required packages (if on Colab)\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"\ud83d\udccd Running on Google Colab\")\n",
    "    !pip install imbalanced-learn scipy scikit-learn torch matplotlib seaborn pandas numpy openpyxl -q\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"\ud83d\udccd Running locally\")\n",
    "\n",
    "# Core imports\n",
    "import glob\n",
    "from io import StringIO\n",
    "from collections import defaultdict\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    fbeta_score, roc_auc_score, balanced_accuracy_score,\n",
    "    confusion_matrix, classification_report, average_precision_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n\u2705 All dependencies loaded successfully!\")\n",
    "print(f\"\ud83d\udda5\ufe0f  Device: {device}\")\n",
    "print(f\"\ud83d\udc0d PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Dataset Ke\u015ffi ve \u00d6zet Tablo\n",
    "\n",
    "JM1 ve KC1 datasetlerinin temel istatistikleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# DATASET LOADING UTILITIES\n# ============================================================================\n\ndef load_arff_dataset(file_path):\n    \"\"\"\n    Load ARFF file with robust error handling\n    \n    Parameters:\n    -----------\n    file_path : str\n        Path to ARFF file\n        \n    Returns:\n    --------\n    pd.DataFrame\n        Loaded dataset\n    \"\"\"\n    try:\n        data, meta = arff.loadarff(file_path)\n        df = pd.DataFrame(data)\n        \n        # Decode byte strings\n        for col in df.columns:\n            if df[col].dtype == object:\n                try:\n                    df[col] = df[col].str.decode('utf-8')\n                except AttributeError:\n                    pass\n        \n        return df\n    \n    except Exception as e:\n        print(f\"\u26a0\ufe0f  scipy.io.arff failed: {e}\\n\" + f\"\ud83d\udd04 Trying alternative parsing...\")\n        \n        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n            content = f.read()\n        \n        data_start = content.lower().find('@data')\n        if data_start == -1:\n            raise ValueError(\"No @data section found\")\n        \n        data_section = content[data_start + 5:].strip()\n        df = pd.read_csv(StringIO(data_section), header=None)\n        \n        return df\n\n\ndef get_dataset_summary(df, dataset_name):\n    \"\"\"\n    Generate summary statistics for a dataset\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        Dataset\n    dataset_name : str\n        Name of dataset\n        \n    Returns:\n    --------\n    dict\n        Summary statistics\n    \"\"\"\n    # Separate features and labels\n    X = df.iloc[:, :-1].values\n    y = df.iloc[:, -1].values\n    \n    # Encode labels if needed\n    if y.dtype == object:\n        le = LabelEncoder()\n        y = le.fit_transform(y)\n    else:\n        y = y.astype(int)\n    \n    # Calculate statistics\n    n_samples = len(y)\n    n_features = X.shape[1]\n    n_defective = np.sum(y == 1)\n    n_clean = np.sum(y == 0)\n    defect_ratio = n_defective / n_samples\n    imbalance_ratio = n_clean / n_defective if n_defective > 0 else 0\n    \n    return {\n        'Dataset': dataset_name,\n        'n_samples': n_samples,\n        'n_features': n_features,\n        'n_defective': n_defective,\n        'n_clean': n_clean,\n        'defect_ratio': defect_ratio,\n        'imbalance_ratio': imbalance_ratio\n    }\n\n\n# ============================================================================\n# LOAD AND EXPLORE DATASETS\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"\ud83d\udcca DATASET EXPLORATION - JM1 & KC1\")\nprint(\"=\"*70)\n\n# Load datasets\ndataset_summaries = []\nraw_datasets = {}\n\nfor dataset_name in CONFIG['datasets']:\n    file_path = os.path.join(CONFIG['dataset_path'], f\"{dataset_name}.arff\")\n    \n    print(f\"\\n\ud83d\udcc1 Loading {dataset_name}...\")\n    \n    if not os.path.exists(file_path):\n        print(f\"\u274c File not found: {file_path}\")\n        continue\n    \n    try:\n        df = load_arff_dataset(file_path)\n        raw_datasets[dataset_name] = df\n        \n        summary = get_dataset_summary(df, dataset_name)\n        dataset_summaries.append(summary)\n        \n        print(f\"\u2705 Loaded successfully: {summary['n_samples']} samples, {summary['n_features']} features\\n\" + f\"   Defective: {summary['n_defective']} ({summary['defect_ratio']:.2%})\")\n        print(f\"   Clean: {summary['n_clean']} ({1-summary['defect_ratio']:.2%})\\n\" + f\"   Imbalance Ratio: {summary['imbalance_ratio']:.2f}:1\")\n        \n    except Exception as e:\n        print(f\"\u274c Failed to load {dataset_name}: {e}\")\n        import traceback\n        traceback.print_exc()\n\n# Create summary table\nsummary_df = pd.DataFrame(dataset_summaries)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"\ud83d\udccb DATASET SUMMARY TABLE\")\nprint(\"=\"*70)\nprint(summary_df.to_string(index=False))\n\n# Save summary\nsummary_path = os.path.join(CONFIG['output_dir'], f\"dataset_summary_{CONFIG['run_id']}.csv\")\nsummary_df.to_csv(summary_path, index=False)\nprint(f\"\\n\ud83d\udcbe Summary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Split + Scaling (Leakage Prevention)\n",
    "\n",
    "**KRITIK:** Scaler sadece train'e fit, val/test'e transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# LEAKAGE-FREE DATA PREPARATION\n# ============================================================================\n\ndef prepare_dataset_splits(df, dataset_name, config):\n    \"\"\"\n    Prepare train/val/test splits with NO LEAKAGE\n    \n    Pipeline:\n    1. Split into train/test (stratified)\n    2. Split train into train/val (stratified)\n    3. Fit scaler ONLY on train\n    4. Transform train/val/test with fitted scaler\n    5. Apply SMOTE ONLY on train\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        Raw dataset\n    dataset_name : str\n        Name of dataset\n    config : dict\n        Configuration dictionary\n        \n    Returns:\n    --------\n    dict\n        Dictionary with train/val/test splits and metadata\n    \"\"\"\n    print(f\"\\n{'='*70}\\n\" + f\"\ud83d\udd27 PREPARING {dataset_name} - LEAKAGE-FREE PIPELINE\")\n    print(f\"{'='*70}\")\n    \n    # Step 1: Extract features and labels\n    X = df.iloc[:, :-1].values.astype(np.float32)\n    y = df.iloc[:, -1].values\n    \n    # Encode labels\n    if y.dtype == object:\n        le = LabelEncoder()\n        y = le.fit_transform(y)\n    else:\n        y = y.astype(int)\n    \n    # Handle missing values\n    if np.any(np.isnan(X)):\n        print(\"\u26a0\ufe0f  Handling missing values with median imputation\")\n        col_median = np.nanmedian(X, axis=0)\n        inds = np.where(np.isnan(X))\n        X[inds] = np.take(col_median, inds[1])\n    \n    print(f\"\\n\ud83d\udcca Original Data:\\n\" + f\"   Total Samples: {len(y)}\")\n    print(f\"   Features: {X.shape[1]}\\n\" + f\"   Defective: {np.sum(y==1)} ({np.mean(y==1):.2%})\")\n    print(f\"   Clean: {np.sum(y==0)} ({np.mean(y==0):.2%})\")\n    \n    # Step 2: Train/Test Split (stratified)\n    print(f\"\\n\ud83d\udd00 Step 1: Train/Test Split (test_size={config['test_size']})\")\n    X_train_full, X_test, y_train_full, y_test = train_test_split(\n        X, y,\n        test_size=config['test_size'],\n        stratify=y,\n        random_state=config['seed']\n    )\n    \n    print(f\"   Train: {len(y_train_full)} samples (defect: {np.mean(y_train_full==1):.2%})\\n\" + f\"   Test:  {len(y_test)} samples (defect: {np.mean(y_test==1):.2%})\")\n    \n    # Step 3: Train/Val Split (from train, stratified)\n    print(f\"\\n\ud83d\udd00 Step 2: Train/Val Split (val_size={config['val_size']})\")\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_train_full, y_train_full,\n        test_size=config['val_size'],\n        stratify=y_train_full,\n        random_state=config['seed']\n    )\n    \n    print(f\"   Train: {len(y_train)} samples (defect: {np.mean(y_train==1):.2%})\\n\" + f\"   Val:   {len(y_val)} samples (defect: {np.mean(y_val==1):.2%})\")\n    print(f\"   Test:  {len(y_test)} samples (defect: {np.mean(y_test==1):.2%})\")\n    \n    # Step 4: Scaling (FIT ONLY ON TRAIN)\n    print(f\"\\n\ud83d\udccf Step 3: Feature Scaling (MinMaxScaler)\\n\" + f\"   \u26a0\ufe0f  CRITICAL: Scaler FIT only on train set\")\n    \n    scaler = MinMaxScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_val_scaled = scaler.transform(X_val)  # Only transform\n    X_test_scaled = scaler.transform(X_test)  # Only transform\n    \n    print(f\"   \u2705 Train: fitted and transformed\\n\" + f\"   \u2705 Val: transformed (no fit)\")\n    print(f\"   \u2705 Test: transformed (no fit)\")\n    \n    # Verify no leakage\n    print(f\"\\n\ud83d\udd0d Leakage Check:\\n\" + f\"   Train min/max: [{X_train_scaled.min():.4f}, {X_train_scaled.max():.4f}]\")\n    print(f\"   Val min/max:   [{X_val_scaled.min():.4f}, {X_val_scaled.max():.4f}]\\n\" + f\"   Test min/max:  [{X_test_scaled.min():.4f}, {X_test_scaled.max():.4f}]\")\n    \n    if X_val_scaled.min() < -0.01 or X_val_scaled.max() > 1.01:\n        print(f\"   \u26a0\ufe0f  Val data outside [0,1] range - expected behavior (no leakage)\")\n    if X_test_scaled.min() < -0.01 or X_test_scaled.max() > 1.01:\n        print(f\"   \u26a0\ufe0f  Test data outside [0,1] range - expected behavior (no leakage)\")\n    \n    return {\n        'dataset_name': dataset_name,\n        'X_train': X_train_scaled,\n        'X_val': X_val_scaled,\n        'X_test': X_test_scaled,\n        'y_train': y_train,\n        'y_val': y_val,\n        'y_test': y_test,\n        'scaler': scaler,\n        'n_features': X.shape[1]\n    }\n\n\n# ============================================================================\n# PREPARE ALL DATASETS\n# ============================================================================\n\nprepared_datasets = {}\n\nfor dataset_name in CONFIG['datasets']:\n    if dataset_name in raw_datasets:\n        prepared_datasets[dataset_name] = prepare_dataset_splits(\n            raw_datasets[dataset_name],\n            dataset_name,\n            CONFIG\n        )\n    else:\n        print(f\"\\n\u274c Skipping {dataset_name} - not loaded\\n\" + f\"\\n{'='*70}\")\nprint(f\"\u2705 ALL DATASETS PREPARED - NO LEAKAGE\")\nprint(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Imbalance Stratejisi - SMOTE\n",
    "\n",
    "SMOTE 0.7 (primary) ve alternatif oranlar (0.5, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# SMOTE STRATEGY - TRAIN ONLY\n# ============================================================================\n\ndef apply_smote(X_train, y_train, ratio, seed):\n    \"\"\"\n    Apply SMOTE to training data ONLY\n    \n    Parameters:\n    -----------\n    X_train : np.ndarray\n        Training features\n    y_train : np.ndarray\n        Training labels\n    ratio : float\n        SMOTE sampling ratio (minority will be ratio * majority)\n    seed : int\n        Random seed\n        \n    Returns:\n    --------\n    tuple\n        (X_train_smote, y_train_smote)\n    \"\"\"\n    print(f\"\\n\ud83d\udd04 Applying SMOTE (ratio={ratio})...\\n\" + f\"   Before SMOTE:\")\n    print(f\"     Total: {len(y_train)}\\n\" + f\"     Clean: {np.sum(y_train==0)} ({np.mean(y_train==0):.2%})\")\n    print(f\"     Defective: {np.sum(y_train==1)} ({np.mean(y_train==1):.2%})\")\n    \n    try:\n        smote = SMOTE(sampling_strategy=ratio, random_state=seed)\n        X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n        \n        print(f\"   After SMOTE:\\n\" + f\"     Total: {len(y_train_smote)} (+{len(y_train_smote)-len(y_train)})\")\n        print(f\"     Clean: {np.sum(y_train_smote==0)} ({np.mean(y_train_smote==0):.2%})\\n\" + f\"     Defective: {np.sum(y_train_smote==1)} ({np.mean(y_train_smote==1):.2%})\")\n        print(f\"   \u2705 SMOTE successful\")\n        \n        return X_train_smote, y_train_smote\n    \n    except Exception as e:\n        print(f\"   \u274c SMOTE failed: {e}\\n\" + f\"   \u26a0\ufe0f  Continuing without SMOTE\")\n        return X_train, y_train\n\n\n# ============================================================================\n# APPLY SMOTE TO ALL DATASETS\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"\ud83c\udfaf IMBALANCE HANDLING - SMOTE (Train Only)\")\nprint(\"=\"*70)\n\n# Apply primary SMOTE ratio (0.7)\nfor dataset_name, data in prepared_datasets.items():\n    print(f\"\\n\ud83d\udcca Dataset: {dataset_name}\")\n    \n    X_train_smote, y_train_smote = apply_smote(\n        data['X_train'],\n        data['y_train'],\n        CONFIG['smote_ratio'],\n        CONFIG['seed']\n    )\n    \n    # Store SMOTE-augmented training data\n    data['X_train_smote'] = X_train_smote\n    data['y_train_smote'] = y_train_smote\n    \n    # CRITICAL: Val and Test remain UNCHANGED\n    print(f\"\\n   \ud83d\udd12 Val and Test sets: UNCHANGED (no SMOTE)\\n\" + f\"      Val: {len(data['y_val'])} samples\")\n    print(f\"      Test: {len(data['y_test'])} samples\\n\" + f\"\\n{'='*70}\")\nprint(f\"\u2705 SMOTE APPLIED - VAL/TEST UNTOUCHED\")\nprint(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Metrics - Unified Function\n",
    "\n",
    "Tek fonksiyon, t\u00fcm metrikler (Recall, Precision, F1, F2, Accuracy, Balanced Accuracy, PR-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# UNIFIED METRICS CALCULATION\n# ============================================================================\n\ndef calculate_all_metrics(y_true, y_pred, y_pred_proba=None):\n    \"\"\"\n    Calculate all required metrics in one function\n    \n    Parameters:\n    -----------\n    y_true : array-like\n        True labels\n    y_pred : array-like\n        Predicted labels\n    y_pred_proba : array-like, optional\n        Predicted probabilities (for AUC metrics)\n        \n    Returns:\n    --------\n    dict\n        Dictionary with all metrics\n    \"\"\"\n    # Confusion matrix\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    \n    # Basic metrics\n    metrics = {\n        # Classification metrics\n        'recall': recall_score(y_true, y_pred, zero_division=0),\n        'precision': precision_score(y_true, y_pred, zero_division=0),\n        'f1': f1_score(y_true, y_pred, zero_division=0),\n        'f2': fbeta_score(y_true, y_pred, beta=2, zero_division=0),\n        'accuracy': accuracy_score(y_true, y_pred),\n        'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n        \n        # Confusion matrix components\n        'tp': int(tp),\n        'fp': int(fp),\n        'tn': int(tn),\n        'fn': int(fn)\n    }\n    \n    # Add AUC metrics if probabilities provided\n    if y_pred_proba is not None:\n        try:\n            metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba)\n        except:\n            metrics['roc_auc'] = 0.0\n        \n        try:\n            metrics['pr_auc'] = average_precision_score(y_true, y_pred_proba)\n        except:\n            metrics['pr_auc'] = 0.0\n    else:\n        metrics['roc_auc'] = 0.0\n        metrics['pr_auc'] = 0.0\n    \n    return metrics\n\n\ndef print_metrics(metrics, prefix=\"\"):\n    \"\"\"\n    Pretty print metrics\n    \n    Parameters:\n    -----------\n    metrics : dict\n        Metrics dictionary\n    prefix : str\n        Prefix for print statements\n    \"\"\"\n    print(f\"{prefix}\ud83d\udcca Performance Metrics:\\n\" + f\"{prefix}   Recall (Defective):    {metrics['recall']:.4f} \u2b50\")\n    print(f\"{prefix}   Precision (Defective): {metrics['precision']:.4f}\\n\" + f\"{prefix}   F1-Score:              {metrics['f1']:.4f}\")\n    print(f\"{prefix}   F2-Score:              {metrics['f2']:.4f} \ud83c\udfaf\\n\" + f\"{prefix}   Accuracy:              {metrics['accuracy']:.4f}\")\n    print(f\"{prefix}   Balanced Accuracy:     {metrics['balanced_accuracy']:.4f}\\n\" + f\"{prefix}   PR-AUC:                {metrics['pr_auc']:.4f}\")\n    print(f\"{prefix}   ROC-AUC:               {metrics['roc_auc']:.4f}\\n\" + f\"{prefix}\\n   Confusion Matrix:\")\n    print(f\"{prefix}      TP: {metrics['tp']:4d}  FP: {metrics['fp']:4d}\\n\" + f\"{prefix}      FN: {metrics['fn']:4d}  TN: {metrics['tn']:4d}\")\n\n\n# Test metrics function\nprint(\"\\n\" + \"=\"*70)\nprint(\"\ud83e\uddea METRICS FUNCTION TEST\")\nprint(\"=\"*70)\n\n# Create dummy data for testing\ny_true_test = np.array([0, 1, 1, 0, 1, 0, 1, 1, 0, 1])\ny_pred_test = np.array([0, 1, 1, 0, 0, 0, 1, 1, 1, 1])\ny_proba_test = np.array([0.2, 0.8, 0.9, 0.1, 0.4, 0.3, 0.7, 0.85, 0.6, 0.75])\n\ntest_metrics = calculate_all_metrics(y_true_test, y_pred_test, y_proba_test)\nprint_metrics(test_metrics)\n\nprint(\"\\n\u2705 Metrics function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Threshold Tuning\n",
    "\n",
    "Val seti \u00fczerinde F2 maksimizasyonu i\u00e7in threshold optimizasyonu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# THRESHOLD OPTIMIZATION (F2-FOCUSED)\n# ============================================================================\n\ndef find_optimal_threshold(y_true, y_pred_proba, metric='f2', \n                          threshold_range=(0.05, 0.95), step=0.05,\n                          accuracy_floor=0.5, verbose=True):\n    \"\"\"\n    Find optimal classification threshold by maximizing target metric\n    with accuracy guardrail\n    \n    Parameters:\n    -----------\n    y_true : array-like\n        True labels\n    y_pred_proba : array-like\n        Predicted probabilities\n    metric : str\n        Metric to optimize ('f1', 'f2', 'recall')\n    threshold_range : tuple\n        (min_threshold, max_threshold)\n    step : float\n        Step size for threshold search\n    accuracy_floor : float\n        Minimum acceptable accuracy\n    verbose : bool\n        Print search results\n        \n    Returns:\n    --------\n    dict\n        {'threshold': best_threshold, 'metrics': best_metrics, 'curve': threshold_curve}\n    \"\"\"\n    thresholds = np.arange(threshold_range[0], threshold_range[1] + step, step)\n    \n    results = []\n    best_score = -1\n    best_threshold = 0.5\n    best_metrics = None\n    \n    if verbose:\n        print(f\"\\n\ud83d\udd0d Threshold Optimization (Target: {metric.upper()})\\n\" + f\"   Range: [{threshold_range[0]}, {threshold_range[1]}]\")\n        print(f\"   Step: {step}\\n\" + f\"   Accuracy Floor: {accuracy_floor:.2f}\\n\")\n    \n    for thresh in thresholds:\n        y_pred = (y_pred_proba >= thresh).astype(int)\n        metrics = calculate_all_metrics(y_true, y_pred, y_pred_proba)\n        \n        # Get target metric score\n        if metric.lower() == 'f2':\n            score = metrics['f2']\n        elif metric.lower() == 'f1':\n            score = metrics['f1']\n        elif metric.lower() == 'recall':\n            score = metrics['recall']\n        else:\n            score = metrics['f2']  # default\n        \n        # Apply accuracy guardrail\n        if metrics['accuracy'] < accuracy_floor:\n            score = 0  # Penalize thresholds with too low accuracy\n        \n        results.append({\n            'threshold': thresh,\n            'score': score,\n            **metrics\n        })\n        \n        # Track best\n        if score > best_score:\n            best_score = score\n            best_threshold = thresh\n            best_metrics = metrics.copy()\n    \n    results_df = pd.DataFrame(results)\n    \n    if verbose:\n        print(f\"\\n\u2705 Optimal Threshold: {best_threshold:.2f}\\n\" + f\"   {metric.upper()}: {best_score:.4f}\")\n        print(f\"   Accuracy: {best_metrics['accuracy']:.4f}\\n\" + f\"   Recall: {best_metrics['recall']:.4f}\")\n        print(f\"   Precision: {best_metrics['precision']:.4f}\")\n        \n        # Show top 5 thresholds\n        print(f\"\\n   Top 5 Thresholds:\")\n        top5 = results_df.nlargest(5, 'score')[['threshold', 'score', 'recall', 'precision', 'f2', 'accuracy']]\n        for idx, row in top5.iterrows():\n            print(f\"      {row['threshold']:.2f} \u2192 {metric.upper()}={row['score']:.4f} \"\n                  f\"(Rec={row['recall']:.3f}, Prec={row['precision']:.3f}, Acc={row['accuracy']:.3f})\")\n    \n    return {\n        'threshold': best_threshold,\n        'metrics': best_metrics,\n        'curve': results_df\n    }\n\n\n# Test threshold function\nprint(\"\\n\" + \"=\"*70)\nprint(\"\ud83e\uddea THRESHOLD TUNING TEST\")\nprint(\"=\"*70)\n\n# Use dummy data\ny_true_dummy = np.random.binomial(1, 0.3, 100)\ny_proba_dummy = np.random.beta(2, 5, 100)\n\nthreshold_result = find_optimal_threshold(\n    y_true_dummy, \n    y_proba_dummy,\n    metric='f2',\n    threshold_range=(0.1, 0.9),\n    step=0.1,\n    accuracy_floor=0.4,\n    verbose=True\n)\n\nprint(\"\\n\u2705 Threshold tuning function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n## 8. Baseline Model - Random Forest\n\nClass-weighted Random Forest as baseline"
  },
  {
   "cell_type": "markdown",
   "source": "---\n## \ud83d\udccb NOTEBOOK \u0130\u00c7ER\u0130K \u00d6ZET\u0130\n\nBu notebook a\u015fa\u011f\u0131daki b\u00f6l\u00fcmleri i\u00e7erir:\n\n### \u2705 Tamamlanan B\u00f6l\u00fcmler:\n\n1. **Config & Seed Management** - \u0130zlenebilirlik i\u00e7in run tracking\n2. **Dataset Exploration** - JM1 & KC1 \u00f6zet istatistikler\n3. **Leakage-Free Split** - Train/Val/Test ayr\u0131m\u0131 (scaler leakage yok)\n4. **SMOTE Strategy** - 0.7 ratio (sadece train'e uygulan\u0131r)\n5. **Unified Metrics** - Recall, Precision, F1, F2, Accuracy, Balanced Acc, PR-AUC\n6. **Threshold Tuning** - Val seti \u00fczerinde F2 maksimizasyonu\n7. **Baseline RF** - Class-weighted Random Forest\n8. **KAN Implementation** - Spline-based activation functions\n9. **Loss Functions** - Weighted BCE & Focal Loss\n10. **GWO Optimizer** - Multi-metric fitness (0.5\\*F1 + 0.3\\*Recall + 0.2\\*Acc)\n11. **Feature Attention** - \u00d6zg\u00fcn katk\u0131: Sample-specific feature weighting\n12. **Export Functions** - CSV, JSON, XLSX format\u0131nda sonu\u00e7 export\n\n### \ud83c\udfaf Ana Hedefler:\n\n- \u2705 **F2 ve Recall maksimizasyonu** (defective=1 i\u00e7in)\n- \u2705 **Accuracy korunmas\u0131** (GWO accuracy floor ile)\n- \u2705 **Leakage prevention** (test seti tamamen izole)\n- \u2705 **Colab CPU optimize** (GPU gereksiz)\n\n### \ud83d\ude80 \u00d6zg\u00fcn Katk\u0131:\n\n**Feature-Level Attention Mechanism**\n- Lightweight attention layer (transformer de\u011fil)\n- Her sample i\u00e7in feature'lara dinamik a\u011f\u0131rl\u0131k\n- KAN'a entegre edilmi\u015f\n- Interpretable (attention weights incelenebilir)\n\n### \ud83d\udcca Beklenen \u00c7\u0131kt\u0131lar:\n\n1. `dataset_summary_<run_id>.csv` - Dataset istatistikleri\n2. `config_<run_id>.json` - Experiment config\n3. `final_results_<run_id>.csv/json/xlsx` - T\u00fcm model sonu\u00e7lar\u0131\n4. `comparison_plot_<run_id>.png` - Kar\u015f\u0131la\u015ft\u0131rma grafi\u011fi\n\n### \ud83d\udd2c Experimental Protocol:\n\n**Metrikler (\u00f6ncelik s\u0131ras\u0131na g\u00f6re):**\n1. F2-Score (defect detection i\u00e7in en \u00f6nemli)\n2. Recall (safety-critical)\n3. Precision (false positive kontrol\u00fc)\n4. Accuracy (genel performans)\n5. Balanced Accuracy (imbalanced data i\u00e7in)\n6. PR-AUC (overall ranking)\n\n**Threshold Tuning:**\n- Val seti \u00fczerinde 0.05-0.95 aras\u0131 grid search\n- F2 maksimizasyonu hedefi\n- Accuracy floor guardrail (0.5)\n\n**GWO Fitness:**\n```\nfitness = 0.5 * F1 + 0.3 * Recall + 0.2 * Accuracy\nif accuracy < 0.5:\n    fitness *= 0.1  # Heavy penalty\n```\n\n---\n\n**\ud83d\udc68\u200d\ud83d\udcbb Haz\u0131rlayan:** Claude (Anthropic)  \n**\ud83d\udcc5 Tarih:** 2026-01-05  \n**\ud83c\udfaf Proje:** NASA Defect Prediction with GWO-KAN  \n**\ud83d\udcda Datasets:** JM1, KC1  \n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 13. EXECUTION TEMPLATE\n\nA\u015fa\u011f\u0131daki h\u00fccreleri s\u0131rayla \u00e7al\u0131\u015ft\u0131rarak t\u00fcm pipeline'\u0131 \u00e7al\u0131\u015ft\u0131rabilirsiniz:\n\n1. **Baseline RF**: Her dataset i\u00e7in Random Forest baseline\n2. **KAN Base**: Temel KAN modeli (Focal Loss ile)\n3. **KAN + GWO**: GWO optimized hyperparameters\n4. **KAN + Attention**: Feature-level attention ile KAN\n5. **Final GWO + Attention**: GWO optimized + Attention (En iyi model)\n6. **Loss Ablation**: WeightedBCE vs Focal kar\u015f\u0131la\u015ft\u0131rmas\u0131\n7. **Compile & Export**: T\u00fcm sonu\u00e7lar\u0131 derle ve export et\n\n### \u00d6rnek \u00c7al\u0131\u015ft\u0131rma:\n\n```python\n# Example: Train baseline for one dataset\ndataset_name = 'JM1'\ndata = prepared_datasets[dataset_name]\n\n# Train RF baseline\nrf = train_baseline_rf(\n    data['X_train_smote'],\n    data['y_train_smote'],\n    data['X_val'],\n    data['y_val'],\n    CONFIG['seed']\n)\n\n# Evaluate with threshold tuning\nresults = evaluate_baseline_with_threshold(\n    rf,\n    data['X_val'],\n    data['y_val'],\n    data['X_test'],\n    data['y_test'],\n    CONFIG\n)\n\nprint(f\\\"Baseline F2: {results['test_metrics']['f2']:.4f}\\\")\n```\n\n**NOT:** Notebook'u Colab'da \u00e7al\u0131\u015ft\u0131r\u0131rken Google Drive'\u0131 mount etmeyi unutmay\u0131n:\n\n```python\nfrom google.colab import drive\ndrive.mount('/content/drive')\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n# FINAL RESULTS COMPILATION AND EXPORT\n# ============================================================================\n\ndef compile_final_results(all_results, config):\n    \"\"\"\n    Compile all experimental results into a summary table\n    \n    Parameters:\n    -----------\n    all_results : dict\n        Dictionary with all experimental results\n    config : dict\n        Experiment configuration\n        \n    Returns:\n    --------\n    pd.DataFrame\n        Final summary table\n    \"\"\"\n    final_rows = []\n    \n    for dataset_name, dataset_results in all_results.items():\n        # Get dataset info\n        dataset_info = dataset_results.get('dataset_info', {})\n        \n        # Compile each model's results\n        for model_name, model_results in dataset_results.items():\n            if model_name == 'dataset_info':\n                continue\n            \n            if 'test_metrics' not in model_results:\n                continue\n            \n            metrics = model_results['test_metrics']\n            \n            row = {\n                'dataset': dataset_name,\n                'model': model_name,\n                'defect_ratio': dataset_info.get('defect_ratio', 0),\n                'n_samples': dataset_info.get('n_samples', 0),\n                'n_features': dataset_info.get('n_features', 0),\n                'recall': metrics['recall'],\n                'precision': metrics['precision'],\n                'f1': metrics['f1'],\n                'f2': metrics['f2'],\n                'accuracy': metrics['accuracy'],\n                'balanced_acc': metrics['balanced_accuracy'],\n                'pr_auc': metrics['pr_auc'],\n                'threshold': model_results.get('optimal_threshold', 0.5)\n            }\n            \n            final_rows.append(row)\n    \n    df = pd.DataFrame(final_rows)\n    \n    return df\n\n\ndef export_results(results_df, config):\n    \"\"\"\n    Export results to multiple formats\n    \n    Parameters:\n    -----------\n    results_df : pd.DataFrame\n        Results dataframe\n    config : dict\n        Configuration\n    \"\"\"\n    base_path = os.path.join(config['output_dir'], f\"final_results_{config['run_id']}\")\n    \n    # CSV\n    if 'csv' in config['export_format']:\n        csv_path = f\"{base_path}.csv\"\n        results_df.to_csv(csv_path, index=False)\n        print(f\"\u2705 CSV saved: {csv_path}\")\n    \n    # JSON\n    if 'json' in config['export_format']:\n        json_path = f\"{base_path}.json\"\n        results_df.to_json(json_path, orient='records', indent=2)\n        print(f\"\u2705 JSON saved: {json_path}\")\n    \n    # Excel\n    if 'xlsx' in config['export_format']:\n        xlsx_path = f\"{base_path}.xlsx\"\n        try:\n            results_df.to_excel(xlsx_path, index=False, sheet_name='Results')\n            print(f\"\u2705 Excel saved: {xlsx_path}\")\n        except Exception as e:\n            print(f\"\u26a0\ufe0f  Excel export failed: {e}\")\n\n\ndef print_final_summary(results_df):\n    \"\"\"\n    Print formatted final summary\n    \n    Parameters:\n    -----------\n    results_df : pd.DataFrame\n        Results dataframe\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\" FINAL RESULTS SUMMARY \".center(80, \"=\"))\n    print(\"=\"*80)\n    \n    # Per-dataset results\n    for dataset in results_df['dataset'].unique():\n        dataset_df = results_df[results_df['dataset'] == dataset]\n        \n        print(f\"\\n\ud83d\udcca Dataset: {dataset}\")\n        print(\"-\" * 80)\n        \n        for _, row in dataset_df.iterrows():\n            print(f\"\\n   {row['model']}:\")\n            print(f\"      Recall:     {row['recall']:.4f} \u2b50\")\n            print(f\"      Precision:  {row['precision']:.4f}\")\n            print(f\"      F1:         {row['f1']:.4f}\")\n            print(f\"      F2:         {row['f2']:.4f} \ud83c\udfaf\")\n            print(f\"      Accuracy:   {row['accuracy']:.4f}\")\n            print(f\"      PR-AUC:     {row['pr_auc']:.4f}\")\n            print(f\"      Threshold:  {row['threshold']:.3f}\")\n    \n    # Average across datasets\n    print(\"\\n\" + \"=\"*80)\n    print(\" AVERAGE ACROSS DATASETS \".center(80, \"=\"))\n    print(\"=\"*80)\n    \n    for model in results_df['model'].unique():\n        model_df = results_df[results_df['model'] == model]\n        \n        print(f\"\\n   {model}:\")\n        print(f\"      Recall:     {model_df['recall'].mean():.4f} \u2b50\")\n        print(f\"      Precision:  {model_df['precision'].mean():.4f}\")\n        print(f\"      F1:         {model_df['f1'].mean():.4f}\")\n        print(f\"      F2:         {model_df['f2'].mean():.4f} \ud83c\udfaf\")\n        print(f\"      Accuracy:   {model_df['accuracy'].mean():.4f}\")\n        print(f\"      PR-AUC:     {model_df['pr_auc'].mean():.4f}\")\n    \n    print(\"\\n\" + \"=\"*80)\n\n\ndef create_comparison_plot(results_df, config):\n    \"\"\"\n    Create comparison visualization\n    \n    Parameters:\n    -----------\n    results_df : pd.DataFrame\n        Results dataframe\n    config : dict\n        Configuration\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    \n    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n    fig.suptitle('Model Performance Comparison - JM1 & KC1', \n                 fontsize=16, fontweight='bold')\n    \n    metrics = ['recall', 'precision', 'f1', 'f2', 'accuracy', 'pr_auc']\n    metric_names = ['Recall', 'Precision', 'F1-Score', 'F2-Score', 'Accuracy', 'PR-AUC']\n    \n    for idx, (metric, name) in enumerate(zip(metrics, metric_names)):\n        ax = axes[idx // 3, idx % 3]\n        \n        # Group by dataset and model\n        pivot = results_df.pivot(index='model', columns='dataset', values=metric)\n        \n        pivot.plot(kind='bar', ax=ax, width=0.8)\n        ax.set_title(name, fontsize=12, fontweight='bold')\n        ax.set_xlabel('')\n        ax.set_ylabel(name)\n        ax.set_ylim(0, 1.0)\n        ax.legend(title='Dataset', loc='lower right')\n        ax.grid(axis='y', alpha=0.3)\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n    \n    plt.tight_layout()\n    \n    # Save plot\n    plot_path = os.path.join(config['output_dir'], \n                            f\"comparison_plot_{config['run_id']}.png\")\n    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n    print(f\"\\n\u2705 Comparison plot saved: {plot_path}\")\n    \n    plt.show()\n\n\nprint(\"\\n\u2705 Export functions ready!\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 12. Final Tables & Export\n\nT\u00fcm sonu\u00e7lar\u0131 toplama ve raporlama",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n# FEATURE-LEVEL ATTENTION MECHANISM (\u00d6ZG\u00dcN KATKI)\n# ============================================================================\n\nclass FeatureAttention(nn.Module):\n    \"\"\"\n    Lightweight Feature-Level Attention\n    \n    Learns to weight input features based on their importance\n    for each sample (sample-specific attention).\n    \n    Parameters:\n    -----------\n    input_dim : int\n        Number of input features\n    attention_dim : int\n        Attention hidden dimension\n    dropout : float\n        Dropout rate\n    \"\"\"\n    \n    def __init__(self, input_dim, attention_dim=16, dropout=0.2):\n        super(FeatureAttention, self).__init__()\n        \n        self.input_dim = input_dim\n        self.attention_dim = attention_dim\n        \n        # Attention network (small MLP)\n        self.attention_fc1 = nn.Linear(input_dim, attention_dim)\n        self.attention_fc2 = nn.Linear(attention_dim, input_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        \"\"\"\n        Forward pass - compute attention weights and apply them\n        \n        Parameters:\n        -----------\n        x : torch.Tensor\n            Input features (batch_size, input_dim)\n            \n        Returns:\n        --------\n        tuple\n            (attended_features, attention_weights)\n        \"\"\"\n        # Compute attention scores\n        attention = self.attention_fc1(x)\n        attention = F.relu(attention)\n        attention = self.dropout(attention)\n        attention = self.attention_fc2(attention)\n        attention = torch.sigmoid(attention)  # [0, 1] weights\n        \n        # Apply attention (element-wise multiplication)\n        attended = x * attention\n        \n        return attended, attention\n\n\nclass KAN_WithAttention(nn.Module):\n    \"\"\"\n    KAN with Feature-Level Attention\n    \n    Architecture:\n    Input -> Feature Attention -> KAN Layers -> Output\n    \"\"\"\n    \n    def __init__(self, input_dim, hidden_dim=64, grid_size=5,\n                 spline_order=3, dropout=0.3, attention_dim=16):\n        super(KAN_WithAttention, self).__init__()\n        \n        self.input_dim = input_dim\n        \n        # Feature Attention (NEW!)\n        self.feature_attention = FeatureAttention(\n            input_dim,\n            attention_dim=attention_dim,\n            dropout=dropout\n        )\n        \n        # KAN layers (same as before)\n        self.kan1 = KANLinear(input_dim, hidden_dim, grid_size, spline_order)\n        self.kan2 = KANLinear(hidden_dim, hidden_dim // 2, grid_size, spline_order)\n        \n        # Output\n        self.output = nn.Linear(hidden_dim // 2, 1)\n        \n        # Normalization\n        self.bn1 = nn.BatchNorm1d(hidden_dim)\n        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n        \n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        \"\"\"Forward pass with attention\"\"\"\n        # Apply feature attention\n        x_attended, attention_weights = self.feature_attention(x)\n        \n        # KAN layers (on attended features)\n        x = self.kan1(x_attended)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        \n        x = self.kan2(x)\n        x = self.bn2(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        \n        # Output\n        x = self.output(x)\n        x = torch.sigmoid(x)\n        \n        return x\n    \n    def get_attention_weights(self, x):\n        \"\"\"Extract attention weights for analysis\"\"\"\n        self.eval()\n        with torch.no_grad():\n            if not isinstance(x, torch.Tensor):\n                x = torch.FloatTensor(x).to(next(self.parameters()).device)\n            _, attention_weights = self.feature_attention(x)\n        return attention_weights.cpu().numpy()\n\n\nprint(\"\\n\u2705 Feature-Level Attention implemented!\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 11. Feature-Level Attention (\u00d6ZG\u00dcN KATKI)\n\nHafif feature attention mechanism - Her \u00f6rnek i\u00e7in feature'lara a\u011f\u0131rl\u0131k verir",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n# GREY WOLF OPTIMIZER - CPU OPTIMIZED\n# ============================================================================\n\nclass GreyWolfOptimizer:\n    \"\"\"\n    GWO for hyperparameter optimization with multi-metric fitness\n    \"\"\"\n    \n    def __init__(self, n_wolves, n_iterations, bounds, fitness_func):\n        self.n_wolves = n_wolves\n        self.n_iterations = n_iterations\n        self.bounds = np.array(bounds)\n        self.fitness_func = fitness_func\n        self.dim = len(bounds)\n        \n        # Initialize positions\n        self.positions = np.random.uniform(\n            self.bounds[:, 0],\n            self.bounds[:, 1],\n            size=(n_wolves, self.dim)\n        )\n        \n        # Alpha, Beta, Delta\n        self.alpha_pos = np.zeros(self.dim)\n        self.alpha_score = float('-inf')\n        self.beta_pos = np.zeros(self.dim)\n        self.beta_score = float('-inf')\n        self.delta_pos = np.zeros(self.dim)\n        self.delta_score = float('-inf')\n        \n        self.convergence_curve = []\n        \n    def optimize(self, verbose=True):\n        \"\"\"Run GWO optimization\"\"\"\n        \n        for iteration in range(self.n_iterations):\n            # Evaluate all wolves\n            for i in range(self.n_wolves):\n                fitness = self.fitness_func(self.positions[i])\n                \n                # Update hierarchy\n                if fitness > self.alpha_score:\n                    self.delta_score = self.beta_score\n                    self.delta_pos = self.beta_pos.copy()\n                    self.beta_score = self.alpha_score\n                    self.beta_pos = self.alpha_pos.copy()\n                    self.alpha_score = fitness\n                    self.alpha_pos = self.positions[i].copy()\n                elif fitness > self.beta_score:\n                    self.delta_score = self.beta_score\n                    self.delta_pos = self.beta_pos.copy()\n                    self.beta_score = fitness\n                    self.beta_pos = self.positions[i].copy()\n                elif fitness > self.delta_score:\n                    self.delta_score = fitness\n                    self.delta_pos = self.positions[i].copy()\n            \n            # Update parameter a\n            a = 2 - iteration * (2.0 / self.n_iterations)\n            \n            # Update positions\n            for i in range(self.n_wolves):\n                for j in range(self.dim):\n                    r1, r2 = np.random.random(2)\n                    A1 = 2 * a * r1 - a\n                    C1 = 2 * r2\n                    D_alpha = abs(C1 * self.alpha_pos[j] - self.positions[i, j])\n                    X1 = self.alpha_pos[j] - A1 * D_alpha\n                    \n                    r1, r2 = np.random.random(2)\n                    A2 = 2 * a * r1 - a\n                    C2 = 2 * r2\n                    D_beta = abs(C2 * self.beta_pos[j] - self.positions[i, j])\n                    X2 = self.beta_pos[j] - A2 * D_beta\n                    \n                    r1, r2 = np.random.random(2)\n                    A3 = 2 * a * r1 - a\n                    C3 = 2 * r2\n                    D_delta = abs(C3 * self.delta_pos[j] - self.positions[i, j])\n                    X3 = self.delta_pos[j] - A3 * D_delta\n                    \n                    self.positions[i, j] = (X1 + X2 + X3) / 3.0\n                    self.positions[i, j] = np.clip(\n                        self.positions[i, j],\n                        self.bounds[j, 0],\n                        self.bounds[j, 1]\n                    )\n            \n            self.convergence_curve.append(self.alpha_score)\n            \n            if verbose and (iteration + 1) % 5 == 0:\n                print(f\"   Iter {iteration+1:2d}/{self.n_iterations} | Best Fitness: {self.alpha_score:.4f}\")\n        \n        return self.alpha_pos, self.alpha_score, self.convergence_curve\n\n\ndef gwo_kan_fitness_function(params, X_train, y_train, X_val, y_val, \n                             input_dim, config):\n    \"\"\"\n    GWO fitness function with multi-metric optimization\n    \n    Fitness = 0.5*F1 + 0.3*Recall + 0.2*Accuracy\n    With accuracy floor guardrail\n    \"\"\"\n    # Parse params\n    grid_size = int(params[0])\n    spline_order = int(params[1])\n    hidden_dim = int(params[2])\n    learning_rate = params[3]\n    \n    try:\n        # Create model\n        model = KAN(\n            input_dim=input_dim,\n            hidden_dim=hidden_dim,\n            grid_size=grid_size,\n            spline_order=spline_order,\n            dropout=0.3\n        )\n        \n        # Train (reduced epochs for speed)\n        model, _ = train_kan_model(\n            model, X_train, y_train, X_val, y_val,\n            learning_rate=learning_rate,\n            epochs=30,  # Reduced for GWO speed\n            batch_size=config['kan_batch_size'],\n            loss_type='Focal',\n            patience=10,\n            verbose=False\n        )\n        \n        # Find optimal threshold\n        model.eval()\n        with torch.no_grad():\n            X_val_t = torch.FloatTensor(X_val).to(device)\n            y_val_proba = model(X_val_t).cpu().numpy().flatten()\n        \n        threshold_result = find_optimal_threshold(\n            y_val, y_val_proba,\n            metric='f2',\n            threshold_range=config['threshold_range'],\n            step=config['threshold_step'],\n            accuracy_floor=config['gwo_accuracy_floor'],\n            verbose=False\n        )\n        \n        metrics = threshold_result['metrics']\n        \n        # Calculate fitness\n        fitness = (\n            config['gwo_fitness_weights']['f1'] * metrics['f1'] +\n            config['gwo_fitness_weights']['recall'] * metrics['recall'] +\n            config['gwo_fitness_weights']['accuracy'] * metrics['accuracy']\n        )\n        \n        # Apply accuracy floor penalty\n        if metrics['accuracy'] < config['gwo_accuracy_floor']:\n            fitness *= 0.1  # Heavy penalty\n        \n        return fitness\n    \n    except Exception as e:\n        print(f\"   \u26a0\ufe0f  Fitness eval failed: {e}\")\n        return 0.0\n\n\nprint(\"\\n\u2705 GWO implementation ready!\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 10. GWO (Grey Wolf Optimizer) - Multi-Metric Fitness\n\nOptimizing KAN hyperparameters with balanced fitness: 0.5*F1 + 0.3*Recall + 0.2*Accuracy",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n# KAN TRAINING FUNCTION\n# ============================================================================\n\ndef train_kan_model(model, X_train, y_train, X_val, y_val,\n                   learning_rate=0.01, epochs=100, batch_size=32,\n                   loss_type='Focal', patience=15, verbose=True):\n    \"\"\"\n    Train KAN model with early stopping based on F2\n    \n    Parameters:\n    -----------\n    model : KAN\n        KAN model instance\n    X_train, y_train : np.ndarray\n        Training data\n    X_val, y_val : np.ndarray\n        Validation data\n    learning_rate : float\n        Learning rate\n    epochs : int\n        Maximum epochs\n    batch_size : int\n        Batch size\n    loss_type : str\n        'WeightedBCE' or 'Focal'\n    patience : int\n        Early stopping patience\n    verbose : bool\n        Print training progress\n        \n    Returns:\n    --------\n    tuple\n        (trained_model, train_history)\n    \"\"\"\n    model = model.to(device)\n    \n    # Convert to tensors\n    X_train_t = torch.FloatTensor(X_train).to(device)\n    y_train_t = torch.FloatTensor(y_train).unsqueeze(1).to(device)\n    X_val_t = torch.FloatTensor(X_val).to(device)\n    y_val_t = torch.FloatTensor(y_val).unsqueeze(1).to(device)\n    \n    # DataLoader\n    train_dataset = TensorDataset(X_train_t, y_train_t)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    \n    # Loss function\n    criterion = get_loss_function(loss_type, y_train)\n    criterion = criterion.to(device)\n    \n    # Optimizer\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    \n    # Training history\n    history = {'train_loss': [], 'val_f2': [], 'val_loss': []}\n    \n    # Early stopping\n    best_f2 = 0\n    patience_counter = 0\n    best_model_state = None\n    \n    if verbose:\n        print(f\"\\n\ud83d\ude80 Training KAN Model (loss={loss_type})...\")\n        print(f\"   Device: {device}\")\n        print(f\"   Epochs: {epochs}, Batch Size: {batch_size}, LR: {learning_rate}\")\n        print(f\"   Early Stopping: patience={patience}, metric=F2\\n\")\n    \n    for epoch in range(epochs):\n        # Training\n        model.train()\n        epoch_loss = 0\n        \n        for batch_X, batch_y in train_loader:\n            optimizer.zero_grad()\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        \n        epoch_loss /= len(train_loader)\n        \n        # Validation\n        model.eval()\n        with torch.no_grad():\n            val_outputs = model(X_val_t)\n            val_loss = criterion(val_outputs, y_val_t).item()\n            \n            val_pred_proba = val_outputs.cpu().numpy().flatten()\n            val_pred = (val_pred_proba >= 0.5).astype(int)\n            val_f2 = fbeta_score(y_val, val_pred, beta=2, zero_division=0)\n        \n        # Record history\n        history['train_loss'].append(epoch_loss)\n        history['val_loss'].append(val_loss)\n        history['val_f2'].append(val_f2)\n        \n        # Early stopping check\n        if val_f2 > best_f2:\n            best_f2 = val_f2\n            patience_counter = 0\n            best_model_state = model.state_dict().copy()\n        else:\n            patience_counter += 1\n        \n        # Print progress\n        if verbose and (epoch + 1) % 10 == 0:\n            print(f\"   Epoch {epoch+1:3d}/{epochs} | Train Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | Val F2: {val_f2:.4f} | Best F2: {best_f2:.4f}\")\n        \n        # Early stopping\n        if patience_counter >= patience:\n            if verbose:\n                print(f\"\\n   \u23f9\ufe0f  Early stopping at epoch {epoch+1}\")\n            break\n    \n    # Restore best model\n    if best_model_state is not None:\n        model.load_state_dict(best_model_state)\n    \n    if verbose:\n        print(f\"\\n\u2705 Training complete! Best Val F2: {best_f2:.4f}\")\n    \n    return model, history\n\n\nprint(\"\\n\u2705 KAN training function ready!\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n# LOSS FUNCTIONS\n# ============================================================================\n\nclass FocalLoss(nn.Module):\n    \"\"\"\n    Focal Loss for imbalanced classification\n    \n    Parameters:\n    -----------\n    alpha : float\n        Weight for positive class\n    gamma : float\n        Focusing parameter (higher = more focus on hard examples)\n    \"\"\"\n    \n    def __init__(self, alpha=0.25, gamma=2.0):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        \n    def forward(self, inputs, targets):\n        \"\"\"Compute focal loss\"\"\"\n        bce_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-bce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n        return focal_loss.mean()\n\n\nclass WeightedBCELoss(nn.Module):\n    \"\"\"\n    Weighted Binary Cross Entropy\n    \n    Parameters:\n    -----------\n    pos_weight : float\n        Weight for positive class\n    \"\"\"\n    \n    def __init__(self, pos_weight=1.0):\n        super(WeightedBCELoss, self).__init__()\n        self.pos_weight = pos_weight\n        \n    def forward(self, inputs, targets):\n        \"\"\"Compute weighted BCE\"\"\"\n        # Manual weighted BCE\n        loss = - (self.pos_weight * targets * torch.log(inputs + 1e-7) + \n                 (1 - targets) * torch.log(1 - inputs + 1e-7))\n        return loss.mean()\n\n\ndef get_loss_function(loss_type, y_train=None):\n    \"\"\"\n    Get loss function by name\n    \n    Parameters:\n    -----------\n    loss_type : str\n        'WeightedBCE' or 'Focal'\n    y_train : array-like, optional\n        Training labels (for calculating class weights)\n        \n    Returns:\n    --------\n    nn.Module\n        Loss function\n    \"\"\"\n    if loss_type == 'Focal':\n        return FocalLoss(alpha=CONFIG['focal_alpha'], gamma=CONFIG['focal_gamma'])\n    \n    elif loss_type == 'WeightedBCE':\n        if y_train is not None:\n            # Calculate pos_weight from class distribution\n            n_pos = np.sum(y_train == 1)\n            n_neg = np.sum(y_train == 0)\n            pos_weight = n_neg / n_pos if n_pos > 0 else 1.0\n        else:\n            pos_weight = 1.0\n        \n        return WeightedBCELoss(pos_weight=pos_weight)\n    \n    else:\n        return nn.BCELoss()\n\n\nprint(\"\\n\u2705 Loss functions implemented!\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n# KAN (KOLMOGOROV-ARNOLD NETWORK) IMPLEMENTATION\n# ============================================================================\n\nclass KANLinear(nn.Module):\n    \"\"\"\n    KAN Linear Layer with learnable spline functions\n    \n    Parameters:\n    -----------\n    in_features : int\n        Input dimension\n    out_features : int\n        Output dimension\n    grid_size : int\n        Number of grid points for spline\n    spline_order : int\n        Order of B-spline\n    \"\"\"\n    \n    def __init__(self, in_features, out_features, grid_size=5, spline_order=3):\n        super(KANLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.grid_size = grid_size\n        self.spline_order = spline_order\n        \n        # Learnable grid points\n        self.grid = nn.Parameter(\n            torch.linspace(-1, 1, grid_size).unsqueeze(0).unsqueeze(0).repeat(\n                out_features, in_features, 1\n            )\n        )\n        \n        # Learnable spline coefficients\n        self.coef = nn.Parameter(\n            torch.randn(out_features, in_features, grid_size + spline_order) * 0.1\n        )\n        \n        # Base linear transformation (residual)\n        self.base_weight = nn.Parameter(\n            torch.randn(out_features, in_features) * 0.1\n        )\n        \n    def b_splines(self, x):\n        \"\"\"Compute B-spline basis functions\"\"\"\n        batch_size = x.shape[0]\n        x = x.unsqueeze(1).unsqueeze(-1)\n        grid = self.grid.unsqueeze(0)\n        \n        distances = torch.abs(x - grid)\n        \n        basis = torch.zeros(\n            batch_size, self.out_features, self.in_features,\n            self.grid_size + self.spline_order,\n            device=x.device\n        )\n        \n        # RBF-like basis functions\n        for i in range(self.grid_size):\n            basis[:, :, :, i] = torch.exp(-distances[:, :, :, i] ** 2 / 0.5)\n        \n        # Polynomial terms\n        for i in range(self.spline_order):\n            basis[:, :, :, self.grid_size + i] = x.squeeze(-1) ** (i + 1)\n        \n        return basis\n    \n    def forward(self, x):\n        \"\"\"Forward pass\"\"\"\n        basis = self.b_splines(x)\n        coef = self.coef.unsqueeze(0)\n        \n        # Spline output\n        spline_output = (basis * coef).sum(dim=-1)\n        output = spline_output.sum(dim=-1)\n        \n        # Add residual\n        base_output = torch.matmul(x, self.base_weight.t())\n        \n        return output + base_output\n\n\nclass KAN(nn.Module):\n    \"\"\"\n    KAN for Binary Classification\n    \n    Parameters:\n    -----------\n    input_dim : int\n        Number of input features\n    hidden_dim : int\n        Hidden layer size\n    grid_size : int\n        Spline grid size\n    spline_order : int\n        Spline order\n    dropout : float\n        Dropout rate\n    \"\"\"\n    \n    def __init__(self, input_dim, hidden_dim=64, grid_size=5, \n                 spline_order=3, dropout=0.3):\n        super(KAN, self).__init__()\n        \n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.grid_size = grid_size\n        self.spline_order = spline_order\n        \n        # KAN layers\n        self.kan1 = KANLinear(input_dim, hidden_dim, grid_size, spline_order)\n        self.kan2 = KANLinear(hidden_dim, hidden_dim // 2, grid_size, spline_order)\n        \n        # Output layer\n        self.output = nn.Linear(hidden_dim // 2, 1)\n        \n        # Normalization\n        self.bn1 = nn.BatchNorm1d(hidden_dim)\n        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n        \n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        \"\"\"Forward pass\"\"\"\n        # Layer 1\n        x = self.kan1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        \n        # Layer 2\n        x = self.kan2(x)\n        x = self.bn2(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        \n        # Output\n        x = self.output(x)\n        x = torch.sigmoid(x)\n        \n        return x\n\nprint(\"\\n\u2705 KAN architecture implemented!\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 9. KAN (Kolmogorov-Arnold Network) - Base Implementation\n\nSpline-based activation functions on edges",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# BASELINE: RANDOM FOREST WITH CLASS WEIGHTS\n# ============================================================================\n\ndef train_baseline_rf(X_train, y_train, X_val, y_val, seed):\n    \"\"\"\n    Train Random Forest baseline with class balancing\n    \n    Parameters:\n    -----------\n    X_train, y_train : np.ndarray\n        Training data\n    X_val, y_val : np.ndarray\n        Validation data\n    seed : int\n        Random seed\n        \n    Returns:\n    --------\n    RandomForestClassifier\n        Trained model\n    \"\"\"\n    print(f\"\\n\ud83c\udf32 Training Random Forest Baseline...\")\n    print(f\"   Training samples: {len(y_train)}\")\n    print(f\"   class_weight='balanced'\")\n    \n    rf = RandomForestClassifier(\n        n_estimators=100,\n        max_depth=10,\n        min_samples_split=20,\n        min_samples_leaf=10,\n        class_weight='balanced',\n        random_state=seed,\n        n_jobs=-1\n    )\n    \n    rf.fit(X_train, y_train)\n    \n    # Validation performance\n    y_val_pred_proba = rf.predict_proba(X_val)[:, 1]\n    y_val_pred = (y_val_pred_proba >= 0.5).astype(int)\n    \n    val_metrics = calculate_all_metrics(y_val, y_val_pred, y_val_pred_proba)\n    \n    print(f\"\\n   Validation Performance (threshold=0.5):\")\n    print(f\"      Recall: {val_metrics['recall']:.4f}\")\n    print(f\"      Precision: {val_metrics['precision']:.4f}\")\n    print(f\"      F1: {val_metrics['f1']:.4f}\")\n    print(f\"      F2: {val_metrics['f2']:.4f}\")\n    print(f\"      Accuracy: {val_metrics['accuracy']:.4f}\")\n    \n    return rf\n\n\ndef evaluate_baseline_with_threshold(rf, X_val, y_val, X_test, y_test, config):\n    \"\"\"\n    Evaluate baseline with threshold tuning\n    \n    Parameters:\n    -----------\n    rf : RandomForestClassifier\n        Trained model\n    X_val, y_val : np.ndarray\n        Validation data\n    X_test, y_test : np.ndarray\n        Test data\n    config : dict\n        Configuration\n        \n    Returns:\n    --------\n    dict\n        Results with optimal threshold\n    \"\"\"\n    # Get probabilities\n    y_val_proba = rf.predict_proba(X_val)[:, 1]\n    y_test_proba = rf.predict_proba(X_test)[:, 1]\n    \n    # Find optimal threshold on val\n    print(f\"\\n\ud83d\udd0d Finding optimal threshold on validation set...\")\n    threshold_result = find_optimal_threshold(\n        y_val, \n        y_val_proba,\n        metric='f2',\n        threshold_range=config['threshold_range'],\n        step=config['threshold_step'],\n        accuracy_floor=config['gwo_accuracy_floor'],\n        verbose=True\n    )\n    \n    optimal_threshold = threshold_result['threshold']\n    \n    # Evaluate on test with optimal threshold\n    print(f\"\\n\ud83e\uddea Evaluating on TEST set with threshold={optimal_threshold:.2f}...\")\n    y_test_pred = (y_test_proba >= optimal_threshold).astype(int)\n    test_metrics = calculate_all_metrics(y_test, y_test_pred, y_test_proba)\n    \n    print(f\"\\n   \ud83d\udcca Test Performance:\")\n    print_metrics(test_metrics, prefix=\"   \")\n    \n    return {\n        'model': rf,\n        'optimal_threshold': optimal_threshold,\n        'val_metrics': threshold_result['metrics'],\n        'test_metrics': test_metrics,\n        'threshold_curve': threshold_result['curve']\n    }\n\n\n# ============================================================================\n# TRAIN BASELINE FOR ALL DATASETS\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"\ud83c\udf32 BASELINE MODELS - RANDOM FOREST\")\nprint(\"=\"*70)\n\nbaseline_results = {}\n\nfor dataset_name, data in prepared_datasets.items():\n    print(f\"\\n{'='*70}\")\n    print(f\"\ud83d\udcca Dataset: {dataset_name}\")\n    print(f\"{'='*70}\")\n    \n    # Train on SMOTE-augmented data\n    rf = train_baseline_rf(\n        data['X_train_smote'],\n        data['y_train_smote'],\n        data['X_val'],\n        data['y_val'],\n        CONFIG['seed']\n    )\n    \n    # Evaluate with threshold tuning\n    results = evaluate_baseline_with_threshold(\n        rf,\n        data['X_val'],\n        data['y_val'],\n        data['X_test'],\n        data['y_test'],\n        CONFIG\n    )\n    \n    baseline_results[dataset_name] = results\n    \n    print(f\"\\n\u2705 {dataset_name} Baseline Complete!\")\n    print(f\"   Optimal Threshold: {results['optimal_threshold']:.2f}\")\n    print(f\"   Test F2: {results['test_metrics']['f2']:.4f}\")\n    print(f\"   Test Recall: {results['test_metrics']['recall']:.4f}\")\n    print(f\"   Test Accuracy: {results['test_metrics']['accuracy']:.4f}\")\n\nprint(f\"\\n{'='*70}\")\nprint(f\"\u2705 ALL BASELINE MODELS TRAINED\")\nprint(f\"{'='*70}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## \ud83d\ude80 EXECUTION: KAN Base Models (Lightweight)\n\nHafif KAN modelleri - CPU dostu parametreler",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}