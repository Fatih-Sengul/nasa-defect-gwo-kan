{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Safety-Aware Software Defect Prediction Framework\n",
    "## GWO-Optimized KAN with SMOTE on NASA MDP Datasets\n",
    "\n",
    "**Objective:** Maximize Recall (Safety) using:\n",
    "- SMOTE for imbalance handling\n",
    "- Grey Wolf Optimizer (GWO) for hyperparameter tuning\n",
    "- Kolmogorov-Arnold Networks (KAN) for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "# Scientific Computing\n",
    "from scipy.io import arff\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CUSTOM KAN IMPLEMENTATION (PyTorch)\n",
    "# ============================================================================\n",
    "\n",
    "class KANLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Kolmogorov-Arnold Network Linear Layer.\n",
    "    Uses B-spline basis functions for learnable non-linear transformations.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, grid_size=5, spline_order=3):\n",
    "        super(KANLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "        \n",
    "        # Number of spline basis functions\n",
    "        self.num_bases = grid_size + spline_order\n",
    "        \n",
    "        # Learnable coefficients for each input-output connection\n",
    "        self.coefficients = nn.Parameter(\n",
    "            torch.randn(in_features, out_features, self.num_bases) * 0.1\n",
    "        )\n",
    "        \n",
    "        # Grid points for spline evaluation (learnable)\n",
    "        grid = torch.linspace(-1, 1, grid_size)\n",
    "        self.register_buffer('grid', grid)\n",
    "        \n",
    "    def b_spline_basis(self, x, i, k):\n",
    "        \"\"\"\n",
    "        Compute B-spline basis function using Cox-de Boor recursion.\n",
    "        \"\"\"\n",
    "        if k == 0:\n",
    "            return ((self.grid[i] <= x) & (x < self.grid[i + 1])).float()\n",
    "        else:\n",
    "            # Avoid division by zero\n",
    "            left_num = x - self.grid[i]\n",
    "            left_den = self.grid[i + k] - self.grid[i] + 1e-8\n",
    "            left = left_num / left_den * self.b_spline_basis(x, i, k - 1)\n",
    "            \n",
    "            right_num = self.grid[i + k + 1] - x\n",
    "            right_den = self.grid[i + k + 1] - self.grid[i + 1] + 1e-8\n",
    "            right = right_num / right_den * self.b_spline_basis(x, i + 1, k - 1)\n",
    "            \n",
    "            return left + right\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: Apply spline-based transformation.\n",
    "        x: (batch_size, in_features)\n",
    "        output: (batch_size, out_features)\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Normalize input to [-1, 1] range\n",
    "        x_normalized = torch.tanh(x)\n",
    "        \n",
    "        # Compute basis functions for each input (simplified approximation)\n",
    "        # Using Gaussian RBF as a practical spline approximation\n",
    "        x_expanded = x_normalized.unsqueeze(-1)  # (batch, in_features, 1)\n",
    "        grid_expanded = self.grid.view(1, 1, -1)  # (1, 1, grid_size)\n",
    "        \n",
    "        # RBF kernel centered at grid points\n",
    "        sigma = 2.0 / self.grid_size\n",
    "        bases = torch.exp(-((x_expanded - grid_expanded) ** 2) / (2 * sigma ** 2))\n",
    "        \n",
    "        # Pad to match num_bases if needed\n",
    "        if bases.shape[-1] < self.num_bases:\n",
    "            padding = self.num_bases - bases.shape[-1]\n",
    "            bases = torch.cat([bases, torch.zeros_like(bases[..., :padding])], dim=-1)\n",
    "        else:\n",
    "            bases = bases[..., :self.num_bases]\n",
    "        \n",
    "        # Apply learnable coefficients\n",
    "        # bases: (batch, in_features, num_bases)\n",
    "        # coefficients: (in_features, out_features, num_bases)\n",
    "        output = torch.einsum('bin,ion->bo', bases, self.coefficients)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class KAN(nn.Module):\n",
    "    \"\"\"\n",
    "    Kolmogorov-Arnold Network for Binary Classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=64, grid_size=5, spline_order=3, dropout=0.2):\n",
    "        super(KAN, self).__init__()\n",
    "        \n",
    "        self.layer1 = KANLinear(input_dim, hidden_dim, grid_size, spline_order)\n",
    "        self.layer2 = KANLinear(hidden_dim, hidden_dim // 2, grid_size, spline_order)\n",
    "        self.layer3 = KANLinear(hidden_dim // 2, 1, grid_size, spline_order)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.SiLU()  # Swish activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.layer1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer3(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "print(\"‚úì KAN Architecture Defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GREY WOLF OPTIMIZER (GWO)\n",
    "# ============================================================================\n",
    "\n",
    "class GreyWolfOptimizer:\n",
    "    \"\"\"\n",
    "    Grey Wolf Optimizer for hyperparameter optimization.\n",
    "    Maximizes Recall (safety metric) for defect prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, objective_func, bounds, n_wolves=10, max_iter=20, verbose=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            objective_func: Function to maximize (returns fitness score)\n",
    "            bounds: List of (min, max) tuples for each parameter\n",
    "            n_wolves: Population size\n",
    "            max_iter: Maximum iterations\n",
    "        \"\"\"\n",
    "        self.objective_func = objective_func\n",
    "        self.bounds = np.array(bounds)\n",
    "        self.n_wolves = n_wolves\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "        self.n_dims = len(bounds)\n",
    "        \n",
    "        # Initialize wolf positions randomly within bounds\n",
    "        self.positions = np.random.uniform(\n",
    "            self.bounds[:, 0], \n",
    "            self.bounds[:, 1], \n",
    "            (n_wolves, self.n_dims)\n",
    "        )\n",
    "        \n",
    "        # Initialize fitness\n",
    "        self.fitness = np.zeros(n_wolves)\n",
    "        \n",
    "        # Alpha, Beta, Delta positions and fitness\n",
    "        self.alpha_pos = np.zeros(self.n_dims)\n",
    "        self.alpha_fitness = -np.inf\n",
    "        \n",
    "        self.beta_pos = np.zeros(self.n_dims)\n",
    "        self.beta_fitness = -np.inf\n",
    "        \n",
    "        self.delta_pos = np.zeros(self.n_dims)\n",
    "        self.delta_fitness = -np.inf\n",
    "        \n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        Run the GWO optimization algorithm.\n",
    "        Returns: Best parameters found\n",
    "        \"\"\"\n",
    "        for iteration in range(self.max_iter):\n",
    "            # Evaluate fitness for all wolves\n",
    "            for i in range(self.n_wolves):\n",
    "                self.fitness[i] = self.objective_func(self.positions[i])\n",
    "                \n",
    "                # Update Alpha, Beta, Delta\n",
    "                if self.fitness[i] > self.alpha_fitness:\n",
    "                    self.delta_fitness = self.beta_fitness\n",
    "                    self.delta_pos = self.beta_pos.copy()\n",
    "                    \n",
    "                    self.beta_fitness = self.alpha_fitness\n",
    "                    self.beta_pos = self.alpha_pos.copy()\n",
    "                    \n",
    "                    self.alpha_fitness = self.fitness[i]\n",
    "                    self.alpha_pos = self.positions[i].copy()\n",
    "                    \n",
    "                elif self.fitness[i] > self.beta_fitness:\n",
    "                    self.delta_fitness = self.beta_fitness\n",
    "                    self.delta_pos = self.beta_pos.copy()\n",
    "                    \n",
    "                    self.beta_fitness = self.fitness[i]\n",
    "                    self.beta_pos = self.positions[i].copy()\n",
    "                    \n",
    "                elif self.fitness[i] > self.delta_fitness:\n",
    "                    self.delta_fitness = self.fitness[i]\n",
    "                    self.delta_pos = self.positions[i].copy()\n",
    "            \n",
    "            # Linearly decrease 'a' from 2 to 0\n",
    "            a = 2 - iteration * (2 / self.max_iter)\n",
    "            \n",
    "            # Update wolf positions\n",
    "            for i in range(self.n_wolves):\n",
    "                for j in range(self.n_dims):\n",
    "                    # Alpha influence\n",
    "                    r1, r2 = np.random.rand(2)\n",
    "                    A1 = 2 * a * r1 - a\n",
    "                    C1 = 2 * r2\n",
    "                    D_alpha = abs(C1 * self.alpha_pos[j] - self.positions[i, j])\n",
    "                    X1 = self.alpha_pos[j] - A1 * D_alpha\n",
    "                    \n",
    "                    # Beta influence\n",
    "                    r1, r2 = np.random.rand(2)\n",
    "                    A2 = 2 * a * r1 - a\n",
    "                    C2 = 2 * r2\n",
    "                    D_beta = abs(C2 * self.beta_pos[j] - self.positions[i, j])\n",
    "                    X2 = self.beta_pos[j] - A2 * D_beta\n",
    "                    \n",
    "                    # Delta influence\n",
    "                    r1, r2 = np.random.rand(2)\n",
    "                    A3 = 2 * a * r1 - a\n",
    "                    C3 = 2 * r2\n",
    "                    D_delta = abs(C3 * self.delta_pos[j] - self.positions[i, j])\n",
    "                    X3 = self.delta_pos[j] - A3 * D_delta\n",
    "                    \n",
    "                    # Update position\n",
    "                    self.positions[i, j] = (X1 + X2 + X3) / 3\n",
    "                    \n",
    "                    # Boundary check\n",
    "                    self.positions[i, j] = np.clip(\n",
    "                        self.positions[i, j],\n",
    "                        self.bounds[j, 0],\n",
    "                        self.bounds[j, 1]\n",
    "                    )\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"  Iter {iteration+1}/{self.max_iter} | \"\n",
    "                      f\"Alpha Fitness (Recall): {self.alpha_fitness:.4f} | \"\n",
    "                      f\"Beta: {self.beta_fitness:.4f} | \"\n",
    "                      f\"Delta: {self.delta_fitness:.4f}\")\n",
    "        \n",
    "        return self.alpha_pos, self.alpha_fitness\n",
    "\n",
    "\n",
    "print(\"‚úì Grey Wolf Optimizer Implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA LOADER & PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "class NASADataLoader:\n",
    "    \"\"\"\n",
    "    Handles loading and preprocessing of NASA MDP datasets (.arff format).\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_dir='./dataset/'):\n",
    "        self.dataset_dir = Path(dataset_dir)\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def load_arff(self, file_path):\n",
    "        \"\"\"\n",
    "        Load .arff file and convert to pandas DataFrame.\n",
    "        Handles byte-string decoding issues.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data, meta = arff.loadarff(file_path)\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            # Decode byte strings if present\n",
    "            for col in df.columns:\n",
    "                if df[col].dtype == object:\n",
    "                    try:\n",
    "                        df[col] = df[col].str.decode('utf-8')\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def preprocess(self, df, target_col='Defective'):\n",
    "        \"\"\"\n",
    "        Preprocess dataset:\n",
    "        1. Separate features and target\n",
    "        2. Handle missing values\n",
    "        3. Encode labels\n",
    "        4. Normalize features\n",
    "        \"\"\"\n",
    "        # Handle common target column names\n",
    "        possible_targets = ['Defective', 'defects', 'bug', 'Defect', 'class']\n",
    "        target_col = None\n",
    "        for col in possible_targets:\n",
    "            if col in df.columns:\n",
    "                target_col = col\n",
    "                break\n",
    "        \n",
    "        if target_col is None:\n",
    "            # Assume last column is target\n",
    "            target_col = df.columns[-1]\n",
    "        \n",
    "        # Separate features and target\n",
    "        X = df.drop(columns=[target_col])\n",
    "        y = df[target_col]\n",
    "        \n",
    "        # Drop non-numeric columns (e.g., module names)\n",
    "        X = X.select_dtypes(include=[np.number])\n",
    "        \n",
    "        # Handle missing values\n",
    "        X = X.fillna(X.mean())\n",
    "        \n",
    "        # Encode labels (True/False or Y/N to 1/0)\n",
    "        if y.dtype == object or y.dtype == bool:\n",
    "            y = self.label_encoder.fit_transform(y)\n",
    "        \n",
    "        # Ensure binary classification\n",
    "        y = y.astype(int)\n",
    "        \n",
    "        return X.values, y\n",
    "    \n",
    "    def apply_smote(self, X_train, y_train, random_state=42):\n",
    "        \"\"\"\n",
    "        Apply SMOTE to training data ONLY.\n",
    "        Prevents data leakage by not touching test data.\n",
    "        \"\"\"\n",
    "        # Check if minority class has enough samples\n",
    "        unique, counts = np.unique(y_train, return_counts=True)\n",
    "        min_samples = min(counts)\n",
    "        \n",
    "        # SMOTE requires at least 2 samples in minority class\n",
    "        if min_samples < 2:\n",
    "            print(\"  Warning: Not enough minority samples for SMOTE. Skipping.\")\n",
    "            return X_train, y_train\n",
    "        \n",
    "        # Determine k_neighbors (must be less than minority samples)\n",
    "        k_neighbors = min(5, min_samples - 1)\n",
    "        \n",
    "        try:\n",
    "            smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n",
    "            X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "            print(f\"  SMOTE applied: {len(y_train)} -> {len(y_resampled)} samples\")\n",
    "            return X_resampled, y_resampled\n",
    "        except Exception as e:\n",
    "            print(f\"  SMOTE failed: {e}. Using original data.\")\n",
    "            return X_train, y_train\n",
    "    \n",
    "    def get_dataset_files(self):\n",
    "        \"\"\"\n",
    "        Get all .arff files from dataset directory.\n",
    "        \"\"\"\n",
    "        return list(self.dataset_dir.glob('*.arff'))\n",
    "\n",
    "\n",
    "print(\"‚úì Data Loader Implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING & EVALUATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def train_kan(model, X_train, y_train, epochs=50, lr=0.01, batch_size=32, verbose=False):\n",
    "    \"\"\"\n",
    "    Train KAN model.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_tensor = torch.FloatTensor(y_train).reshape(-1, 1).to(device)\n",
    "    \n",
    "    # DataLoader\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Optimizer and Loss\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_X, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(loader):.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_kan(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate KAN model and return metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X_test).to(device)\n",
    "        y_pred_prob = model(X_tensor).cpu().numpy()\n",
    "        y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    # AUC (requires probabilities)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    except:\n",
    "        auc = 0.0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úì Training & Evaluation Functions Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MAIN PIPELINE: GWO-OPTIMIZED KAN WITH SMOTE\n",
    "# ============================================================================\n",
    "\n",
    "class SafetyAwareDefectPredictor:\n",
    "    \"\"\"\n",
    "    Complete pipeline for safety-aware defect prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_dir='./dataset/'):\n",
    "        self.data_loader = NASADataLoader(dataset_dir)\n",
    "        self.results = []\n",
    "        \n",
    "    def create_objective_function(self, X_train, y_train, X_val, y_val, input_dim):\n",
    "        \"\"\"\n",
    "        Create objective function for GWO.\n",
    "        Maximizes Recall on validation set.\n",
    "        \"\"\"\n",
    "        def objective(params):\n",
    "            # Decode parameters\n",
    "            grid_size = int(params[0])\n",
    "            spline_order = int(params[1])\n",
    "            hidden_dim = int(params[2])\n",
    "            lr = params[3]\n",
    "            \n",
    "            # Create and train model\n",
    "            model = KAN(\n",
    "                input_dim=input_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                grid_size=grid_size,\n",
    "                spline_order=spline_order\n",
    "            ).to(device)\n",
    "            \n",
    "            # Train for fewer epochs during optimization\n",
    "            model = train_kan(model, X_train, y_train, epochs=30, lr=lr, verbose=False)\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            metrics = evaluate_kan(model, X_val, y_val)\n",
    "            \n",
    "            # Return Recall (our safety metric)\n",
    "            # We could also use F2-score which weights recall higher: (5 * p * r) / (4 * p + r)\n",
    "            return metrics['recall']\n",
    "        \n",
    "        return objective\n",
    "    \n",
    "    def process_dataset(self, dataset_path):\n",
    "        \"\"\"\n",
    "        Process a single dataset through the complete pipeline.\n",
    "        \"\"\"\n",
    "        dataset_name = dataset_path.stem\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing: {dataset_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # 1. Load Data\n",
    "        print(\"[1/6] Loading dataset...\")\n",
    "        df = self.data_loader.load_arff(dataset_path)\n",
    "        if df is None:\n",
    "            return None\n",
    "        \n",
    "        X, y = self.data_loader.preprocess(df)\n",
    "        print(f\"  Shape: {X.shape}, Defect Rate: {y.mean():.2%}\")\n",
    "        \n",
    "        # 2. Train/Test Split (Stratified)\n",
    "        print(\"[2/6] Splitting data (80/20)...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    "        )\n",
    "        \n",
    "        # 3. Apply SMOTE (Training Only)\n",
    "        print(\"[3/6] Applying SMOTE...\")\n",
    "        X_train_smote, y_train_smote = self.data_loader.apply_smote(X_train, y_train)\n",
    "        \n",
    "        # Normalize features\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_smote)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Create validation split for GWO\n",
    "        X_train_gwo, X_val_gwo, y_train_gwo, y_val_gwo = train_test_split(\n",
    "            X_train_scaled, y_train_smote, test_size=0.2, random_state=RANDOM_SEED\n",
    "        )\n",
    "        \n",
    "        # 4. GWO Hyperparameter Optimization\n",
    "        print(\"[4/6] Running GWO for hyperparameter optimization...\")\n",
    "        print(\"  Optimizing: grid_size, spline_order, hidden_dim, learning_rate\")\n",
    "        print(\"  Objective: Maximize Recall (Safety Metric)\")\n",
    "        \n",
    "        # Define search space\n",
    "        bounds = [\n",
    "            (3, 10),      # grid_size\n",
    "            (2, 5),       # spline_order\n",
    "            (16, 128),    # hidden_dim\n",
    "            (0.001, 0.1)  # learning_rate\n",
    "        ]\n",
    "        \n",
    "        objective_func = self.create_objective_function(\n",
    "            X_train_gwo, y_train_gwo, X_val_gwo, y_val_gwo, X.shape[1]\n",
    "        )\n",
    "        \n",
    "        gwo = GreyWolfOptimizer(\n",
    "            objective_func=objective_func,\n",
    "            bounds=bounds,\n",
    "            n_wolves=8,\n",
    "            max_iter=15,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        best_params, best_fitness = gwo.optimize()\n",
    "        \n",
    "        # Decode best parameters\n",
    "        best_grid_size = int(best_params[0])\n",
    "        best_spline_order = int(best_params[1])\n",
    "        best_hidden_dim = int(best_params[2])\n",
    "        best_lr = best_params[3]\n",
    "        \n",
    "        print(f\"\\n  ‚úì Best Parameters Found:\")\n",
    "        print(f\"    Grid Size: {best_grid_size}\")\n",
    "        print(f\"    Spline Order: {best_spline_order}\")\n",
    "        print(f\"    Hidden Dim: {best_hidden_dim}\")\n",
    "        print(f\"    Learning Rate: {best_lr:.4f}\")\n",
    "        print(f\"    Validation Recall: {best_fitness:.4f}\")\n",
    "        \n",
    "        # 5. Train Final Model with Best Parameters\n",
    "        print(\"\\n[5/6] Training final model with optimized parameters...\")\n",
    "        final_model = KAN(\n",
    "            input_dim=X.shape[1],\n",
    "            hidden_dim=best_hidden_dim,\n",
    "            grid_size=best_grid_size,\n",
    "            spline_order=best_spline_order\n",
    "        ).to(device)\n",
    "        \n",
    "        final_model = train_kan(\n",
    "            final_model, \n",
    "            X_train_scaled, \n",
    "            y_train_smote, \n",
    "            epochs=100, \n",
    "            lr=best_lr,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # 6. Evaluate on Test Set\n",
    "        print(\"\\n[6/6] Evaluating on test set...\")\n",
    "        test_metrics = evaluate_kan(final_model, X_test_scaled, y_test)\n",
    "        \n",
    "        print(f\"\\n  üìä Test Results:\")\n",
    "        print(f\"    Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"    Precision: {test_metrics['precision']:.4f}\")\n",
    "        print(f\"    Recall:    {test_metrics['recall']:.4f} ‚ö†Ô∏è  (Safety Metric)\")\n",
    "        print(f\"    F1-Score:  {test_metrics['f1']:.4f}\")\n",
    "        print(f\"    AUC:       {test_metrics['auc']:.4f}\")\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'Dataset': dataset_name,\n",
    "            'Grid_Size': best_grid_size,\n",
    "            'Spline_Order': best_spline_order,\n",
    "            'Hidden_Dim': best_hidden_dim,\n",
    "            'Learning_Rate': best_lr,\n",
    "            'Accuracy': test_metrics['accuracy'],\n",
    "            'Precision': test_metrics['precision'],\n",
    "            'Recall': test_metrics['recall'],\n",
    "            'F1_Score': test_metrics['f1'],\n",
    "            'AUC': test_metrics['auc']\n",
    "        }\n",
    "        \n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def run_all_datasets(self):\n",
    "        \"\"\"\n",
    "        Process all datasets in the directory.\n",
    "        \"\"\"\n",
    "        dataset_files = self.data_loader.get_dataset_files()\n",
    "        \n",
    "        if not dataset_files:\n",
    "            print(\"‚ö†Ô∏è  No .arff files found in ./dataset/ directory!\")\n",
    "            print(\"Please add NASA MDP datasets (e.g., CM1.arff, JM1.arff, etc.)\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüöÄ Found {len(dataset_files)} datasets to process\\n\")\n",
    "        \n",
    "        for dataset_path in dataset_files:\n",
    "            try:\n",
    "                self.process_dataset(dataset_path)\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Error processing {dataset_path.name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save results\n",
    "        self.save_results()\n",
    "    \n",
    "    def save_results(self, output_file='final_results.xlsx'):\n",
    "        \"\"\"\n",
    "        Save consolidated results to Excel file.\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            print(\"\\n‚ö†Ô∏è  No results to save.\")\n",
    "            return\n",
    "        \n",
    "        df_results = pd.DataFrame(self.results)\n",
    "        df_results.to_excel(output_file, index=False, sheet_name='GWO-KAN Results')\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"‚úÖ Results saved to: {output_file}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(\"\\nüìä Summary Statistics:\")\n",
    "        print(df_results[['Dataset', 'Recall', 'F1_Score', 'AUC']].to_string(index=False))\n",
    "        print(f\"\\nAverage Recall (Safety): {df_results['Recall'].mean():.4f}\")\n",
    "        print(f\"Average F1-Score: {df_results['F1_Score'].mean():.4f}\")\n",
    "\n",
    "\n",
    "print(\"‚úì Main Pipeline Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXECUTE PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = SafetyAwareDefectPredictor(dataset_dir='./dataset/')\n",
    "\n",
    "# Run on all datasets\n",
    "predictor.run_all_datasets()\n",
    "\n",
    "print(\"\\nüéâ Pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
